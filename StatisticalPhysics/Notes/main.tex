\documentclass{book}
% \usepackage[table,xcdraw,svgnames]{xcolor}
\usepackage{blindtext} % Package to generate dummy text throughout this template 
% \usepackage{float}
\usepackage[table,xcdraw]{xcolor}
% \usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
 % Line spacing - Palatino needs more space between lines
\usepackage{cancel} % 'left' option added here
\usepackage{euler}
\usepackage{multirow}
\usepackage{braket}

%\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{graphicx}
\usepackage[spanish]{babel} % Language hyphenation and typographical rules
\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{enumerate} % Customized lists
 % Make itemize lists more compact
% \usepackage{abstract} % Allows abstract customization
% \renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
% \renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text
\usepackage{titlesec} % Allows customization of titles
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles
\usepackage{fancyhdr} % Headers and footers
 % All pages have headers and footers
\fancyhead[R]{$\hspace{2mm}$} % Blank out the default header

 % Blank out the default footer
% Custom header text
\usepackage{booktabs}


\usepackage{amsmath, amsthm, amssymb}
\usepackage{titling} % Customizing the title section
\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{mathtools}
\usepackage{physics}
\usepackage{xcolor, colortbl}
\usepackage{array, multirow, multicol}
% \usepackage{amsmath}
\usepackage{longtable}
% \usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
% \usepackage{amsmath}
\usepackage{footnote}
% \usepackage{amsfonts}
% \usepackage{amssymb}
% \usepackage{tablefootnote}
% \usepackage{graphicx}
% \usepackage[table]{xcolor}
% \usepackage{colortbl}
% \usepackage{array, multirow, multicol, tabularx}
\usepackage{tcolorbox}
% \usepackage{graphicx}
\usepackage{caption}
% \usepackage{lipsum}
% \usepackage{caption}
\usepackage{xpatch}
\xpatchcmd{\proof}{\itshape}{\normalfont\proofnamefont}{}{}

\newcommand{\brackets}[1]{\left[#1\right]}
\newcommand{\curlybraces}[1]{\left\{#1\right\}}

\newcommand{\qedh}{\hfill\hspace{5mm}\fbox{\phantom{\rule{.5ex}{.5ex}}}}

\newenvironment{Figura}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

% \renewcommand{\refname}{Bibliografía de Figuras}
% \renewcommand{\bibname}{Nuevo Título de Bibliografía}
% \usepackage[usenames,dvipsnames]{color}
%Este pequeño bloque permite que el número de la ecuación permanezca del tamaño del resto del texto
%si se decide hacer small el tamaño de la ecuación.
%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%
\newtheorem{propiedad}{Propiedades}[section]
\newtheorem{axiom}{Axioma}[section]
\newtheorem{thm}{Teorema}[section]
\newtheorem{theorem}{Teorema}[section]
\newtheorem{proposition}[thm]{Proposición} 
\newtheorem{lemma}[thm]{Lema}
\newtheorem{corollary}[thm]{Corolario} 
\newtheorem{conv}[thm]{Convención}
\newtheorem{defi}[thm]{Definición}
\newtheorem{definition}[theorem]{Definición}
\newtheorem{notation}[thm]{Notación} 
\newtheorem{exe}[thm]{Ejemplo}
\newtheorem{conjecture}[thm]{Conjetura} 
\newtheorem{prob}[thm]{Problema}
\newtheorem{remark}[thm]{Observación}
\newtheorem{example}[thm]{Ejemplo}
\newtheorem{note}[thm]{Nota}

%\newcommand{\brackets}[1]{\left[#1\right]}
%\newcommand{\curlybraces}[1]{\left\{#1\right\}}
%\newcommand{\qedh}{\hfill\hspace{5mm}\qedsymbol}
\newcommand{\scalar}[2]{\langle #1, #2 \rangle}
\newcommand{\ptensor}[2]{#1 \otimes #2}
\newcommand{\pcart}[2]{#1 \times #2}
\newcommand{\funct}[3]{#1:\hspace{1mm} #2\to #3}
%\newcommand{\abs}[1]{|#1|}

\newtcolorbox[auto counter, number within=section]{mytheorem}[2][]{
  enhanced,
  breakable,
  title=Teorema~\thetcbcounter: #2,
  #1,
}
\newtcolorbox[auto counter, number within=section]{propositionbox}[2][]{
  enhanced,
  breakable,
  title=Proposition~\thetcbcounter: #2,
  #1,
}

\newtcolorbox[auto counter, number within=section]{corollarybox}[2][]{
  enhanced,
  breakable,
  title=Corollary~\thetcbcounter: #2,
  #1,
}

\newtcolorbox[auto counter, number within=section]{remarkbox}[2][]{
  enhanced,
  breakable,
  title=Remark~\thetcbcounter: #2,
  #1,
}

\newtcolorbox[auto counter, number within=section]{notebox}[2][]{
  enhanced,
  breakable,
  title=Note~\thetcbcounter: #2,
  #1,
}


%\newenvironment{Figura}
 % {\par\medskip\noindent\minipage{\linewidth}}
  %{\endminipage\par\medskip}


\newcommand{\proofnamefont}{\bfseries}
\setlength {\marginparwidth}{2cm}
\begin{document}
\pagestyle{empty}
\begin{titlepage} 

	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
	
	\center 
	\textsc{\LARGE Facultad de Ciencias}\\[1.5cm] 
	
	\textsc{\Large Grado de Física}\\[0.5cm] 
	\textsc{\large Física Estadística}\\[0.5cm] 
	
	\HRule\\[0.4cm]
	
	\huge\bfseries Temario Física Estadística 23-24 \\[0.3cm] 
	\HRule\\[1cm]
	
	\vfill
	\includegraphics[width=10cm]{logo-uco.jpg} 
	\vfill \vfill \vfill
	\begin{minipage}{0.7\textwidth}
		\begin{flushleft}
		\Large
			\textit{Autor}\\
			Rubén Carrión Castro\\
			
		\end{flushleft}
	\end{minipage}
	
	
	\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	

	\vfill 
	
\end{titlepage}

\tableofcontents
\listoffigures
\newpage
\setcounter{page}{1}
\pagestyle{fancy}
\chapter*{Tema 0: Repaso de Estadística}
\section{Conjuntos}
En Estadística siempre vamos a trabajar en el denominado \textbf{espacio muestral} $S$, que es el conjuntos que contiene todos los resultados de un experimento aleatorio. Dentro de este espacio se definen subconjuntos de este espacio, llamados \textit{eventos} $(A,B)$. Las operaciones que actúan sobre estos subconjuntos son la unión, $\bigcup\limits_{k=1}^NA_k$, la intersección, $\bigcap\limits_{k=1}^NA_k$ y la negación $A'$.\\ \\
Para dos subconjuntos $A,B\in S$, estas operaciones actúan como $A\cup B$, y se lee como '$A$ o $B$'; la intersección como $A\cap B$, y se lee como '$A$ y $B$'; y la negación se lee como 'no $A$', también denominado \textit{complemento de} $A$.\\ \\
Estas operaciones satisfacen que si $A\cap B=\emptyset$, entonces $A$ y $B$ son conjuntos disjuntos, si $A\cap B\neq\emptyset$, entonces tendrán una parte común. Además, el conjunto $A\cup A'=S$ $(\text{con }A,A'\in S)$ ,corresponderá a todo el espacio (todo el 'universo').
\section{Definiciones}
\begin{definition}{Clásica}
    Si un evento puede ocurrir de $h$ maneras diferentes de un número total de $n$ maneras posibles, todos ellos son igualmente posibles. Entonces, la probabilidad del evento será
    \begin{equation}
        \mathcal{P}(h)=\frac{h}{n}
    \end{equation}
    es decir, número de eventos posibles partido del número de eventos totales.
\end{definition}
\begin{definition}{Frecuentista}
    Si después de $n$ repeticiones de un experimento, donde $n$ es muy grande $(n\ggg1)$, se observa que un evento ocurre $h$ veces, entonces la probabilidad de dicho evento es
    \begin{equation}
        \mathscr{P}(h)=\frac{h}{n}
    \end{equation}
    Esto también se denomina \textit{probabilidad empírica} de un evento.
\end{definition}
\section{Axiomas de Kolmogórov}
\begin{axiom}
    La probabilidad $S$ es no negativa y menor o igual que 1.
    \[0\leq p(S)\leq 1\]
\end{axiom}
\begin{axiom}
    La probabilidad del evento seguro, $S$, es igual a 1, denotado simbólicamente.
\end{axiom}
\begin{axiom}
    Si $A_1$, $A_2$, $\dots$ son eventos mutuamente excluyentes (incompatibles dos a dos), entonces:
    \begin{equation}
        P(A_1\cup A_2\cup\dots)=\sum P(A_i)
    \end{equation}
\end{axiom}
Veamos ahora los teoremas que se pueden derivar de esta axiomática:
\begin{theorem}
    Para todo evento $A$, se cumple que $0\leq P(A)\leq1$.
\end{theorem}
Fácilmente demostrable con los Axiomas 1 y 3.
\begin{theorem}
    El evento imposible tiene probabilidad cero, $P(\emptyset)=0$.
\end{theorem}
\begin{proof}
    Trivial.
\end{proof}
\begin{theorem}
    Si $A'$ es el complemento de $A$, entonces $P(A')=1-P(A)$.
\end{theorem}
\begin{proof}
    Si el espacio total es $S=A+A'$, entonces $P(S)=1=P(A)+P(A')$, luego $P(A')=1-P(A)$.
\end{proof}
\begin{theorem}
    Si $A$ y $B$ son dos eventos cualesquiera, entonces 
    \begin{equation}
        P(A\cup B)=P(A)+P(B)-P(A\cap B)
    \end{equation}
\end{theorem}
Podemos tener eventos que condiciones la probabilidad de que sucedan otro tipo de eventos, por ello, se introduce el concepto de \textit{probabilidad condicionada}. Consideramos $P(A|B)$, siendo la probabilidad de que ocurra un evento $A$ sabiendo que también sucede otro evento $B$. La probabilidad condicional se escribe $P(A|B)$, y se lee 'la probabilidad de $A$ dado $B$', tal que
\begin{equation}
    P(A|B)=\frac{P(A\cap B)}{P(B)}
\end{equation}
Con el concepto de probabilidad condicionada podremos encontrar aquellos eventos que sean independientes, pues si $P(A|B)=P(A)$, es decir, que la probabilidad de que ocurra $A$ no depende de la ocurrencia o no de $B$, entonces decimos que $A$ y $B$ son eventos independientes. Esto equivale a,
\begin{equation}
    P(A\cap B)=P(A)P(B)
\end{equation}
\begin{proof}
    $P(A)=P(A|B)=\frac{P(A\cap B)}{P(B)} \Rightarrow P(A\cap B)=P(A)P(B)$
\end{proof}
\section{Combinatoria}
La combinatoria estudia la enumeración, construcción y existencia de propiedades de configuraciones que satisfacen ciertas condiciones establecidas.\\ \\
Las permutaciones son variaciones del orden o al disposición de los elementos. Si tenemos $n$ objetos diferentes, la ordenación en línea de éstos sería $n(n-1)(n-2)\dots1\equiv n!$, que equivale al factorial de $n$. Si quisiéramos ordenar solo $k$ elementos y que no se repitiera ninguno, tendríamos
\begin{equation}
    \frac{n!}{(n-k)!}
\end{equation}
y si quisiéramos que se repitieran, tendríamos $n^k$.\\ \\
Las combinaciones nos interesarán en muchas ocasiones, pues solo nos interesará la selección de los objetos sin tener en cuenta su orden, luego, sin repetición, podemos escoger $k$ elementos de un conjunto de $n$ elementos usando
\begin{equation}
    \begin{pmatrix}
        k\\
        n
    \end{pmatrix}=C_{(n,k)}=\frac{n!}{k!(n-k)!}
\end{equation}
Con repeticiones tendríamos,
\begin{equation}
    \begin{pmatrix}
        n+k-1\\
        k
    \end{pmatrix}=\frac{(n+k-1)!}{k!(n-1)!}
\end{equation}
\subsection{Aproximación de Stirling}
Cuando $n$ es grande, no es práctica la evaluación directa de $n!$. En tal caso, se pueden emplear las siguientes fórmulas de aproximación:
\begin{equation}
    \ln{n!}\approx n\ln{n}-n
\end{equation}
\begin{equation}
    n!\sim\sqrt{2\pi n}n^ne^{-n}
\end{equation}
siendo la denominada \textit{aproximación de Stirling}.
\section{Distribución de probabilidad}
Supongamos que a cada punto del espacio muestral le asignamos un número. Tenemos entonces una función definida en el espacio muestral; esta función es una variable estocástica $(X)$, siendo variables discretas y continuas.
\subsection{Distribuciones discretas}
Sea $X$ una variable aleatoria discreta, con valores $x_1,x_2,x_3,\dots$ a los que se asocia una probabilidad $f(x_k)$, tal que $P(X=x_k)=f(x_k)$ con $k=1,2,\dots$ donde $f(x)$ se define como la \textit{función o distribución de probabilidad}. Sus propiedades son que $f(x)\geq 0$ y que $\sum\limits_{k}f(x_k)=1$.
\subsubsection{Función de distribución}
La función de distribución acumulada o función de distribución para $X$ está definida por,
\begin{equation}
    F(x)=P(X\leq x),\hspace{4mm}x\in(-\infty,\infty)
\end{equation}
Sus propiedades son,
\begin{enumerate}
    \item $F(x)$ es creciente.
    \item $\lim\limits_{x\to-\infty}F(x)=0$, $\lim\limits_{x\to\infty}F(x)=1$.
    \item $F(x)$ es continua desde la derecha. Es decir, $\lim\limits_{h\to0^+}F(x+h)=F(x)$, $\forall x$.
\end{enumerate}
\subsection{Distribuciones continuas}
Se define $f(x)$ como la función de densidad de probabilidad, tal que
\begin{equation}
    f(x)=\frac{dF}{dx}
\end{equation}
cuyas propiedades son,
\begin{enumerate}
    \item $f(x)\geq0$, $\forall x$.
    \item $\int_{-\infty}^{\infty}f(x)dx=1$.
    \item $\int_{-\infty}^{x}f(u)du=F(x)$.
    \item $P(a<X<b)=\int_{a}^{b}f(x)dx$
\end{enumerate}
\section{Momentos de una distribución}
Una vez definida las distribuciones de probabilidad, debemos caracterizarlas.\\ \\
Sea $k\in\mathbb{N}$, el momento de orden $k$ es $E[X^k]$, y se calcula como,
\begin{equation}
    \left<x^k\right>=\sum_ix_i^kP(X=x_i)
\end{equation}
para el caso discreto, y
\begin{equation}
    \left<x^k\right>=\int x^kf(x)dx
\end{equation}
para el caso continuo.
\subsection{Función generadora de momentos}
La función generadora de momentos es,
\begin{equation}
    M_X(t):=\mathbb{E}\left(e^{tX}\right),\hspace{3mm}t\in\mathbb{R}
\end{equation}
para el caso discreto tendremos,
\begin{equation}
    M_X(t)=\sum_{i=1}^{\infty}e^{tx_i}P(X=x_i)
\end{equation}
y para el caso continuo,
\begin{equation}
    M_X(t)=\int_{-\infty}^{\infty}e^{tx}f(x)dx
\end{equation}
Permite generar los momentos,
\begin{equation}
    \mathbb{E}(X^n)=M_X^{(n)}(0)=\left.\frac{d^nM_X}{dt^n}\right|_{t=0}
\end{equation}
\subsection{Función generadora de cumulantes}
La función generadora de cumulantes se define como
\begin{equation}
    \varphi_X(t)=\mathbb{E}\brackets{e^{itX}}=\left\lbrace\begin{matrix}
        =\int_{-\infty}^{\infty}e^{itx}f_X(x)dx & \text{para continuo}\\
        \sum e^{itx_i}P(X=x_i) & \text{para discreto}
    \end{matrix}\right.
\end{equation}
Si los momentos existen, entonces $\varphi_X^{(n)}(0)=i^n\mathbb{E}[X^n]$. Se definen los cumulantes $(\kappa_n)$ como 
\begin{equation}
    H(t)=\log{\mathbb{E}\brackets{e^{itX}}}=\sum_{n=1}^{\infty}\kappa_n\frac{(it)^n}{n!}=\mu it-\sigma^2\frac{t^2}{2}+\dots
\end{equation}
En física estadística, la función de partición canónica es $Z(\beta)=\left<e^{-\beta E}\right>$, usando la energía libre de Helmholtz, $F=-\frac{1}{\beta}\log{Z}$.



\chapter{TEMA 1: Fundamentos de la Física Estadística Clásica }
\section{Introducción}

La mecánica estadística se encargar de deducir e interpretar las leyes que rigen el
comportamiento de los sistemas macroscópicos a partir
de una descripción microscópica de los mismos. Es decir, la Mecánica Estadística considera a los sistemas constituidos por un gran número de partículas cuyo comportamiento viene regido por las leyes de la Mecánica, y trata de obtener a partir de esa descripción las leyes fenomenológicas de la Termodinámica, el Magnetismo, etc.\\ \\
Parece que lo que debemos hacer para obtener estos resultados será resolver las ecuaciones del movimiento para el sistema. Sin embargo, la posibilidad de realizar este cálculo es imposible si recordamos que un sistema macroscópico contiene un número de partículas que es del orden de $10^{23}$ partículas, que es orden del número de Avogadro, $6.022 140 76 \times10^{23}$ mol$^{-1}$. \cite{avogadro}\\ \\

Por otro lado, es lógico ver que no necesitamos tal nivel de información para nuestros fines. La propia generalidad de las leyes fenomenológicas nos indica que deben de ser válidas para una gran variedad de modelos e independientes de las condiciones iniciales mecánicas que se escojan. De manera matemática, un sistema de $N$ partículas exige para su especificación el conocimiento de las posiciones y velocidades de cada una de ellas, es decir, en general $6N$ parámetros distintos, mientras que el estado macroscópico de un sistema se caracteriza por un pequeño número de parámetros, como son la presión, la temperatura, el volumen, etc. Luego, es evidente que al pasar de la escala microscópica a la macroscópica se efectúa una contracción en la descripción del sistema, seleccionando parte de la información contenida en la descripción microscópica.\\ \\

Vamos a repasar algunas nociones de la mecánica clásica que usaremos bastante en esta asignatura.\footnote{Esto no lo ha dicho Alcaraz, pero me parece útil explicarlo.} \\ \\

En un sistema clásico, para determinar la posición en un instante dado de $N$ partículas puntuales, son necesarias en principio $3N$ magnitudes, que pueden ser, por ejemplo, las coordenadas cartesianas de cada partícula. Sin embargo, si entre estas $3N$ coordenadas cartesianas pueden establecerse $m$ ecuaciones que representen restricciones o ligaduras, resultará que solo $3N-m$ magnitudes son independientes. Se dice que el sistema posee $3N-m$ grados de libertad. Así pues, denominaremos \textbf{grados de libertad de un sistema} al número de parámetros necesarios y suficientes para fijar la posición de todos los puntos del sistema en cualquier instante. Cada conjunto de coordenadas independientes del sistema en número igual a los grados de libertad del mismo, constituye un conjunto de \textbf{coordenadas generalizadas} $\left\lbrace q_i\right\rbrace$. A las derivadas de las coordenadas generalizadas respecto al tiempo se las denomina \textbf{velocidades generalizadas}. Por su propia definición, es claro que el estado dinámico de un sistema dado está totalmente especificado mediante los valores de un conjunto de coordenadas y velocidades generalizadas.\\ \\
Es útil en algunos casos, introducir para un sistema con $f$ grados de libertad un espacio de $f$ dimensiones definido mediante un conjunto de coordenadas generalizadas $\left\lbrace q_1,\dots,q_f \right\rbrace$. Este espacio recibe el nombre de \textbf{espacio de configuración}. Evidentemente, si el sistema está constituido por $N$ partículas puntuales, tendremos $f=3N$ y el espacio de configuración puede definirse mediante un conjunto de coordenadas cartesianas de las partículas del sistema.\\ \\
Si consideramos ahora un sistema con $f$ grados de libertad y suponemos que somos capaces de expresar la energía cinética $T$ y la energía potencial $U$ en función de un conjunto de coordenadas generalizadas y las correspondientes velocidades generalizadas, se define la función Lagrangiana como
\begin{equation}
    \mathcal{L}(\left\lbrace q_i\right\rbrace,\left\lbrace\dot{q}_i\right\rbrace,t)=T-U
\end{equation}

de forma que, bajo ciertas hipótesis bastantes generales, las ecuaciones del movimiento del sistema se pueden escribir como

\begin{equation}
    \frac{\partial\mathcal{L}}{\partial q_i}-\frac{d}{dt}\frac{\partial\mathcal{L}}{\partial\dot{q}_i}=0\hspace{10mm}i=1,2,\dots,f
\end{equation}

ecuaciones que se conocen como ecuaciones de Euler-Lagrange.\\ \\
Para algunos desarrollos teóricos y, concretamente, para la formulación de la Mecánica Estadística, es conveniente introducir unas nuevas variables que denominaremos \textbf{cantidades de movimiento generalizadas} y una nueva función que recibe el nombre de \textbf{Hamiltoniano} del sistema.\\ \\
A partir de cada velocidad generalizada, definimos una cantidad de movimiento generalizada $p_i$ mediante la relación
\begin{equation}
    p_i=\frac{\partial\mathcal{L}}{\partial\dot{q}_i}
\end{equation}
y, a partir de la Lagrangiana, definimos el Hamiltoniano $H$ como

\begin{equation}
    H(\left\lbrace q_i\right\rbrace,\left\lbrace\dot{q}_i\right\rbrace,t)=\sum_{i=1}^fp_i\dot{q}_i-\mathcal{L}
\end{equation}

donde hemos indicado explícitamente que, en la descripción hamiltoniana, el estado dinámico del sistema se define mediante las variables $q_i$ y $p_i$. A partir del Hamiltoniano, las ecuaciones del movimiento son ahora las conocidas ecuaciones de Hamilton, es decir, el conjunto de $2f$ ecuaciones

\begin{equation}
    \dot{q}_i=\frac{\partial H}{\partial p_i}\hspace{10mm}\dot{p}_i=-\frac{\partial H}{\partial q_i}
    \label{ec5}
\end{equation}

Una propiedad muy importante del Hamiltoniano es la igualdad de sus derivadas parcial y total respecto al tiempo, es decir,

\begin{equation}
    \frac{dH}{dt}=\frac{\partial H}{\partial t}
\end{equation}

de forma que si el Hamiltoniano no depende explícitamente del tiempo, es una constante del movimiento. Otra propiedad relevante, es que si $U(q,t)\neq U(q,\dot{q},t)$ entonces el Hamiltoniano es
\begin{equation}
    H=T+U
\end{equation}
y el Hamiltoniano coincidirá con la energía del sistema.\\ \\
De igual forma que las coordenadas generalizadas $\left\lbrace q_i\right\rbrace$ pueden utilizarse para construir el espacio de configuración, las cantidades de movimiento generalizadas pueden usarse para formar un \textbf{espacio de cantidades de movimiento}, también de $f$ dimensiones. Introducimos entonces un correspondiente espacio de $2f$ dimensiones que corresponden a $f$ coordenadas generalizadas y a sus correspondientes $f$ cantidades de movimiento generalizados. Este espacio es denominado \textbf{espacio de fases}. Un punto de este espacio determina de forma única el estado dinámico del sistema, y su evolución temporal vendrá representada por una curva en dicho espacio de fases.

\section{Descripción estadística de los sistemas
macroscópicos}

Entre la descripción macroscópica del estado de un sistema físico y la descripción microscópica, existen diferencias muy importantes. La descripción macroscópica de un sistema se hace mediante un número muy reducido de parámetros; concretamente la Termodinámica admite que un estado de equilibrio queda totalmente especificado, por ejemplo, mediante los valores de los parámetros externos del sistema y de la temperatura (P, V, T, ...). Por otro lado, para la descripción microscópica debemos especificar el estado microscópico del sistema nos serán necesarios $6N$ parámetros, que teniendo en cuenta sistemas poco densos, como los gases, el número de partículas que componen un sistema macroscópico es del orden de $10^{23}$.\\ \\

Es claro, que si partimos de los microestados, o estados microscópicos del sistema, podremos llegar a saber el macroestado, o estado macroscópico del sistema. Pero la inversa no es verdad, pues un mismo macroestado puede partir de diferentes microestados.\\ \\
Para saber en cuál microestado está el sistema a partir de un macroestado, deberemos asignar probabilidades a los microestados.\\ \\

Utilizaremos la mecánica de Hamilton para describir los estados.\\ \\
Partimos de un sistema de $N$ partículas. Describiremos un sistema clásico usando las coordenadas del espacio fásico $(q_1,q_2,\dots,q_N,p_1,p_2,\dots,p_N)$. Cada microestado corresponde a un conjunto $\left\lbrace q,p\right\rbrace$, es decir, un microestado es un punto del espacio fásico.\\ \\

Definimos una función, que no tiene sentido físico, en el espacio fásico como


    \[\rho(q,p;t)\hspace{10mm}\begin{matrix}
    q=q_1,q_2,\dots,q_N\\
    p=p_1,p_2,\dots,p_N
    \end{matrix}\]

tal que, la densidad de probabilidad es

\begin{equation}
    \rho(q,p,;t)dqdp\equiv \rho(q_1,\dots,q_N,p_1\dots,p_N;t)dq_1\cdot\dots\cdot dq_N\cdot dp_1\cdot\dots\cdot dp_N
\end{equation}

que sí tiene sentido físico, pues es la probabilidad de que en un instante $t$ dado, las coordenadas $q_i$ y cantidades de movimiento $p_i$ del sistema tengan valores comprendidos en los intervalos $q_i+dq_i$ y $p_i+dp_i$.\\ \\

La relación entre las propiedades microscópicas y as propiedades macroscópicas de un sistema viene dado por $U$, que es la energía interna de un sistema y representa todas las formas de energía que tiene un sistema. \\ \\

Consideramos que

\begin{equation}
    U=\sum_{i=1}^N\epsilon_i
\end{equation}

siendo $\epsilon_i$ la energía de cada partícula del sistema. Usando esto, es fácil ver que el Primer Principio de la Termodinámica equivale al Principio de conservación de la energía. Luego, tendremos sistemas en los que la energía se conserva.

\section{Primer postulado de la Mecánica Estadística}

El Primer postulado dice:
\begin{center}
\textit{"Los valores de los parámetros macroscópicos que definen el estado de un sistema son iguales a los valores medios, sobre el conjunto de microestados asociados, de la correspondiente magnitud microscópica"}
\end{center}

De forma matemática podemos verlo considerando una variable que es función de las coordenadas $(q,p)$, tal que $A(q,p)$. A cada macroestado le corresponde un número muy grande de microestados en los que, en general, los valores de $p$ y $q$ son distintos, por lo tanto también lo es $A(q,p)$. Por el primer postulado, el valor de $A$ en el sistema macroscópico en un instante $t$ es

\begin{equation}
    A_{macroscopico}=\overline{A}=\int dqdp\rho(q,p;t)A(q,p)
\end{equation}

Esta regla solo es válida cuando el parámetro macroscópico considerado tiene significado sobre la escala microscópica, es decir, a nivel de partícula. Por tanto, debemos buscar magnitudes propias que se puedan relacionar con los $(q,p)$.

\section{Colectividades y fluctuaciones}

Se define un \textbf{sistema aislado} como aquel que no interacciona de ninguna manera con el exterior, de forma que no hay ningún tipo de intercambio de energía ni de materia. Un \textbf{sistema cerrado} es el que puede intercambiar energía con sus alrededores, pero no materia. Finalmente, un \textbf{sistema abierto} en el que puede intercambiar, tanto energía como materia, con el entorno.\\ \\

Para que un conjunto de microestados sean compatibles con un macroestado tendremos dos casos:

\begin{enumerate}[Caso a)]
    \item 
    Todos los microestados tienen el mismo valor de la propiedad física, es decir, asignamos un valor nulo para la probabilidad asociada a los microestados que corresponden a una propiedad distinta de la del sistema macroscópico.
    \item 
    Los microestados tienen distintos valores de la propiedad física pero con una distribución de probabilidad adecuada.
\end{enumerate}

Ambos conjuntos o \textbf{colectividades} representan situaciones físicas en parte coincidentes y en parte discrepantes.\\ \\

Si lo que nos interesa es el estudio de las leyes que relacionan magnitudes macroscópicas en el equilibrio, la forma de estas leyes será independiente de la manera en que se haya alcanzado el equilibrio e incluso de cómo se mantenga. También es de esperar que sean independientes de la forma de la función de distribución escogida. Es decir, para la propiedad física de equilibrio, podremos usar tanto el Caso a), como el Caso b). Además, para el equilibrio en detalle se podrá usar el Caso a) (siempre que sea un sistema aislado) y el Caso b) (en cualquier sistema).\\ \\

Las \textbf{fluctuaciones} son oscilaciones del valor de una magnitud macroscópica alrededor de un valor dado, tal que
\begin{equation}
    \Delta A=\sqrt{\overline{(A^2)}-(\overline{A})^2}
\end{equation}

tal que

\begin{equation}
    A=\overline{A}\pm r\cdot\Delta A
\end{equation}

siendo $r$ un número del orden de la unidad.\\ \\
Si las fluctuaciones son muy pequeñas, es decir $\frac{\Delta A}{\overline{A}}\ll$ ($10^{-2,-3}$ dependiendo del experimento o de las unidades de la magnitud empleada), entonces tendremos una ley física para el comportamiento de la magnitud $A$, en el sentido de que podemos predecir casi con certeza absoluta su valor verdadero. Por el contrario, si no es despreciable frente a $\overline{A}$, el conocimiento de $\overline{A}(t)$ no representa ninguna ley física, ya que no podemos predecir el verdadero valor de $A$, es decir, el valor que se va a obtener no sería reproducible, teniendo en este caso simplemente una ley estadística para valores medios.

\section{Ecuación de Liouville}

Buscamos calcular una forma de determinar la densidad de probabilidad. La probabilidad de que nuestro sistema, en un instante $t$, se encuentre en un punto del espacio fásico (microestado) que esté comprendido dentro del volumen $V$ es

\begin{equation}
    \int_V\rho(q_1,q_2,\dots,q_N,p_1,p_2,\dots,p_N;t)dq_1\cdot dq_2\cdot\dots\cdot dq_N\cdot dp_1\cdot dp_2\cdot\dots\cdot dp_N
\end{equation}

Si tenemos un objeto como en la Figura \ref{figura1}, con un flujo de densidad de probabilidad a través de $S$, entonces

\begin{equation}
    \frac{d}{dt}\int_V\rho(q,p;t)dqdp=-\int_S\rho(q,p;t)\vec{v}d\vec{S}
    \label{ec1-14}
\end{equation}
siendo $\vec{v}$ la velocidad y $d\vec{S}$ un vector unitario perpendicular a la superficie que apunta hacia el exterior del volumen, usando el signo negativo para que el flujo sea positivo cuando este sale hacia fuera. En un espacio normal (cartesiano), $\vec{v}\equiv(\dot{x},\dot{y})$ y en el espacio fásico, $(\dot{q},\dot{p})$.

\begin{Figura}
    \centering
    \includegraphics[width=0.7\textwidth]{liouville.png}
    \captionof{figure}{Representación de un sistema de volumen $V$ encerrado en una superficie $S$ con un flujo de probabilidad.}
    \label{figura1}
\end{Figura}

\subsubsection*{Teorema de Gauss}

Recordamos que el Teorema de Gauss era

\begin{equation}
    \int_S\vec{F}\cdot d\vec{S}=\int_V\vec{\nabla}\vec{F}dV
\end{equation}

siendo $\int_S$ una integral de superficie cerrada y $\int_V$ una integral de volumen.\\ \\

Aplicamos el Teorema de Gauss a nuestro problema y obtenemos

\begin{equation}
    \int_S\rho(q,p;t)\vec{v}\cdot d\vec{S}=\int_V\vec{\nabla}(\rho\vec{v})dV
\end{equation}

Igualamos este resultado a la integral de volumen de la ecuación \ref{ec1-14}, tal que

\begin{equation}
    \frac{d}{dt}\int_V\rho(q,p;t)dqdp=-\int_V\vec{\nabla}(\rho\vec{v})dqdp
\end{equation}

usamos $dV=dqdp$, pues este es el hipervolumen fundamental del espacio fásico.\\ \\

Seguimos operando,

\begin{equation}
    \int_V\left[\frac{\partial}{\partial t}\rho(q,p;t)+\vec{\nabla}(\rho\vec{v})\right]dqdp=0
\end{equation}
que se cumple si y solo si el interior de la integral es cero, es decir,
\begin{equation}
    \frac{\partial}{\partial t}\rho(q,p;t)+\vec{\nabla}(\rho\vec{v})=0
    \label{ec19}
\end{equation}
siendo esta la \textbf{Ecuación de continuidad}. Recuerdo que lo que buscamos calcular es $\rho$.\\ \\

Calculamos cuánto vale $\vec{\nabla}(\rho\vec{v})$

\begin{equation}
    \vec{\nabla}(\rho\vec{v})=(\vec{\nabla}\rho)\cdot\vec{v}+\rho\cdot(\vec{\nabla}\vec{v})
    \label{ec20}
\end{equation}

además, el operador nabla, $\vec{\nabla}$, en el espacio fásico viene dado por

\begin{equation}
    \vec{\nabla}=\left(\frac{\partial}{\partial q},\frac{\partial}{\partial p}\right)
    \label{ec21}
\end{equation}

Desarrollamos la ecuación \ref{ec19}, sustituyendo la ecuación \ref{ec20} con la notación de la \ref{ec21}, tal que

\begin{equation}
    \frac{\partial\rho}{\partial t}+\sum_{i=1}^f\left(\frac{\partial\rho}{\partial q_i}\dot{q}_i+\frac{\partial\rho}{\partial p_i}\dot{p}_i\right)+\rho\sum_{i=1}^f\left(\frac{\partial\dot{q}_i}{\partial q_i}+\frac{\partial\dot{p}_i}{\partial p_i}\right)=0    
\end{equation}

siendo $f$ el número de grados de libertad del sistema.\\ \\

Recordando las ecuaciones de Hamilton del movimiento, ecuación \ref{ec5}, si las derivamos respecto a $q$ y respecto a $p$ respectivamente, obtenemos

\begin{equation}
    \frac{\partial\dot{q}}{\partial q}=\frac{\partial^2H}{\partial p\partial q}\hspace{10mm}\frac{\partial\dot{p}}{\partial p}=-\frac{\partial^2H}{\partial q\partial p}
\end{equation}

que sustituyendo en la ecuación anterior, vemos que el tercer sumando se anula, pues suponemos que se satisface el Teorema de Schwarz, por tanto estamos restando dos magnitudes iguales que no importa que estén dentro del sumatorio. Luego, nos queda

\begin{equation}
     \frac{\partial\rho}{\partial t}+\sum_{i=1}^f\left(\frac{\partial\rho}{\partial q_i}\dot{q}_i+\frac{\partial\rho}{\partial p_i}\dot{p}_i\right)=0
     \label{ec24}
\end{equation}

pero esta ecuación es la derivada total de $\rho$ respecto al tiempo, por tanto

\begin{equation}
    \frac{\partial\rho}{\partial t}+\sum_{i=1}^f\left(\frac{\partial\rho}{\partial q_i}\dot{q}_i+\frac{\partial\rho}{\partial p_i}\dot{p}_i\right)=\frac{d\rho}{dt}=0
\end{equation}

luego, se debe satisfacer

\begin{equation}
    \rho(q,p;t)=\rho(q(t+\tau),p(t+\tau);t+\tau)
    \label{ec26}
\end{equation}

siendo esta la conocida \textbf{Ecuación de Liouville}.

Por otro lado, si en la ecuación \ref{ec24} sustituimos las ecuaciones del movimiento de Hamilton (ecuación \ref{ec5}), nos queda

\begin{equation}
    \frac{\partial\rho}{\partial t}+\sum_{i=1}^f\left(\frac{\partial\rho}{\partial q_i}\frac{\partial H}{\partial p}-\frac{\partial\rho}{\partial p_i}\frac{\partial H}{\partial q}\right)=0
\end{equation}

donde es fácil ver que

\begin{equation}
    \sum_{i=1}^f\left(\frac{\partial\rho}{\partial q_i}\frac{\partial H}{\partial p}-\frac{\partial\rho}{\partial p_i}\frac{\partial H}{\partial q}\right)=\left[\rho,H\right]
\end{equation}

siendo esto el Corchete de Poisson de $\rho$ y $H$. Luego,

\begin{equation}
    \frac{\partial\rho}{\partial t}+\left[\rho,H\right]=0\Longrightarrow
    \frac{\partial\rho}{\partial t}=-\left[\rho,H\right]\Longrightarrow
    \frac{\partial\rho}{\partial t}=\left[H,\rho\right]
\end{equation}

Volviendo a la ecuación de Liouville, tendremos:

\[\begin{matrix}
    \text{En }t:&\hspace{2mm}\rho(q(t),p(t),;t)dq(t)dp(t)\\ \\
    \text{En }t+\tau:&\hspace{2mm}\rho(q(t+\tau),p(t+\tau);t+\tau)dq(t+\tau)dp(t+\tau)
\end{matrix}\]

Luego, se deberá cumplir

\begin{equation}
    \rho(q(t+\tau),p(t+\tau);t+\tau)dq(t+\tau)dp(q+\tau)=\rho(q,p;t)dqdp
\end{equation}

Por el Teorema de Liouville, que es la ecuación \ref{ec26}, se debe satisfacer

\begin{equation}
    dq(t+\tau)dp(t+\tau)=dqdp
\end{equation}

dando un resultado importante, y es que el volumen fásico se conserva en la evolución de los sistemas. Esta igualdad es, como sabemos, equivalente a escribir que el jacobiano de la transformación vale la unidad, o sea que

\begin{equation}
    \frac{\partial\left[q(\tau),p(\tau)\right]}{\partial (q,p)}=1\Leftrightarrow \abs{\mathcal{J}}=1
\end{equation}

siendo $\abs{\mathcal{J}}$ el Jacobiano.\\ \\

No podemos dejar de recalcar que este es un resultado conocido de la Mecánica Clásica. Pues el Jacobiano vale la unidad en todas las transformaciones canónicas, y la evolución temporal de las coordenadas fásicas de un sistema puede representarse como una transformación canónica.

\section{Ecuación de Liouville: soluciones estacionarias}

Tenemos la ecuación de Liouville en forma de corchete,

\begin{equation}
    \frac{\partial\rho}{\partial t}=\left[H,\rho\right]
\end{equation}

Buscamos soluciones que cumplan $\frac{\partial\rho}{\partial t}=0$, pues buscamos situaciones que estén en el equilibrio. Además, si se cumple esto, también se cumplirá $\left[H,\rho\right]=0$, cosa que implica que $\rho$ deba ser una constante del movimiento.\\ \\

Recordamos que hay una propiedad de los corchetes de Poisson que era

\begin{equation}
    \left[f,g(f)\right]=0
\end{equation}

Por tanto, buscaremos funciones de $\rho$ que dependan de $H$, tal que

\begin{equation}
    \rho(q,p)=\rho\left[H(q,p),\xi_1(q,p),\xi_2(q,p),\dots\right]
\end{equation}

tal que

\begin{equation}
    \left[H(q,p),\xi_i(q,p)\right]=0
\end{equation}

La mecánica estadística busca postular formas de la densidad de probabilidad para caracterizar y describir los sistemas físicos.\\ \\

Para tener sistemas en equilibrio, buscaremos parámetros macroscópicos independientes del tiempo, así $\rho(q,p)\neq\rho(t)$.

\section{Segundo postulado de la Mecánica Estadística}

\begin{center}
"\textit{A un estado de equilibrio macroscópico de un \textbf{sistema aislado} corresponde una
descripción microscópica en la que todos los \textbf{microestados} accesibles al
sistema son \textbf{igualmente probables}}".
\end{center}

Este postulado establece una igualdad de probabilidades a priori.

\chapter{TEMA 2: Colectividades de equilibrio: microcanónica, canónica y macrocanónica}

\section{Colectivo microcanónico: sistemas
aislados en equilibrio}

Vamos a limitarnos a partir de ahora a sistemas que se encuentren en equilibrio. Por \textit{equilibrio} en Mecánica Estadística se entiende que la función de distribución de probabilidades $\rho(q,p)$ es independiente del tiempo. Esta definición implica que al pasar de una definición macroscópica todos los parámetros que definen el estado del sistema serán independientes del tiempo, de acuerdo con el concepto de equilibrio termodinámico. \footnote{Pg 15 del Brey.}\\ \\

Vamos a considerar un sistema con energía comprendida entre $E$ y $E+\Delta E$, haciendo $\Delta E\rightarrow0$, es decir, podemos hacer $\Delta E$ todo lo pequeño que queramos. Por el segundo principio, vamos a asignar la misma densidad de probabilidad a todos los microestados disponibles del sistema, tal que

\[\rho(q,p,t)\neq\rho(t)\Rightarrow\left\lbrace\begin{matrix}
    C \text{ si } E\leq H\leq E+\Delta E\\
    0 \text{ en cualquier otro caso}
\end{matrix}\right.\]

es cero en cualquier otro caso porque los microestados no serán accesibles.\\ \\

Como cada microestado corresponde a un punto del espacio fásico, si cuando cogemos un microestado $H\in\left[E,E+\Delta E\right]$, será $C$ y en otro caso será cero.\\ \\

Esto lo expresamos matemáticamente como

\begin{equation}
    \rho(q,p)=\frac{1}{\Omega(E)}\delta\left[E-H(q,p)\right]
\end{equation}
tal que $\frac{1}{\Omega(E)}\equiv C$ y $\delta$ es la Delta de Dirac.\footnote{$\delta(x)$ está definida en el origen, donde vale infinito y en el resto del espacio vale cero. Cumple la propiedad $\int_{-\infty}^{+\infty}\delta(x)dx=1$}

Además, debe cumplirse la condición

\begin{equation}
    \int dqdp\rho(q,p)=1
\end{equation}

donde integramos en todo el espacio fásico. Esta condición significa que estamos buscando la probabilidad de que exista en algún punto del espacio fásico y como debe existir, la probabilidad será 1. Así podemos hacer lo siguiente,

\begin{equation}
    \int dqdp\frac{1}{\omega(E)}\delta\left[E-H(q,p)\right]=1\Rightarrow\frac{1}{\Omega(E)}\int dqdp\delta\left[E-H(q,p)\right]=1\Rightarrow\Omega(E)=\int dqdp\delta\left[E-H(q,p)\right]
    \label{ec2-3}
\end{equation}

donde $\Omega(E)$ representa el número de microestados accesibles al sistema.\\ \\
\\ \\
El conjunto de microestados con una densidad de probabilidad dada por


    \[\rho(q,p)=\frac{1}{\Omega(E)}\delta\left[E-H(q,p)\right]\]

se denomina \textbf{colectivo microcanónico}. Esta distribución de probabilidad se denomina \textbf{distribución microcanónica} y describe sistemas aislados en equilibrio.\\ \\

Se cumple que $\rho(q,p)=\rho(E)$ y por tanto, esta densidad de probabilidad cumple el Teorema de Liouville.\\ \\

Definimos $\Gamma(E)$ como

\begin{equation}
    \Gamma(E)=\int_{E_0}^E dE'\Omega(E')
    \label{ec4}
\end{equation}

con $E'$ una variable de integración y $E_0$ la energía mínima que puede tener el sistema.\\ \\

Sustituyendo la ecuación \ref{ec2-3} en la ecuación anterior, tenemos

\begin{equation}
     \Gamma(E)=\int_{E_0}^E dE'\int dqdp\delta\left[E-H(q,p)\right]=\int dqdp\int_{E_0}^EdE'\delta\left[E-H(q,p)\right]=\int_{E_0< H< E}dqdp
\end{equation}
donde la integral es distinta de cero si $H\in\left[E_0,E\right]$ y $dqdp$ representa el elemento de volumen del espacio fásico, por tanto esta integral representa el volumen fásico. Que será el volumen comprendido entre las hiper-superficies $E_0=H(q,p)$ y $E=H(q,p)$, como se muestra en la Figura \ref{fig1}.

\begin{Figura}
    \centering
    \includegraphics[width=0.6\textwidth]{Captura de pantalla 2023-10-05 225152.png}
    \captionof{figure}{Representación del volumen fásico (color verde) comprendido entre dos hiper-superficies (líneas rojas) comprendidas entre $E_0$ y $E$.}
    \label{fig1}
\end{Figura}

$\Gamma(E)$ y $\Omega(E)$ están relacionados, pues matemáticamente, un volumen es una forma de 'contar' puntos.\\ \\
Partimos de,

\[\begin{matrix}
    H(q,p)=E & \Rightarrow & \Gamma(E)\\
    H(q,p)=E+\Delta E & \Rightarrow & \Gamma(E+\Delta E)
\end{matrix}\]

Como $\Delta E\lll$, hacemos un desarrollo en serie de potencias, tal que

\[\Gamma(E+\Delta E)=\Gamma(E)+\frac{\partial\Gamma}{\partial E}dE+\dots\]
\[\Gamma(E+\Delta E)-\Gamma(E)=\frac{\partial\Gamma}{\partial E}dE+\dots\]

tal que $\frac{\partial \Gamma}{\partial E}=\Omega(E)$, por la ecuación \ref{ec4}. Por tanto,

\begin{equation}
    \Gamma(E+\Delta E)-\Gamma(E)=\Omega(E)dE
\end{equation}

Luego, $\Omega(E)dE$ representa el volumen de espacio fásico encerrado entre dos hiper-superficies muy próximas que corresponden a energías $E$ y $E+\Delta E$ constantes. Además, $\Omega(E)$ puede interpretarse como una medida del \textbf{área de la hiper-superficie} de energía $E=H(q,p)$. Esto es debido a un teorema matemático que dice que el volumen comprendido entre dos superficies separadas una distancia $dR$, viene dado por el producto $\sigma(R)dR$, donde $\sigma(R)$ es el área de la superficie.\\ \\

Vemos que $\Omega(E)$ y $\Gamma(E)$ dependen de la energía, cuya forma depende de la forma del sistema, es decir, del Hamiltoniano que tengamos. Esta independencia es de la forma:

\begin{equation}
    \Omega(E)\propto E^{\nu f}
\end{equation}

\begin{equation}
    \Gamma(E)\propto E^{\nu' f}
\end{equation}

donde $\nu$ y $\nu'$ son números del orden de la unidad y $f$ es el número de grados de libertad. Luego, crecen muy rápido con la energía.

\section{Conexión entre la Mecánica Estadística
y la Termodinámica}

Consideramos dos sistemas aislados y en equilibrio $A$ y $A'$, los juntamos en $A+A'$, imponiendo que este nuevo sistema sea aislado. Ver Figura \ref{fig2}.

\begin{Figura}
    \centering
    \includegraphics[width=0.8\textwidth]{Captura de pantalla 2023-10-07 194749.png}
    \captionof{figure}{Representación de la fusión de dos sistemas aislados $A$ y $A'$ en uno.}
    \label{fig2}
\end{Figura}

Consideremos varios casos:\\ \\

-Consideremos primero que la interacción entre $A$ y $A'$ es térmica, es decir, que solo hay intercambio de energía pero no de materia.\\ \\
Si conocemos $\rho(q,p)$ para cada sistema $A$ y $A'$ antes de la interacción y después de ella, podemos calcular la variación de la energía media de cada sistema $\Delta \overline{E}$ y $\Delta\overline{E'}$. Tal que

\[\rho_A(q,p)\longrightarrow \Delta\overline{E}=Q\]
\[\rho_{A'}(q,p)\longrightarrow \Delta\overline{E'}=Q'\]

Por ser un sistema aislado se debe cumplir que

\begin{equation}
    \Delta\overline{E}+\Delta\overline{E'}=0
\end{equation}

donde $\Delta\overline{E}$ es el calor absorbido, $\Delta\overline{E'}$ es el calor cedido y $\Delta\overline{E}$ y $\Delta\overline{E'}$ deben tener signo contrario, así $Q=-Q'$. Además, en termodinámica $\Delta\overline{E}$ representa la energía interna del sistema $U$.\\ \\

-Ahora consideremos otro sistema $A+A'$ aislado, permitiendo que haya una interacción entre $A$ y $A'$ mecánica, pero no térmica. Ver Figura \ref{fig3}.

\begin{Figura}
    \centering
    \includegraphics[width=0.4\textwidth]{Captura de pantalla 2023-10-07 202443.png}
    \captionof{figure}{Representación de la fusión de dos sistemas aislados $A$ y $A'$ en uno.}
    \label{fig3}
\end{Figura}

Análogamente al caso anterior, podremos calcular $\Delta\overline{E}$ y $\Delta\overline{E'}$, tal que

\[\rho_A(q,p)\longrightarrow \Delta\overline{E}=W\]
\[\rho_{A'}(q,p)\longrightarrow \Delta\overline{E'}=W'\]

Por ser un sistema aislado se debe cumplir que

\begin{equation}
    \Delta\overline{E}+\Delta\overline{E'}=0
\end{equation}

donde $\Delta\overline{E}$ es el trabajo absorbido y $\Delta\overline{E'}$ es el trabajo cedido y ambos deben ser de signos contrarios, tal que $W=-W'$.\\ \\

-Por último, consideramos el caso más general, es decir, que ambos sistemas interaccionen mecánica y térmicamente. Ver Figura $\ref{fig4}$.

\begin{Figura}
    \centering
    \includegraphics[width=0.4\textwidth]{Captura de pantalla 2023-10-07 203554.png}
    \captionof{figure}{Representación de la fusión de dos sistemas aislados $A$ y $A'$ en uno.}
    \label{fig4}
\end{Figura}

En este sistema varían los parámetros externos y además ambos sistemas intercambian energía en forma de calor. En este caso escribiremos para la variación de energía de un sistema

\begin{equation}
    \Delta\overline{E}=(\Delta\overline{E})_{\text{térmica}}+(\Delta\overline{E})_{\text{mecánica}}
\end{equation}

o bien

\begin{equation}
    \Delta\overline{E}=Q-W
\end{equation}

donde ponemos el signo $'-'$ delante de $W$ porque si realizo un trabajo la energía disminuyo y si recibo trabajo, aumenta. Además, esta expresión constituye al \textit{Primer Principio de la Termodinámica}.

\subsection{Procesos cuasiestáticos}

Los procesos cuasiestáticos son interacciones suficientemente lentas como para considerar al sistema en equilibrio en todo instante.\\ \\

Todo sistema aislado tiende a una situación de equilibrio.\\ \\

El tiempo de relajación será el tiempo que se tarda en alcanzar la situación de equilibrio.\\ \\

\textbf{Podemos limitar zonas del espacio fásico con barreras de potencial}. Estas restricciones se verán reflejadas en el Hamiltoniano como parámetros, tal que $H=H(q,p,X_{\alpha})$, donde $X_{\alpha}$ son los parámetros externos.\\ \\

Consideremos que variamos $X_{\alpha}$ manteniendo el sistema térmicamente aislado, entonces habrá un cambio en el Hamiltoniano,

\[dH=\sum_{alpha}\frac{\partial H}{\partial X_{\alpha}}dX_{\alpha}\]

Tenemos que 

\begin{equation}
    d\overline{E}=\overline{dE}=\overline{dH}=\overline{\frac{\partial H}{\partial X_{\alpha}}}dX_{\alpha}=-dW
\end{equation}

Llamamos $Y_{\alpha}$ a la fuerza generalizada asociada al parámetro $X_{\alpha}$ tal que

\begin{equation}
    Y_{\alpha}=-\frac{\partial H}{\partial X_{\alpha}}
\end{equation}

Por tanto,

\begin{equation}
    dW=\overline{Y_{\alpha}}dX_{\alpha}
\end{equation}

\subsection{Reversibilidad e irreversibilidad}

Consideramos un sistema aislado donde todas las partículas del sistema están en una mitad, separadas de la otra mitad vacía por una pared. Cuando quitamos la pared, las partículas tenderán a ocupar el espacio vacío, ocupando todo el volumen. Ver Figura \ref{fig5}

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{aaaaa.jpg}
    \captionof{figure}{Representación de la ocupación de todo el volumen.}
    \label{fig5}
\end{Figura}

Los microestados posibles de \textbf{(2)} serán mayores o iguales que los microestados de \textbf{(1)}, pues obviamente en \textbf{(2)} hay un volumen mayor ocupado por las partículas que en \textbf{(1)}, tal que
\[\Gamma_f(E)\geq\Gamma_i(E)\hspace{5mm}\text{y}\hspace{5mm}\Omega_f(E)\geq\Omega_i(E)\]

Si se da la igualdad, diremos que el proceso será reversible y si no se cumple, será irreversible.

\subsection{Temperatura y entropía}

Partimos de $\Gamma(E,X_{\alpha})$ que dependerá de los parámetros externos $X_{\alpha}$.\\ \\

Debemos calcular $d\Gamma(E,X_{\alpha}$, pero para simplificar los cálculos lo haremos con el logaritmo, es decir, calculamos $d\ln{\Gamma(E,X_{\alpha})}$, tal que

\begin{equation*}
    d\ln{\Gamma(E,X_{\alpha})}=\left.\frac{\partial \ln{\Gamma}}{\partial E}\right|_{X_{\alpha}}dE+\left.\frac{\partial \ln{\Gamma}}{\partial X_{\alpha}}\right|_{E}=\frac{1}{\Gamma}\cancelto{\Omega(E)}{\left.\frac{\partial \Gamma}{\partial E}\right|_{X_{\alpha}}}dE+\frac{1}{\Gamma}\cancelto{-\Omega(E,X_{\alpha})\overline{\frac{\partial H}{\partial X_{\alpha}}}}{\left.\frac{\partial\Gamma}{\partial X_{\alpha}}\right|_{E}}dX_{\alpha}
\end{equation*}

\[\left\lbrace\begin{matrix}
    \left.\frac{\partial\Gamma}{\partial E}\right|_{X_{\alpha}} = \left.\frac{\partial}{\partial E}\left(\int_{E_0}^E dE\Omega(E)\right)\right|_{X_{\alpha}} = \Omega(E)\\ \\
    \left.\frac{\partial\Gamma}{\partial X_{\alpha}}\right|_{E}=\dots=-\Omega(E,X_{\alpha})\overline{\frac{\partial H}{\partial X_{\alpha}}}
\end{matrix}\right\rbrace\]

\[d\ln{\Gamma(E,X_{\alpha})}=\frac{\Omega(E,X_{\alpha}}{\Gamma(E,X_{\alpha}}dE-\frac{\Omega(E,X_{\alpha}}{\Gamma(E,X_{\alpha}}\overline{\frac{\partial H}{\partial X_{\alpha}}}dX_{\alpha}=\frac{\Omega(E,X_{\alpha}}{\Gamma(E,X_{\alpha}}\left(dE-\cancelto{-\overline{Y_{\alpha}}}{\overline{\frac{\partial H}{\partial X_{\alpha}}}}dX_{\alpha}\right)\]

\begin{equation}
    d\ln{\Gamma(E,X_{\alpha})}=\left.\frac{\partial\ln{\Gamma(E,X_{\alpha}}}{\partial E}\right|_{X_{\alpha}}(dE+\overline{Y_{\alpha}}dX_{\alpha})
\end{equation}

 donde $dE+\overline{Y_{\alpha}dX_{\alpha}}=dQ$, que es el calor, así

 \begin{equation}
    d\ln{\Gamma(E,X_{\alpha})}=\left.\frac{\partial\ln{\Gamma(E,X_{\alpha}}}{\partial E}\right|_{X_{\alpha}}dQ
    \label{ec17}
\end{equation}

En termodinámica tenemos 

\begin{equation}
    dS=\frac{dQ}{T}
\end{equation}

luego, podemos asociar esta ecuación con la ecuación \ref{ec17} y así darle un significado microcanónico a las magnitudes termodinámicas, así

\begin{equation}
    \left\lbrace\begin{matrix}
        S=\ln{\Gamma(E)}\\ \\
        \frac{1}{T}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X-{\alpha}}
    \end{matrix}\right\rbrace
\end{equation}

Pero vemos que hay un problema con las unidades, pues el logaritmo neperiano es una función sin unidades, para solucionar esto debemos agregar una constante $k$ con unidades de entropía, así las ecuaciones que usaremos serán

\begin{equation}
    \left\lbrace\begin{matrix}
        S=k\ln{\Gamma(E)}\\ \\
        \frac{1}{T}=k\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X-{\alpha}}\Rightarrow\beta=\frac{1}{kT}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X-{\alpha}}
    \end{matrix}\right\rbrace
\end{equation}

Vamos a darle significado físico:\\ \\
$\Gamma(E)$ es el volumen fásico comprendido entre las hiper-superficies $E$ y $E_0$, que representa el número de microestados accesibles al sistema. Luego, la entropía está relacionada con el número de microestados accesibles del sistema, considerando como 'desorden' un número elevado de microestados. Idealmente, si solo hubiera un microestado, la entropía sería cero. \\
La temperatura es cómo varían el número de microestados accesibles con la energía.\\ \\

Debemos comprobar que estas definiciones de entropía y temperatura satisfacen las propiedades de estas magnitudes que sí cumplen en la Termodinámica.\\ \\

Tenemos,

\[\left.\begin{matrix}
    d\ln{\Gamma(E)}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X_{\alpha}}(dE+\overline{Y_{X_{\alpha}}}dX_{\alpha})\\
    d\ln{\Gamma(E)}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X_{\alpha}}dE+\left.\frac{\partial\ln{\Gamma(E)}}{\partial X_{\alpha}}\right|_{E}dX_{\alpha}
\end{matrix}\right\rbrace\Rightarrow\left.\frac{\partial\ln{\Gamma(E)}}{\partial X_{\alpha}}\right|_{E}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X_{\alpha}}\overline{Y_{X_{\alpha}}}\]

con $\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X_{\alpha}}=\beta=\frac{1}{kT}$, por tanto

\begin{equation}
    \overline{Y_{X_{\alpha}}}=\frac{1}{\beta}\left.\frac{\partial\ln{\Gamma(E)}}{\partial X_{\alpha}}\right|_{E}\Longleftrightarrow\overline{Y_{X_{\alpha}}}=T\left.\frac{\partial S}{\partial X_{\alpha}}\right|_{E}
\end{equation}

Esta ecuación representa la \textbf{ecuación de estado} de un sistema.\\ \\

Cuando el número de partículas es muy grande, es decir, $N\ggg$,

\begin{equation}
    \ln{\Gamma(E)}\approx\ln{\Omega(E)}
\end{equation}

\begin{proof}
    (...)
\end{proof}

Así, podemos reescribir

\[\left\lbrace\begin{matrix}
    S=k\ln{\Gamma(E)}\Rightarrow S=k\ln{\Omega(E)}\\ \\
    \beta=\frac{1}{kT}=\left.\frac{\partial\ln{\Gamma(E)}}{\partial E}\right|_{X_{\alpha}}\Rightarrow \beta=\frac{1}{kT}=\left.\frac{\partial\ln{\Omega(E)}}{\partial E}\right|_{X_{\alpha}}\\ \\
    \overline{Y_{X_{\alpha}}}=\frac{1}{\beta}\left.\frac{\partial\ln{\Gamma(E)}}{\partial X_{\alpha}}\right|_{E}\Rightarrow\overline{Y_{X_{\alpha}}}=\frac{1}{\beta}\left.\frac{\partial\ln{\Omega(E)}}{\partial X_{\alpha}}\right|_{E}
\end{matrix}\right\rbrace\]

El problema de las unidades de $\Gamma(E)$ y $\Omega(E)$ viene dado porque, 

\[\Gamma(E)=\int_{E_o}^EdE'd\Omega(E');\hspace{5mm}\Gamma(E)=\int dqdp\]

entonces, las unidades de $\Gamma$ son

\[\brackets{\Gamma}=\brackets{q}^f\brackets{p}^f=(\brackets{q}\brackets{p})^f\]

donde $f$ son los grados de libertad y $\brackets{q},\brackets{p}$ son las unidades de $q$ y $p$ respectivamente, así $(\brackets{q}\brackets{p})^f$ tiene unidades de acción, y por tanto, $\Gamma$ tiene también unidades de acción, pero $\Gamma$ está dentro de un logaritmo, por lo que debe ser adimensional. Por tanto, debemos redefinir $\Gamma$, tal que

\begin{equation}
    \Gamma(E)=\frac{1}{h^f}\int{E_0\leq H\leq E}dqdp
\end{equation}
 y lo mismo para $\Omega(E)$, así

\begin{equation}
    \omega(E)=\int_{E_0\leq H\leq E}dqdp\delta\brackets{E-H(q,p)}
\end{equation}

siendo $h$ la constante de Planck, que tiene unidades de acción.

\subsection{Aditividad de la entropía}

Consideramos dos sistemas $A_1(q,p)$ y $A_2(Q,P)$, tal que el sistema $A_1$ viene descrito por el Hamiltoniano $H_1$ y el sistema $A_2$, por $H_2$, teniendo $A_1$, $f_1$ grados de libertado y $A_2$, $f_2$ grados de libertad. Ver Figura \ref{fig6}.

\begin{Figura}
    \centering
    \includegraphics[width=0.4\textwidth]{Captura de pantalla 2023-10-09 182055.png}
    \captionof{figure}{Representación de dos sistemas aislados $A$ y $A'$ separados por una pared diaterma.}
    \label{fig6}
\end{Figura}

Dejamos en contacto ambos sistemas por una pared diaterma, es decir, solo deja el traspaso de calor, no materia y consideramos que el sistema completo es aislado, $A=A_1+A_2$. El Hamiltoniano de $A$ será

\begin{equation}
    H(q,p,Q,P)=H_1(q,p)+H_2(Q,P)+H_{12}(q,Q,p,P)
\end{equation}

donde $H_{12}$ es el término que da cuenta de la interacción.\\ \\

Suponemos que $H_{1}(q,Q,p,P)\lll H_1(q,p),H_2(Q,P)$ tal que

\begin{equation}
    H(q,p,Q,P)\approx H_1(q,p)+H_1(Q,P)
\end{equation}

La distribución microcanónica para el sistema completo será

\begin{equation}
    \rho(q,Q,p,P)=\frac{1}{h^f\Omega(E)}\delta\brackets{E-H_1(q,p)-H_2(Q,P)}
\end{equation}

donde $f=f_1+f_2$.\\ \\

Queremos calcular la probabilidad de que $A$ esté en equilibrio tal que $A_1$ tenga energía comprendida entre $E_1$ y $E_1+dE_1$, sin importar cómo esté $A_2$. Para ello, hacemos la probabilidad de que $A_1$ tenga energía $E_1$ y $A_2$ tenga energía $E_2$, pero como $E_2$ nos da igual, sumamos todas las posibilidades anteriores para todos los casos de $A_2$. Así,

\begin{equation}
    \rho_{A_1}(q,p)=\int dQdP\rho(q,Q,p,P)
\end{equation}

Una vez obtenida esta suma, podremos calcular la probabilidad de que $A$ esté en equilibrio con la energía de $A_1$ entre $E_1$ y $E_1+dE_1$. Para ello,

\begin{equation*}
    \begin{matrix}
        \rho_{A_1}(q,p)=\int dQdP\rho(q,Q,p,P)=\int dQdP\frac{1}{h^f\Omega(E)}\delta\brackets{E-H_1(q,p)-H_2(Q,P)}=\\
        \frac{1}{h^f\Omega(E)}\int dQdP\delta\brackets{E-H_1(q,p)-H_2(Q,P)}=\curlybraces{\begin{matrix}
            \Omega_2(E)=\frac{1}{h^{f_2}}\int dQdP\delta\brackets{E-H_2(Q,P)}\\
            f-f_2=f_1
        \end{matrix}}=\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))
    \end{matrix}
\end{equation*}

Así,

\begin{equation}
    \rho_{A_1}(q,p)=\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))
    \label{ec29}
\end{equation}

donde $\Omega(E)$ representa todos los microestados posibles del sistema, $\omega_2(E-H_1(q,p))$ representa los microestados posibles de $A_2$ con energía $E-H_1$, es decir, los microestados posibles de $A_1$ con energía $E_1$.\\ \\

Así, la probabilidad de que $A$ esté en equilibrio con la energía de $A_1$ entre $E_1$ y $E_1+dE_1$ será

\begin{equation*}
    \begin{matrix}
        \omega(E_1)dE_1=\int_{E_1\leq H_1(q,p)\leq E_1+dE_1}dqdp\rho_{A_1}(q,p)=\int_{E_1\leq H_1(q,p)\leq E_1+dE_1}dqdp\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))=\\ \\
        \curlybraces{\text{consideramos }H_1\approx E_1\text{ para dejarlo constante}}\\ \\
        \frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-E_1)\int_{E_1\leq H_1(q,p)\leq E_1+dE_1}dqdp=\curlybraces{\Omega_1(E_1)dE_1=\frac{1}{h^{f_1}}\int_{E_1\leq H_1(q,p)\leq E_1+dE_1}dqdp}=\\ \\
        =\frac{1}{\Omega(E)}\Omega_2(E-E_1)\Omega_1(E_1)dE_1
    \end{matrix}
\end{equation*}

Así queda,

\begin{equation}
    \omega(E_1)dE_1=\frac{1}{\Omega(E)}\Omega_2(E-E_1)\Omega_1(E_1)dE_1
\end{equation}

tal que $\int\omega(E_1)dE_1=1$, pues la suma de todas las probabilidades debe dar 1, así

\begin{equation}
    \Omega(E)=\int\Omega_2(E-E_!)\Omega_1(E_1)dE_1
\end{equation}

Vamos a intentar ver cómo es el producto $\Omega_2(E-E_!)\Omega_1(E_1)$. Como sabemos que $\omega(E)$ crece mucho con la energía, entonces $\Omega_1(E_1)$ crece muy rápido y $\Omega_2(E-E_1)$ decrece muy rápido, como vemos en la Figura \ref{fig7}.

\begin{Figura}
    \centering
    \includegraphics[width=0.8\textwidth]{bbbb.jpg}
    \captionof{figure}{En la primea parte se representan las funciones $\omega_1(E_1)$ y $\Omega_2(E-E_1)$ y la segunda parte representa la función total $\Omega_2(E-E_1)\Omega_1(E_1)$.}
    \label{fig7}
\end{Figura}

Como la integral es el área bajo la curva de la función, nos podemos traer el origen más arriba y aproximar la función a un rectángulo. Ver Figura \ref{fig8}.

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{ccc.jpg}
    \captionof{figure}{Aproximación de la función por un rectángulo.}
    \label{fig8}
\end{Figura}

donde $\widetilde{E}_1$ es el valor más probable de $E_1$.\\ \\

Así, podemos hacer la aproximación

\begin{equation}
    \Omega(E)\approx\Omega_1(\widetilde{E}_1)\Omega_2(E-\widetilde{E}_1)n(\Delta^* E)
\end{equation}

Calculamos el logaritmo,

\[\ln{\Omega(E)}=\ln{\Omega_1(\widetilde{E}_1)}+\ln{\Omega_2(E-\widetilde{E}_1)}+\cancelto{0}{\ln{n \Delta^*E}}\]

que el último es cero, pues $\Omega\approx E^{f\nu}\rightarrow\ln{\Omega}=\ln{E^f\nu}$, como $n\lll$ y $\Delta^*E\lll$ lo despreciamos. Además, $E-\widetilde{E}_1=\widetilde{E}_2$ Luego,

\[\ln{\Omega(E)}=\ln{\Omega_1(\widetilde{E}_1)}+\ln{\Omega_2(\widetilde{E}_2)}\]

Luego, si $f\ggg$, entonces podemos hacer $\ln{\Omega(E)}\approx\ln{\Gamma(E)}$. Así,

\[\ln{\Gamma(E)}=\ln{\Gamma_1(\widetilde{E}_)}+\ln{\Gamma_2(\widetilde{E}_2)}\]

como $S=k\ln{\Gamma(E)}$, podemos hacer

\[k\ln{\Omega(E)}=k\ln{\Omega_1(\widetilde{E}_1)}+k\ln{\Omega_2(\widetilde{E}_2)}\]

\begin{equation}
    S=S_1+S_2
\end{equation}

luego, la entropía cumple la aditividad.

\subsection{Cómo alcanzar el equilibrio}

Considerando el caso de que solo haya equilibrio térmico.\\ \\

Partimos de,

\begin{equation}
    \omega(E_1)dE_1=\frac{1}{\Omega(E)}\Omega_2(E-E_1)\Omega_1(E_1)dE_1
\end{equation}

lo pasamos a logaritmo, para simplificar cálculos, derivamos e igualamos a cero,

\[\begin{matrix}
    \left.d\ln{\omega(E_1)}\right|_{E_1=\widetilde{E}_1}=0\\ \\

    \left[\left.d\ln{\Omega_1(E_1)}+d\ln{\Omega_2(E-E_1)}\right]\right|_{E=\widetilde{E}_1}=0\\ \\

    
\end{matrix}\]

 podemos desarrollar los diferenciales, tal que

 \[\begin{matrix}
     \left.d\ln{\omega(E_1)}\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial \ln{\omega(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}dE_1=0\\ \\

     \left[\left.d\ln{\Omega_1(E_1)}+d\ln{\Omega_2(E-E_1)}\right]\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial \ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}dE_1+\left.\frac{\partial \ln{\Omega_2(E-E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}\frac{\partial(E-E_1)}{\partial E_1}dE_1=0
 \end{matrix}\]

 usando que $E=E_1+E_2$,
 
 \[\left[\left.d\ln{\Omega_1(E_1)}+d\ln{\Omega_2(E-E_1)}\right]\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial \ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}dE_1+\left.\frac{\partial \ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2}\cancelto{-1}{\frac{\partial E_2}{\partial E_1}}dE_1=0\]

igualando,

\[\left.\frac{\partial \ln{\omega(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}\cancel{dE_1}=\left.\frac{\partial \ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}\cancel{dE_1}-\left.\frac{\partial \ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2}\cancel{dE_1}=0\]

\[\left.\frac{\partial \ln{\omega(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial \ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}-\left.\frac{\partial \ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2}=0\]

\[\left.\frac{\partial \ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial \ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2}\]

usando que $\beta=\frac{1}{kT}=\left.\frac{\partial\ln{\Omega(E)}}{\partial E}\right|_{E}$, 

\[\beta_1=\beta_2\]

y por tanto,

\[\frac{1}{\cancel{k}T_1}=\frac{1}{\cancel{k}T_2}\Rightarrow T_1=T_2\]

Así, el estado de equilibrio se alcanza cuando se tiene la misma temperatura, $T$.\\ \\

Considerando el caso general, es decir, hay intercambio de energía y variación de parámetros externos, donde la densidad de probabilidad viene dada por

\[\rho(q,Q,p,P)=\frac{1}{h^f\Omega(E,V)}\delta\brackets{E-H_1(q,p,V)-H_2(Q,P,V)}\]

con 

\[\omega(E_1,V_1)=\frac{1}{\Omega(E,V)}\Omega_2(E-E_1,V-V_1)\Omega_1(E_1,V_1)\]

Análogamente al desarrollo anterior, vemos que al sistema $A_1$ corresponden los valores más probables $\widetilde{E}_1$, $\widetilde{V}_1$ y al $A_2$, los valores $\widetilde{E}_2=E-\widetilde{E}_1$, $\widetilde{V}_2=V-\widetilde{V}_1$. Estos valores por definición son los que hacen máxima a $\omega(E_1,V_1)$ o equivalentemente a su logaritmo. Se cumplirá entonces,

\[\left.d\ln{\omega(E_1,V_1)}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}=0\]

que desarrollando el diferencial tenemos,

\[\left.d\ln{\omega(E_1,V_1)}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}=\left.\frac{\partial\ln{\Omega}}{\partial E}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}dE+\left.\frac{\partial\ln{\Omega}}{\partial V}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}dV=0\]

    Usando que $\beta=\left.\frac{\partial\ln{\Omega}}{\partial E}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}$ y $\overline{P}=\frac{1}{\beta}\left.\frac{\partial\ln{\Omega}}{\partial V}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}$, 

    \[\left.\frac{\partial\ln{\Omega}}{\partial E}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}dE+\left.\frac{\partial\ln{\Omega}}{\partial V}\right|_{
    E_1=\widetilde{E}_1,
    V_1=\widetilde{V}_1}dV=\beta dE+\beta\overline{P}dV\]

entonces

\[(\beta_1-\beta_2)dE_1+(\beta_1\overline{P}_1-\beta_2\overline{P}_2)dV_1=0\]

como $dE_1,dV_1\neq0$, entonces

\[\beta_1-\beta_2=0\hspace{5mm}\text{y}\hspace{5mm}(\beta_1\overline{P}_1-\beta_2\overline{P}_2=0)\]

entonces

\[\beta_1=\beta_2\hspace{5mm}\text{y}\hspace{5mm}\overline{P}_1=\overline{P}_2\]

es decir,

\[T_1=T_2\hspace{5mm}\text{y}\hspace{5mm}\overline{P}_1=\overline{P}_2\]

Por tanto, el estado de equilibrio se alcanza cuando se tiene la misma temperatura $T$ y la misma presión $\overline{P}$.

\section{Colectivo canónico: sistemas fásicos en equilibrio térmico con un foco}

Consideramos un sistema aislado $A=A_1+A_2$, ver Figura \ref{fig9}.

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{Captura de pantalla 2023-10-10 175245.png}
    \captionof{figure}{Representación del sistema $A=A_1+A_2$.}
    \label{fig9}
\end{Figura}

Tomamos $A_2$ mucho más grande que $A_1$, es decir, que $f_2\ggg f_1$. \\ \\

La distribución de probabilidad obtenida antes era

\[\rho_{A_1}(q,p)dqdp=\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))dqdp\]

que solo presenta valores apreciables en un entorno del punto dado por,

\begin{equation}
    \left.\frac{\partial\ln{\Omega_1(E_1)}}{\partial E_1}\right|_{E_1=\widetilde{E}_1}=\left.\frac{\partial\ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2=E-\widetilde{E}_1}
\end{equation}

como  

\[\Omega(E)\approx E^{\nu f}\Rightarrow\ln{\Omega(E)}\approx\nu f\ln{E}\Rightarrow\frac{\partial\ln{\omega(E)}}{\partial E}\approx\frac{\nu f}{E}\Rightarrow\]
\[\Rightarrow\frac{\nu f_1}{\widetilde{E}_1}\approx\frac{\nu f_2}{\widetilde{E}_2}\Rightarrow\frac{\widetilde{E}_1}{\widetilde{E}_2}\approx\frac{f_1}{f_2}\]

usando que $f_2\ggg f_1$,

\[ \frac{f_1}{f_2}\lll1\Rightarrow\widetilde{E}_2\ggg\widetilde{E}_1\]

Vamos a hacer un desarrollo en serie del $\Omega_2$, pero con el logaritmo $ln{\Omega_2}$, pues $\widetilde{E}_1\lll1\Rightarrow H_1\lll1$ comparado con $E$. Así,

\begin{equation}
    \ln{\Omega_2}\brackets{E-H_1(q,p)}\approx\ln{\Omega_2}-\left.\frac{\partial\ln{\Omega_2(E)}}{\partial E_1}\right|_{E_2=E}H_1
\end{equation}

Sabemos que

\[\beta_2=\left.\frac{\partial\ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=\widetilde{E}_2=E-\widetilde{E}_1}\approx\left.\frac{\partial\ln{\Omega_2(E_2)}}{\partial E_2}\right|_{E_2=E}=\beta\]

pues usamos que $\widetilde{E}_1\lll E$.\\ \\

Por tanto, a $A_2$ vamos a asociarle una temperatura $T$, y cuando $A_1$ alcance el equilibrio térmico, tendrá también la misma temperatura $T$, sin variar la temperatura de $A_2$. Así, $A_2$ será un foco de temperatura $T$ constante. Entonces,

\begin{equation}
    \ln{\Omega_2}\brackets{E-H_1(q,p)}\approx\ln{\Omega_2(E)}-\beta H_1=\ln{\Omega_2(E)}-\ln{e^{\beta H_1}}=\ln{(\Omega_2(E)e^{-\beta H_1})}
\end{equation}
por tanto, quitando los logaritmos, tenemos
\begin{equation}
    \Omega_2\brackets{E-H_1(q,p)}\approx\Omega_2(E)e^{-\beta H_1}
\end{equation}

Esta expresión la introducimos en la expresión de la probabilidad (ecuación \ref{ec29}), así

\begin{equation}
    \rho_{A_1}(q,p)dqdp=\frac{\Omega_2(E)}{h^f\Omega(E)}e^{-\beta H_1(q,p)}dqdp=Ce^{-\beta H_1(q,p)}dqdp
    \label{ec2-39}
\end{equation}

con $C=\frac{\Omega_2(E)}{h^f\Omega(E)}$ y para calcular $C$ imponemos,

\[\int\rho_{A_1}(q,p)dqdp=1\]

Por tanto, sustituyendo la densidad por la ecuación $\ref{ec2-39}$ tenemos

\[\begin{matrix}
    \int Ce^{-\beta H_1(q,p)}dqdp=1\\ \\
    \frac{1}{C}=\int e^{-\beta H_1(q,p)}dqdp\\ \\
    C=\frac{1}{\int e^{-\beta H_1(q,p)}dqdp}
\end{matrix}\]

sustituyendo $C$ en \ref{ec2-39}, tenemos

\begin{equation}
  \rho(q,p)=\frac{e^{-\beta H_1(q,p)}}{\int e^{-\beta H_1(q,p)}dqdp}  
\end{equation}

siendo esta la \textbf{distribución canónica}. Así:\\ 
\textit{Un colectivo canónico será un conjunto de microestados con una densidad de probabilidad dada por la distribución canónica y que nos permitirá describir sistemas más complejos.}\\ \\

Como $\beta=\frac{1}{kT}$, entonces $e^{-\beta H}=e^{\frac{H}{kT}}$, donde $H$ es la energía característica del sistema y $kT$ es la energía térmica del sistema y esta fracción es adimensional. Cabe recalcar que tenemos una especie de 'lucha' entre $H$ y $kT$, pues $H$ tiende a que el sistema se ordene, mientras que $kT$ tiende a su desorden.\\ 

En todos los sistemas que veremos tendremos un término del tipo $\frac{H}{kT}$.

\subsection{Cálculo de valores medios}

Vamos a calcular los valores medios $A(q,p)$. Por el Primer Postulado tenemos

\begin{equation}
    A_{\text{macroscopico}}=\overline{A}=\int dqdp\rho(q,p;t)A(q,p)
\end{equation}

con

\begin{equation*}
  \rho(q,p)=\frac{e^{-\beta H_1(q,p)}}{\int e^{-\beta H_1(q,p)}dqdp}  
\end{equation*}

Luego, sustituyendo,

\begin{equation}
    \overline{A}=\frac{\int dqdp\cdot e^{-\beta H_1(q,p)}A(q,p)}{\int e^{-\beta H_1(q,p)}dqdp}
\end{equation}

Calculamos la parcial de $\widetilde{A}$ respecto $\beta$,

\[\begin{matrix}
    \frac{\partial\overline{A}}{\partial\beta}=\frac{\partial}{\partial\beta}\left(\frac{\int dqdp\cdot e^{-\beta H_1(q,p)}A(q,p)}{\int e^{-\beta H_1(q,p)}dqdp}\right)
\end{matrix}\]

Usando la regla de la división, tenemos

\[\begin{matrix}
    \frac{\partial\overline{A}}{\partial\beta}=\frac{-\int dqdpA(q,p)H_1(q,p)e^{-\beta H_1(q,p)}\int e^{-\beta H_1(q,p)}dqdp+\int dqdpA(q,p)e^{-\beta H_1(q,p)}\int H_1(q,p)e^{-\beta H_1(q,p)}dqdp}{\brackets{\int e^{-\beta H_1(q,p)}dqdp}^2}=\\ \\
    =\frac{-\int dqdpA(q,p)H_1(q,p)e^{-\beta H_1(q,p)}\cancel{\int e^{-\beta H_1(q,p)}dqdp}}{\brackets{\int e^{-\beta H_1(q,p)}dqdp}^{\cancel{2}}}+\frac{\cancelto{\overline{A}}{\int dqdpA(q,p)e^{-\beta H_1(q,p)}}\int H_1(q,p)e^{-\beta H_1(q,p)}dqdp}{\brackets{\int e^{-\beta H_1(q,p)}dqdp}^{\cancel{2}}}=\\ \\
    =-\frac{\int dqdpA(q,p)H_1(q,p)e^{-\beta H_1(q,p)}}{\int e^{-\beta H_1(q,p)}dqdp}+\overline{A}\cdot\frac{\int H_1(q,p)e^{-\beta H_1(q,p)}dqdp}{\int e^{-\beta H_1(q,p)}dqdp}=\\ \\
    \curlybraces{\begin{matrix}
        \overline{H}_1=\frac{\int H_1(q,p)e^{-\beta H_1(q,p)}dqdp}{\int e^{-\beta H_1(q,p)}dqdp}\hspace{5mm}\text{Pues podemos poner cualquier cosa en el lugar de }A\text{ en la ecuación (42)}\\ \\
        \overline{A\cdot H_1}=\frac{\int dqdpA(q,p)H_1(q,p)e^{-\beta H_1(q,p)}}{\int e^{-\beta H_1(q,p)}dqdp}\hspace{5mm}\text{Igual que arriba.}
    \end{matrix}}\\ \\
    =-\overline{A H_1}+\overline{A}\cdot\overline{H}_1
\end{matrix}\]

Por tanto,

\begin{equation}
    \frac{\partial\overline{A}}{\partial\beta}=-\overline{AH_1}+\overline{A}\cdot\overline{H}_1
    \label{ec43}
\end{equation}

Usando que $\overline{\overline{A}(H_1-\overline{H}_1)}=0$,

\begin{proof}
    \[\overline{\overline{A}(H_1-\overline{H}_1)}=\overline{A}\overline{(H_1-\overline{H}_1)}=\overline{A}(\overline{H}_1-\overline{\overline{H}}_1)=\overline{A}(\cancelto{0}{\overline{H}_1-\overline{H}_1})=0\]
\end{proof}

Podemos sumar esta expresión en la ecuación $\ref{ec43}$ pues es cero, tal que

\[\begin{matrix}
     \frac{\partial\overline{A}}{\partial\beta}=-\overline{AH_1}+\overline{A}\cdot\overline{H}_1+\overline{\overline{A}(H_1-\overline{H}_1)}=-\overline{AH_1}+\overline{A\overline{H}_1}+\overline{\overline{A}(H_1-\overline{H}_1)}=\\ \\
     =-\overline{AH_1+A\overline{H}_1}+\overline{\overline{A}(H_1-\overline{H}_1)}=-\overline{AH_1+A\overline{H}_1}+\overline{\overline{A}H_1-\overline{A}\overline{H}_1}=-\overline{AH_1+A\overline{H}_1+\overline{A}H_1-AH_1}=\\ \\
     =-\overline{\brackets{A(H_1-\overline{H}_1)-\overline{A}(H_1-\overline{H_1})}}=-\overline{\brackets{(A-\overline{A})(H_1-\overline{H}_1)}}
\end{matrix}\]

Luego,

\begin{equation}
    \frac{\partial\overline{A}}{\partial\beta}=-\overline{\brackets{(A-\overline{A})(H_1-\overline{H}_1)}}
\end{equation}

Vamos a analizar qué ocurre cuando $A=H(q,p)$, usando la ecuación $\ref{ec43}$

\[\begin{matrix}
    \frac{\partial\overline{H}}{\partial\beta}=-\overline{HH}+\overline{H}\overline{H}=-(\overline{H^2}-\overline{H}^2)=\curlybraces{\text{aproximamos }\overline{H}\approx\overline{E}}=-(\overline{E^2}-\overline{E}^2)=-(\Delta E)^2
\end{matrix}\]

Calculamos la parte de la izquierda de la igualdad,

\[\frac{\partial\overline{H}}{\partial\beta}=\frac{\partial\overline{E}}{\partial\beta}=\frac{\partial\overline{E}}{\partial T}\frac{\partial T}{\partial\beta}=\curlybraces{\begin{matrix}
    \beta=\frac{1}{kT}\\
    T=\frac{1}{k\beta}
\end{matrix}}=\frac{\partial\overline{E}}{\partial T}\left(-\frac{1}{\cancel{k}}\frac{1}{\frac{1}{k^{\cancel{2}}T^2}}\right)=-kT^2\frac{\partial\overline{E}}{\partial T}\]

Por tanto,

\begin{equation}
    kT^2\frac{\partial\overline{E}}{\partial T}=\overline{E^2}-\overline{E}^2
\end{equation}

donde $\frac{\partial\overline{E}}{\partial\beta}=C$ representa la capacidad calorífica del sistema.
\\ \\
Si dividimos a ambos lados por $\overline{E}^2$, tenemos 

\[\frac{(\Delta E)^2}{\overline{E}^2}=kT^2\frac{C}{\overline{E}^2}\Rightarrow\frac{\Delta E}{\overline{E}}=\frac{T}{\overline{E}}\sqrt{kC}=\curlybraces{\text{si} E\neq E(T)\Rightarrow E=C\cdot T}=\frac{\cancel{T}}{C\cancel{T}}\sqrt{kC}=\sqrt{\frac{k}{C}}\]

como $C=N\cdot C_e$, con $C_e$ calor específico, podemos aproximar esta raíz por los órdenes de magnitud con

\begin{equation}
    \frac{\Delta E}{\overline{E}}=\sqrt{\frac{k}{C}}\approx\sqrt{\frac{k}{N}}
\end{equation}

Analizamos la situación cuando $N\ggg$, 

\[\frac{\Delta E}{\overline{E}}\sim\sqrt{\frac{1}{N}}\Rightarrow\sqrt{\frac{1}{N}}\lll\Rightarrow\frac{\Delta E}{\overline{E}}\lll\]

Luego, es una ley física y además la energía del sistema está bien definida.

\subsection{Función de partición y cálculo de valores medios}

En la práctica no se trabaja con la distribución canónica, se trabaja con la función de partición que es

\begin{equation}
    Z=\frac{1}{h^f}\int e^{-\beta H(q,p)}dqdp
    \label{ec47}
\end{equation}

y la distribución canónica se expresará como

\begin{equation}
    \rho(q,p)=\frac{e^{-\beta H(q,p)}}{H^fZ}
\end{equation}

Vamos a calcular algunos ejemplos de valores medios:

\subsubsection*{Ejemplo 1}

\textbf{Cálculo de la energía media del sistema.}\\ \\

La energía media del sistema vendrá dada por

\begin{equation}
    \overline{E}=\frac{\int dqdpH(q,p)e^{-\beta H(q,p)}}{\int e^{-\beta H(q,p)}dqdp}
\end{equation}

Usando la ecuación $\ref{ec47}$, podemos sustituir el denominador. Vamos a calcular ahora el numerador, 

\[\int dqdpH(q,p)e^{-\beta H(q,p)}=\curlybraces{\frac{\partial e^{-\beta H}}{\partial\beta}}=-H\cdot e^{-\beta H}=\int dqdp\frac{\partial e^{-\beta H(q,p)}}{\partial\beta}=-\frac{\partial}{\partial\beta}\cancelto{h^fZ}{\int dqdp\cdot e^{-\beta H(q,p)}}\]

Así,

\[\overline{E}=\frac{\int dqdpH(q,p)e^{-\beta H(q,p)}}{\int e^{-\beta H(q,p)}dqdp}=-\frac{\cancel{h^f}\frac{\partial Z}{\partial\beta}}{\cancel{h^f}Z}=-\frac{1}{Z}\frac{\partial Z}{\partial \beta}=-\frac{\partial\ln{Z}}{\partial\beta}\]

Luego,

\begin{equation}
    \overline{E}=-\left.\frac{\partial\ln{Z}}{\partial\beta}\right|_{X_{\alpha}}
\end{equation}

\subsubsection*{Ejemplo 2}

\textbf{Cálculo de la fuerza generalizada media del sistema.}\\ \\

La fuerza generalizada media viene dada por

\begin{equation}
    \overline{Y}_{X_{\alpha}}=\frac{\int dqdp\cdot Y_{\alpha}e^{-\beta H(q,p,X_{\alpha})}}{\int e^{-\beta H(q,p,X_{\alpha})}dqdp}
\end{equation}

Usando la ecuación $\ref{ec47}$, podemos sustituir el denominador. Vamos a calcular ahora el numerador, 

\[\int dqdp\cdot Y_{\alpha}e^{-\beta H(q,p,X_{\alpha})}=\int dqdp\cdot\frac{-\partial H}{\partial X_{\alpha}}e^{-\beta H(q,p,X_{\alpha})}=\]
\[\curlybraces{\begin{matrix}
    \frac{\partial e^{-\beta H}}{\partial X_{\alpha}}=e^{-\beta H}\frac{\partial}{\partial X_{\alpha}}(-\beta H)=-\beta e^{-\beta H}\frac{\partial H}{\partial X_{\alpha}}\\ \\
    -\frac{\partial H}{\partial X_{\alpha}}e^{-\beta H}=\frac{1}{\beta}\frac{\partial e^{-\beta H}}{\partial}
\end{matrix}}\]
\[=\frac{1}{\beta}\int dqdp\frac{\partial e^{-\beta H}}{\partial X_{\alpha}}=\frac{1}{\beta}\frac{\partial}{\partial X_{\alpha}}\cancelto{h^fZ}{\int e^{-\beta Hdqdp}}=\frac{\frac{1}{\beta}\cancel{h^f}\frac{\partial Z}{\partial X_{\alpha}}}{\cancel{h^f}Z}=\frac{1}{\beta Z}\frac{\partial Z}{\partial X_{\alpha}}=\frac{1}{\beta}\frac{\partial\ln{Z}}{\partial X_{\alpha}}\]

Por tanto,

\begin{equation}
    \overline{Y}_{\alpha}=\frac{1}{\beta}\frac{\partial\ln{Z}}{\partial X_{\alpha}}
\end{equation}

que representa la \textbf{ecuación de estado} del sistema en el colectivo canónico.

\subsubsection*{Ejemplo 3}

\textbf{Demostrar que}

\[\overline{(\Delta E)^2}=-\left.\frac{\partial \overline{E}}{\partial\beta}\right|_{X_{\alpha}}\]

Vamos a desarrollar la parte izquierda y luego la derecha, para ver que son iguales,

\[\overline{(\Delta E)^2}=\overline{E^2}-\overline{E}^2\]

\[\begin{matrix}
    \overline{E^2}=\frac{\int dqdpH^2(q,p)e^{-\beta H(q,p)}}{\cancelto{h^fZ}{\int e^{-\beta H(q,p)dqdp}}}=\curlybraces{\frac{\partial^2e^{-\beta H}}{\partial\beta^2}=H^2e^{-\beta H}}=\frac{\int dqdp\frac{\partial^2e^{-\beta H(q,p)}}{\partial\beta^2}}{h^fZ}=\frac{\frac{\partial^2}{\partial\beta^2}\cancelto{h^fZ}{\int dqdpe^{-\beta H}}}{h^fZ}=\frac{1}{Z}\frac{\partial^2Z}{\partial\beta^2}=\\ \\
    =\frac{\partial}{\partial Z}\brackets{\frac{1}{Z}\frac{\partial Z}{\partial\beta}}=\frac{\partial }{\partial\beta}\brackets{\frac{\partial\ln{Z}}{\partial\beta}}
\end{matrix}\]

\[\overline{E}^2=\brackets{-\frac{1}{Z}\frac{\partial Z}{\partial\beta}}^2=\frac{1}{Z^2}\brackets{\frac{\partial Z}{\partial\beta}}^2=\brackets{\frac{\partial\ln{Z}}{\partial\beta}}^2\]

\[(...)\]

\section{Relación entre mecánica estadística y termodinámica en sistemas en equilibrio que intercambian solo calor}

\subsection{Definición de entropía}

Partimos de $Z=Z(\beta,X_{\alpha})$ y vamos a calcular el diferencial del logaritmo de $Z$,

\[d(\ln{Z})=\cancelto{-\overline{E}}{\left.\frac{\partial\ln{Z}}{\partial\beta}\right|_{X_{\alpha}}}d\beta+\sum_{\alpha}\cancelto{\beta\overline{Y}_{\alpha}}{\frac{\partial\ln{Z}}{\partial X_{\alpha}}}dX_{\alpha}\]

Así,

\begin{equation}
    d(\ln{Z})=-\overline{E}d\beta+\beta\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}
\end{equation}

Queremos tener el $d\overline{E}$ y el trabajo, para ello, sumamos a ambos miembros $d(\beta\overline{E})$,

\[\begin{matrix}
    d(\ln{Z}+\beta\overline{E})=d(\beta\overline{E})-\overline{E}d\beta+\beta\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}=\\ \\
    =\cancel{\overline{E}d\beta}+\beta d\overline{E}-\cancel{\overline{E}d\beta}+\beta\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}=\\ \\
    =\beta d\overline{E}+\beta\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}=\beta(d\overline{E}+\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha})
\end{matrix}\]

Sabemos que

\[\delta W=\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}\]
\[d\overline{E}=\delta Q-\delta W\]

Luego,

\[d\overline{E}+\sum_{\alpha}\overline{Y}_{\alpha}dX_{\alpha}=\delta Q-\cancel{\delta W}+\cancel{\delta W}=\delta Q\]

Por tanto,

\[d(\ln{Z}+\beta\overline{E})=\beta\delta Q=\frac{1}{kT}\delta Q\]
\[k\cdot d(\ln{Z}+\beta\overline{E})=\frac{\delta Q}{T}\]
\[d\brackets{k(\ln{Z}+\beta\overline{E})}=\frac{\delta Q}{T}\]

Por Termodinámica sabemos que $dS=\frac{\delta Q}{T}$, por tanto, comparando tenemos que

\begin{equation}
    dS=k\brackets{\ln{Z}+\beta\overline{E}}
\end{equation}

luego,

\begin{equation}
    S=k\brackets{\ln{Z}+\beta\overline{E}}
\end{equation}

Resulta que esta definición de entropía no es muy buena, y para solucionarlo, debemos introducir un nuevo término, así queda

\begin{equation}
    S=k\brackets{\ln{\frac{Z}{N!}}+\beta\overline{E}}
\end{equation}

donde $N$ es el número de partículas.

Pero lo que se puede hacer también es redefinir la función de partición, para así tener una función de la entropía más simple, es decir

\begin{equation}
    Z=\frac{1}{h^fN!}\int e^{-\beta H(q,p)}dqdp
\end{equation}

y tenemos

\begin{equation}
    S=k\brackets{\ln{Z}+\beta\overline{E}}
    \label{ec58}
\end{equation}

Cómo sabemos que la expresión $S=k\ln{\Omega(E)}$ está bien definida, vamos a comprobar que la ecuación \ref{ec58} está bien definida. Para ello partimos de la función de partición,

\[Z=\frac{1}{h^fN!}\int e^{-\beta H(q,p)}dqdp=\frac{1}{h^fN!}\int dqdp\int e^{-\beta H(q,p)}\delta\brackets{E-H(q,p)}dE=\]
\[=\frac{1}{h^fN!}\int dqdp\int e^{-\beta E}\delta\brackets{E-H(q,p)}dE=\frac{1}{h^fN!}\int e^{-\beta E}dE\cancelto{\Omega(E)}{\int dqdp\delta\brackets{E-H(q,p)}}=\int e^{-\beta E}\Omega(E)dE\]

Vamos a comparar ahora cómo es la multiplicación de $\Omega(E)e^{-\beta E}$. Sabemos que $\Omega(E)$ crece muy rápido con la energía y que $e^{-\beta E}$ decrece rápidamente con la energía. Ver Figura \ref{fig10}

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{Captura de pantalla 2023-10-16 173003.png}
    \captionof{figure}{Representación de las funciones $\Omega(E)$ y $e^{-\beta E}$.}
    \label{fig10}
\end{Figura}

La gráfica de ambas funciones multiplicadas será la Figura \ref{fig11}

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{Captura de pantalla 2023-10-16 173842.png}
    \captionof{figure}{Representación de la función total $e^{-\beta E}\Omega(E)$.}
    \label{fig11}
\end{Figura}

Como estamos haciendo la integral de esta multiplicación y la integral representa el área bajo de la curva, entonces podremos aproximar este área con la de un rectángulo, como se muestra en la Figura \ref{fig12}.

\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{Captura de pantalla 2023-10-16 174601.png}
    \captionof{figure}{Aproximación de la función por un rectángulo.}
    \label{fig12}
\end{Figura}

Así, tendremos

\[Z=\int e^{-\beta E}\Omega(E)dE\approx e^{-\beta\widetilde{E}}\omega(\widetilde{E})n\Delta^*E\]

Luego, podremos aproximar el logaritmo neperiano de $Z$ de la forma,

\[\ln{Z}\approx\ln\brackets{e^{-\beta\widetilde{E}}\omega(\widetilde{E})n\Delta^*E}=\ln{e^{-\beta\widetilde{E}}}+\ln{\Omega(\widetilde{E})}+\ln{n\delta^*E}=-\beta\widetilde{E}+\ln{\Omega(\widetilde{E})}+\ln{n\Delta*E}\]

Para un sistema macroscópico $\ln{n\Delta*E}\approx0$, pues las fluctuaciones son muy pequeñas y por tanto $\overline{E}$ está bien definida, por lo que $E\approx\overline{E}\approx\widetilde{E}$. Por tanto,

\[\ln{Z}+\beta\overline{E}=-\beta\widetilde{E}+\cancelto{0}{\ln{n\Delta*E}}+\beta\overline{E}+\ln{\omega(\widetilde{E})}\]
\[\ln{Z}+\beta\overline{E}=-\cancel{\beta\widetilde{E}}+\cancel{\beta\widetilde{E}}+\ln{\omega(E)}\]

Luego,

\[S=k\brackets{\ln{Z}+\beta\overline{E}}\approx k\ln{\Omega(E)}\]

Así, la expresión de la entropía del colectivo canónico está bien definida y cumple las propiedades propias de la entropía.

\subsection{Energía libre de Helmholtz}

La energía libre de Helmholtz es una función $F$ que en termodinámica se define como $F=E-TS$. Vamos a calcularla en términos de la función de partición.\\ \\

Sabemos que

\[S=k\brackets{\ln{Z}+\beta\overline{E}}\hspace{5mm}\text{y}\hspace{5mm}\overline{E}=-\left.\frac{\partial\ln{Z}}{\partial\beta}\right|_{X_{\alpha}}\]

Luego,

\[\overline{F}=\overline{E}-TS=\overline{E}-Tk\brackets{\ln{Z}+\beta\overline{E}}=\overline{E}-Tk\brackets{\ln{Z}+\frac{1}{kT}\overline{E}}\]
\[\overline{F}=\cancel{\overline{E}}-Tk\ln{Z}-\overline{E}=-kT\ln{Z}\]

Por tanto,

\begin{equation}
    \overline{F}=kT\ln{Z}
\end{equation}

Además,

\begin{equation}
    \ln{Z}=-\frac{\overline{F}}{kT}\Rightarrow Z=e^{-\frac{\overline{F}}{kT}}
\end{equation}

Para calcular la presión hacemos,

\[\overline{P}=kT\left.\frac{\partial\ln{Z}}{\partial V}\right|_T=kT\left.\frac{\partial\frac{-\overline{F}}{kT}}{\partial V}\right|_T=\frac{-kT}{kT}\left.\frac{\partial\overline{F}}{\partial V}\right|_T=-\left.\frac{\partial\overline{F}}{\partial V}\right|_T\]

por tanto,

\begin{equation}
    \overline{P}=-\left.\frac{\partial\overline{F}}{\partial V}\right|_T
\end{equation}

\section{Colectivo canónico generalizado: sistemas físicos abiertos en equilibrio térmico y químico}

Para describir este colectivo, tomamos dos sistemas $A_1$ y $A_2$, tal que $A_2\ggg A_1$, que componen un sistema aislado $A=A_1+A_2$, donde se introduce el parámetro del número de partículas de cada sistema, $N_i$. Ver Figura \ref{fig13}

\begin{Figura}
    \centering
    \includegraphics[width=0.45\linewidth]{Captura de pantalla 2023-10-16 175502.jpg}
    \captionof{figure}{Representación del sistema $A=A_1+A_2$.}
    \label{fig13}
\end{Figura}

Vemos que $f_2\ggg f_1$ y $N_2\ggg N_1$. \\ \\

Como $A=A_1+A_2$ es un sistema aislado, su distribución de probabilidades vendrá dada por la forma microcanónica, así

\begin{equation}
    \rho_{\text{aislado}}(q,p,Q,P)dqdpdQdP=\frac{1}{h^f\Omega(E)}\delta\brackets{E-H(q,p,Q,P)}
\end{equation}

con $H(q,p,Q,P)=H_1(q,p)+H_2(Q,P)+\cancelto{0}{H_{12}(q,p,Q,P)}$. $H_{12}$ es cero, pues lo suponemos despreciable frente $H_1$ y $H_2$, es decir, $H_1,H_2\ggg H_{12}$.\\ \\

Nos interesa el comportamiento de $A_1$. Por ello, calculamos la probabilidad de que el sistema $A_1$ esté comprendido entre $q$ y $q+dq$ y entre $p$ y $p+dp$. Tal que,

\[\rho_{A_1}(q,p)=\int dQdP\rho_{\text{aislado}}(q,p,Q,P)=\int dQdP\frac{1}{h^{f}\Omega(E)}\delta\brackets{E-H_1(q,p)-H_2(Q,P)}=\]
\[=\frac{1}{h^f\Omega(E)}\int dQdP\delta\brackets{E-H_1(q,p)-H_2(Q,P)}=\frac{1}{h^{f_1}\omega(E)}\Omega_2(E-H_1(q,p))\]

Por tanto, tenemos

\begin{equation}
    \rho_{A_1}(q,p)dqdp=\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))dqdp
\end{equation}

Entonces, la probabilidad de que $A_1$ tenga energía $E_1$ comprendida entre $E_1$ y $E_1+dE_1$, tenemos

\[\omega_1(E_1)dE_1=\int_{E_1\leq H_1\leq E_1+dE_1}dqdp\rho_{A_1}(q,p)=\int_{e_1\leq H_1\leq E_1+dE_1}dqdp\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))=\]
\[=\frac{1}{h^{f_1}\Omega(E)}\int_{E_1\leq H_1\leq E_1+dE_1}dqdp\Omega_{2}(E-H_1(q,p))=\]
\[\curlybraces{\text{Como }dE_1\lll\text{ y }H_1\in\brackets{E_1,E_1+dE_1}\text{ consideramos }H_1\approx E_1}\]
\[=\frac{\Omega_2(E-E_1)}{\Omega(E)}\cancelto{\Omega_1(E-E_1)dE_1}{\frac{1}{h^{f_1}}\int_{E_1\leq H_1\leq E_1+dE_1}dqdp}=\frac{\Omega_2(E-E_1)}{\Omega(E)}\Omega_1(E-1)dE_1\]

Por tanto, tenemos

\begin{equation}
    \omega_1(E_1)dE_1=\frac{\Omega_2(E-E_1)}{\Omega(E)}\Omega_1(E-1)dE_1
\end{equation}

Ahora nos preguntamos cuál es la densidad de probabilidad de encontrar $A_1$ en un microestados tal que contenga $N_1$ partículas dadas, con posiciones comprendidas en el intervalo $(q,q+dq)$ y cantidades de movimiento dentro del margen $(p,p+dp)$, viene dada de acuerdo con 

\[\rho_{A_1}(q,p)=\frac{1}{h^{f_1}\Omega(E)}\Omega_2(E-H_1(q,p))\]

por (en las expresiones de $\rho$ fijamos primero el número de partículas y después las coordenadas y momentos generalizados, pues este es el orden lógico; además, por sencillez, consideramos un único tipo de partículas)

\begin{equation}
    \rho_{A_1}(N_1,q,p)=\frac{1}{h^{f_1}\Omega(E.N)}\Omega_2(E-H_N(q,p),N-N_1)
\end{equation}

Calculamos,

\[\omega_1(E_1,N_1)dE_1=\int_{E_1\leq H_1\leq E_1+dE_1}dqdp\rho_{A_1}(N_1,q,p)=\]
\[=\int_{E_1\leq H_1\leq E_1+dE_1}dqdp\frac{1}{h^{f_1}\Omega(E.N)}\Omega_2(E-H_N(q,p),N-N_1)=\]
\[=\frac{1}{h^{f_1}\Omega(E,N)}\int_{E_1\leq H_1\leq E_1+dE_1}dqdp\Omega_2(E-H_N(q,p),N-N_1)=\]
\[\curlybraces{\text{Como }dE_1\lll\text{ y }H_1\in\brackets{E_1,E_1+dE_1}\text{ consideramos }H_1\approx E_1}\]
\[=\frac{\Omega_2(E-E_1,N-N_1)}{\Omega(E,N)}\cancelto{\Omega_1(E-E_1,N-N_1)dE_1}{\frac{1}{h^{f_1}}\int_{E_1\leq H_1\leq E_1+dE_1}dqdp}=\frac{\Omega_2(E-E_1,N-N_1)}{\Omega(E,N)}\Omega_1(E-E_1,N-N_1)dE_1\]

Por tanto,

\begin{equation}
    \omega_1(E_1,N_1)dE_1=\frac{\Omega_2(E-E_1,N-N_1)}{\Omega(E,N)}\Omega_1(E-E_1,N-N_1)dE_1
\end{equation}

Como tenemos $N$ partículas, es decir

\[1,2,\dots,N_1,N_1+1,\dots,N\]

el orden de las partículas no nos importa, pues solo queremos que hayan $N_1$ partículas, es decir, podremos tener varias combinaciones,

\[N\in\brackets{1,N_1},\hspace{3mm}N\in\brackets{2,N_1+1},\hspace{3mm},N\in\brackets{1,N-1}\cup\brackets{N+1},\dots\]

Así, las combinaciones serán $N$ sobre $N_1$, es decir,

\[\begin{pmatrix}
    N\\
    N_1
\end{pmatrix}=\frac{N!}{N_1!(N-N_1)!}\]

Luego, la probabilidad de que $A_1$ tenga $E_1\in\brackets{E_1,E_1+dE_1}$ y $N_1$ partículas cualesquiera será

\[\omega_1(E_1,N_1)dE_1=\frac{N!}{N_1!(N-N_1)!}\frac{\Omega_2(E-E_1,N-N_1)}{\Omega(E,N)}\Omega_1(E-E_1,N-N_1)dE_1\]

Por tanto,

\begin{equation}
    \omega_1(E_1,N_1)dE_1=\frac{N!}{\Omega(E,N)}\frac{\Omega_1(E-E_1,N-N_1)}{N_1!}\frac{\Omega_2(E-E_1,N-N_1)}{(N-N_1)!}dE_1
\end{equation}
\\ \\

Como tenemos factoriales, $\frac{\Omega_1(E-E_1,N-N_1)}{N_1!}$ decrece mucho y $\frac{\Omega_2(E-E_1,N-N_1)}{(N-N_1)!}$ crece mucho, entonces los valores distintos de cero de $\frac{\Omega_1(E-E_1,N-N_1)}{N_1!}\frac{\Omega_2(E-E_1,N-N_1)}{(N-N_1)!}$ estarán cerca de $E_1=\widetilde{E}_1$ y $N_1=\widetilde{N}_1$. Entonces, vemos que las derivadas son iguales,

\begin{equation}
    \frac{\partial}{\partial E_1}\left.\left(\ln\brackets{\frac{\Omega_1(E_1,N_1)}{N_1!}}\right)\right|_{\begin{matrix}
        E_1=\widetilde{E}_1\\
        N_1=\widetilde{N}_1
    \end{matrix}}=\frac{\partial}{\partial E_2}\left.\left(\ln\brackets{\frac{\Omega_2(E_2,N_2)}{N_2!}}\right)\right|_{\begin{matrix}
        E_2=\widetilde{E}_2=E-\widetilde{E}_1\\
        N_2=\widetilde{N}_2=N-\widetilde{N}_1
    \end{matrix}}
\end{equation}
\begin{equation}
    \frac{\partial}{\partial N_1}\left.\left(\ln\brackets{\frac{\Omega_1(E_1,N_1)}{N_1!}}\right)\right|_{\begin{matrix}
        E_1=\widetilde{E}_1\\
        N_1=\widetilde{N}_1
    \end{matrix}}=\frac{\partial}{\partial N_2}\left.\left(\ln\brackets{\frac{\Omega_2(E_2,N_2)}{N_2!}}\right)\right|_{\begin{matrix}
        E_2=\widetilde{E}_2=E-\widetilde{E}_1\\
        N_2=\widetilde{N}_2=N-\widetilde{N}_1
    \end{matrix}}
\end{equation}

Como $f_2\ggg f_1$, entonces $E_1\lll E_2\approx E$ y $N_1\lll N_2\approx N$. Por tanto, podemos hacer un desarrollo en serie en torno a $E$ y $N$,

\[\ln\brackets{\frac{\Omega_2(E-H_{N_1}(q,p),N-N_1)}{(N-N_1)!}}=\ln\brackets{\frac{\Omega_2}{N!}}-\cancelto{\beta}{\frac{\partial}{\partial E_2}\left.\brackets{\ln\left(\frac{\Omega_2}{N_2!}\right)}\right|_{\begin{matrix}
    E_1=\widetilde{E}_1\\
    N_1=\widetilde{N}_1
\end{matrix}}}H_{N_1}-\cancelto{-\beta\mu}{\frac{\partial}{\partial N_2}\left.\brackets{\ln\left(\frac{\Omega_2}{N_2!}\right)}\right|_{\begin{matrix}
    E_1=\widetilde{E}_1\\
    N_1=\widetilde{N}_1
\end{matrix}}}N_1\]

Por tanto,

\begin{equation}
    \ln\brackets{\frac{\Omega_2(E-H_{N_1}(q,p),N-N_1)}{(N-N_1)!}}=\ln\brackets{\frac{\Omega_2}{N!}}-\beta H_{N_1}+\beta\mu N_1
\end{equation}

donde $\mu$ es el potencial químico.

Quitamos el logaritmo,

\begin{equation}
    \frac{\Omega_2(E-H_{N_1}(q,p),N-N_1)}{(N-N_1)!}=\frac{\Omega_2}{N!}e^{-\beta H_{N_1}+\beta\mu N_1}
\end{equation}

Luego, la probabilidad de que $N_1$ partículas cualesquiera del total de $N$ partículas tengan $q$ entre $q$ y $q+dq$ y $p$ entre $p$ y $p+dp$ será,

\[\rho_{A_1}(N_1,q,p)=\frac{N!}{\Omega(E,N)}\frac{1}{h^{f_1}N_1!}\frac{\Omega_2(E-H_{N_1}(q,p),N-N_1)}{(N-N_1)!}=\frac{\cancel{N_1!}}{\Omega(E,N)}\frac{1}{h^{f_1}N_1!}\frac{\Omega_2(E,N)}{\cancel{N!}}e^{-\beta H_{N_1}+\beta\mu N_1}\]

Por tanto,

\begin{equation}
    \rho_{A_1}(N_1,q,p)=\frac{\Omega_2(E,N)}{\Omega(E,N)}\frac{1}{h^{f_1}N_1!}e^{-\beta(H_{N_1}-\mu N_1)}
\end{equation}

donde vemos que $\frac{\Omega_2(E,N)}{\Omega(E,N)}$ no depende de $N_1$, $q$ ni $p$.\\ \\

Definimos la fugacidad del sistema $\xi$ como

\begin{equation}
    \xi=e^{-\alpha}
\end{equation}

donde

\[\alpha=-\beta\mu=\frac{\partial}{\partial N}\left(\ln{\frac{\Omega}{N!}}\right)\]

Por tanto, tenemos

\begin{equation}
    \rho(N,q,p)=C\frac{1}{h^{f}N!}e^{-\beta H_{N}-\alpha N}
\end{equation}

Debemos normalizar $\rho$ y para ello sumamos todas las probabilidades posibles (la integral) para todas las partículas (el sumatorio),

\begin{equation}
    \sum_{N=0}^{\infty}\int\rho(N,q,p)dqdp=1
\end{equation}

Vamos a ver cuánto vale $C$,

\[\sum_{N=0}^{\infty}\int\rho(N,q,p)dqdp=1\]
\[\sum_{N=0}^{\infty}\int C\frac{1}{h^{f}N!}e^{-\beta H_{N}-\alpha N}dqdp=1\]
\[\sum_{N=0}^{\infty}C\frac{1}{h^{f}N!}e^{-\alpha N}\int e^{-\beta H_{N}}dqdp =1\]
\begin{equation}
C^{-1}=\sum_{N=0}^{\infty}\frac{1}{h^{f}N!}e^{-\alpha N}\int e^{-\beta H_{N}}dqdp=Q
\label{ec76}
\end{equation}

siendo $Q$ la \textbf{función de partición generalizada o gran función de partición}. Por tanto,

\begin{equation}
    \rho(N,q,p)=\frac{1}{Q}\frac{1}{h^{f}N!}e^{-\beta H_{N}-\alpha N}
    \label{ec77}
\end{equation}

que llamamos \textbf{distribución gran canónica, macrocanónica o canónica generalizada}.\\ \\

Así, el colectivo gran canónico es un colectivo de microestados que sigue la distribución gran canónica.
\\ \\
Vamos a calcular la distribución de probabilidades para la energía,

\[\omega(E,N)dE=\int_{E<H_N<E+dE}dqdp\rho(N.q,p)\]
Usando que $\rho(N,q,p)=\frac{e^{-\beta H_N-\alpha N}}{Qh^fN!}$, tenemos
\[\omega(E,N)=\int_{E<H_N<E+dE}dqdp\frac{e^{-\beta H_N-\alpha N}}{Qh^fN!}\]
Usamos que $\Omega(E,N)=\int_{E< H_N<E+dE}dqdp$, y cuando vayamos a sustituir, para poder quitarnos la integral, suponemos que $e^{-\beta H_N-\alpha N}\approx e^{-\beta E-\alpha N}$, pues como $H_N\in(E,E+dE)$, si $dE\lll$, entonces $H_N\approx E$. Así queda
\begin{equation}
    \omega(E,N)=\frac{\Omega(E,N)}{QN!}e^{-\beta E-\alpha N}
\end{equation}

\subsubsection*{¿Qué ocurre si hay distintos tipos de partículas?}

Sabemos que la función de partición es
\[Z=\frac{1}{h^fN!}\int dqdpe^{-\beta H}\]
y que la gran función de partición es
\[Q=\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdpe^{-\beta H_N}\]
Por tanto, podremos reescribir la gran función de partición como
\begin{equation}
    Q=\sum_{N=0}^{\infty}e^{-\alpha N}Z_N
\end{equation}
usando $Z_N$, pues $Z$ depende de $N$, al haber el $\frac{1}{N!}$.\\ \\
Si consideramos que hay $s-$tipos de partículas, cada tipo de partícula deberá tener sus propios grados de libertad $f_i$, con $i=1,2,\dots,s$. Por tanto, la gran función de partición se puede descomponer en 
\begin{equation}
Q=\sum_{N_1=0}^{\infty}\sum_{N_2=0}^{\infty}\dots\sum_{N_s=0}^{\infty}\frac{e^{-\beta\sum_{i=1}^{s}\mu_i N_i}}{h^{f_1+f_2+\dots+f_s}N_1!N_2!\dots N_s!}\int dqdpe^{-\beta H_{N_1+N_2+\dots+N_s}}
\end{equation}
\subsubsection{Cálculo de valores medios}
\begin{enumerate}[a)]


\item Cálculo de la energía media del sistema con $\alpha$ constante:
\\ \\
Usamos la definición de valor medio,

\[
    \overline{E}  =  \sum_{N=0}^{\infty}\int dqdpH_N(q,p)\rho(N,q,p)  \overset{(\ref{ec77})}{=} \sum_{N=0}^{\infty}\int dqdp H_N(q,p)\frac{1}{Q}\frac{1}{h^fN!}e^{-\beta H_N-\alpha N}=
\]
\[=\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdpH_N(q,p)e^{-\beta H_N}\overset{\curlybraces{He^{-\beta H}=-\frac{\partial e^{-\beta H}}{\partial \beta}}}{=}\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp\brackets{-\frac{\partial e^{-\beta H}}{\partial \beta}}=\]
\[=-\frac{1}{Q}\frac{\partial}{\partial\beta}\brackets{\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp e^{-\beta H}}=\overset{(\ref{ec76})}{=}-\frac{1}{Q}\left.\frac{\partial Q}{\partial \beta}\right|_{\alpha,X}\]
Usando que $\frac{\partial \ln(u(x))}{\partial x}=\frac{1}{u}\frac{\partial u(x)}{\partial x}$, tenemos que la energía media vale
\begin{equation}
    \overline{E}=-\left.\frac{\partial\ln(Q)}{\partial\beta}\right|_{\alpha,X}
    \label{ec81}
\end{equation}
con $\alpha$ y $X$ constantes.\\ \\

\item Cálculo del valor medio del número de partículas:
\\ \\
Usamos la definición de valor medio,
\[\overline{N}=\sum_{N=0}^{\infty}\int dqdp N\rho(N,q,p)=\overset{(\ref{ec77})}{=}\sum_{N=0}^{\infty}\int dqdpN\frac{1}{Q}\frac{1}{h^fN!}e^{-\beta H_N-\alpha N}\]
\[\overline{N}=\frac{1}{Q}\sum_{N=0}^{\infty}\frac{Ne^{-\alpha N}}{h^f N!}\int dqdpe^{-\beta H_N}\overset{\curlybraces{Ne^{-\alpha N}=-\frac{\partial e^{-\alpha N}}{\partial \alpha}}}{=}\frac{1}{Q}\sum_{N=0}^{\infty}-\frac{\partial e^{-\alpha N}}{\partial\alpha}\cdot\frac{1}{h^fN!}\int dqdpe^{-\beta H_N}\]
\begin{equation}
\overline{N}=-\frac{1}{Q}\frac{\partial}{\partial\alpha}\brackets{\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdpe^{-\beta H_N}}\overset{(\ref{ec76})}{=}-\frac{1}{Q}\left.\frac{\partial Q}{\partial\alpha}\right|_{\beta,X}
\label{ec82}
\end{equation}
Usando que $\frac{\partial \ln(u(x))}{\partial x}=\frac{1}{u}\frac{\partial u(x)}{\partial x}$, tenemos que el número de partículas medio es
\begin{equation}
    \overline{N}=-\left.\frac{\partial\ln(Q)}{\partial\alpha}\right|_{\beta,X}
    \label{ec83}
\end{equation}
con $\beta$ y $X$ constantes.

\item Cálculo de la energía media del sistema con $\mu$ constante:
\\ \\
Usamos la definición de valor medio,
\[\overline{E}=\sum_{N=0}^{\infty}\int dqdpH_N(q,p)\rho(N,q,p)\overset{(\ref{ec77})}{=}\sum_{N=0}^{\infty}\int dqdpH_N(q,p)\frac{1}{Q}\frac{1}{h^fN!}e^{-\beta H_N-\alpha N}\]
\[\overline{E}\overset{\curlybraces{\alpha=-\beta\mu}}{=}\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{\beta\mu N}}{h^fN!}\int dqdp H_N(q,p)e^{-\beta H_N}\overset{\curlybraces{H_Ne^{-\beta H_N}=-\frac{\partial e^{-\beta H_N}}{\partial\beta}}}{=}-\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{\beta\mu N}}{h^fN!}\int dqdp\frac{\partial e^{-\beta H_N}}{\partial\beta} \]
\[\overline{E}=-\frac{1}{Q}\sum_{N=0}^{\infty}\frac{1}{h^fN!}\frac{\partial}{\partial\beta}\brackets{e^{\beta\mu N}\int dqdp e^{-\beta H_N}}\]
Usando la regla de la cadena $\frac{\partial(A(x)\cdot B(x))}{\partial x}=\frac{\partial A(x)}{\partial x}B(x)+A(x)\frac{\partial B(x)}{\partial x}$. Nombrando $A=e^{\beta\mu N}$ y $B=\int dqdp e^{-\beta H_N}$. Así,
\[\overline{E}=-\frac{1}{Q}\sum_{N=0}^{\infty}\brackets{\frac{1}{h^fN!}\frac{\partial e^{\beta\mu N}}{\partial \beta}\int dqdp e^{-\beta H_N}}-\frac{1}{Q}\sum_{N=0}^{\infty}\brackets{\frac{e^{\beta\mu N}}{h^fN!}\frac{\partial}{\partial\beta}\int dqdp e^{-\beta H_N}}\]
\[\overline{E}=\frac{1}{Q}\frac{\partial}{\partial\beta}\brackets{\sum_{N=0}^{\infty}\frac{(-1)}{h^fN!}e^{\beta\mu N}\int dqdp e^{-\beta H_N}}-\frac{1}{Q}\sum_{N=0}^{\infty}\frac{\mu N}{h^fN!}e^{-\alpha N}\int dqdp e^{-\beta H_N}\]
Usando la ecuación (\ref{ec76}), y haciendo un cambio de variable en la segunda ecuación, derivando respecto $\alpha$ y dejando constante $\beta$, tenemos
\[\overline{E}=-\frac{1}{Q}\brackets{\frac{\partial Q}{\partial\beta}}_{\mu,X}+\frac{\mu}{Q}\frac{\partial}{\partial\alpha}\brackets{\sum_{N=0}^{\infty}\frac{N}{h^fN!}e^{-\alpha N}\int dqdp e^{-\beta H_N}}_{\beta,X}\]
Usando en la primera parte que $\frac{1}{f(x)}\frac{\partial f(x)}{\partial x}=\frac{\partial\ln(f(x))}{\partial x}$ y en la segunda parte, la ecuación (\ref{ec82}), tenemos
\begin{equation} 
\overline{E}=-\brackets{\frac{\partial\ln(Q)}{\partial\beta}}_{\mu,X}+\mu\overline{N}
\end{equation}
\\\\
\item Por tanto, podemos ver que se cumple,
\[
\left.\frac{\partial\overline{E}}{\partial\alpha}\right|_{\beta,X}=\frac{\partial}{\partial\alpha}\brackets{-\frac{\partial\ln(Q)}{\partial\beta}}=-\frac{\partial^2\ln(Q)}{\partial\alpha\partial\beta}=-\frac{\partial^2\ln(Q)}{\partial\beta\partial\alpha}=\frac{\partial}{\partial\beta}\brackets{-\frac{\partial\ln(Q)}{\partial\alpha}}=\left.\frac{\partial\overline{N}}{\partial\beta}\right|_{\alpha,X}
\]
Luego,
\begin{equation}
    \left.\frac{\partial\overline{E}}{\partial\alpha}\right|_{\beta,X}=\left.\frac{\partial\overline{N}}{\partial\beta}\right|_{\alpha,X}
\end{equation}
\item Cálculo de la ecuación de estado:
\\ \\
Usamos la definición, sabiendo que $Y_{\alpha}=\frac{\partial H}{\partial X_{\alpha}}$
\[\overline{Y}_{\alpha}=\sum_{N=0}^{\infty}\int dqdp\frac{\partial H}{\partial X_{\alpha}}\rho(N,q,p)\overset{(\ref{ec77})}{=}-\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp\frac{\partial H}{\partial X_{\alpha}}e^{-\beta H_N}\]
Usando que $\frac{\partial H}{\partial X_{\alpha}}e^{-\beta H}=-\frac{1}{\beta}\frac{\partial e^{-\beta H}}{\partial X_{\alpha}}$ tenemos
\[\overline{Y}_{\alpha}=\cancel{-}\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp\frac{\cancel{-}1}{\beta}\frac{\partial e^{-\beta H}}{\partial X_{\alpha}}\overset{(\ref{ec76})}{=}\frac{1}{Q}\frac{1}{\beta}\brackets{\frac{\partial e^{-\beta H}}{\partial X_{\alpha}}}_{\alpha,\beta,X_{k\neq\alpha}}\]
Usando que  $\frac{1}{f(x)}\frac{\partial f(x)}{\partial x}=\frac{\partial\ln(f(x))}{\partial x}$,
\begin{equation}
    \overline{Y}_{\alpha}=\frac{1}{\beta}\brackets{\frac{\partial\ln(Q)}{\partial X_{\alpha}}}_{\alpha,\beta,X_{k\neq\alpha}}
    \label{ec86}
\end{equation}
\item Dispersión de la energía:
\\ \\
Vamos a calcular $\overline{(\Delta E)^2}$, que se define como $\overline{(\Delta E)^2}=\overline{E^2}-\overline{E}^2$. Sabemos que por la ecuación (\ref{ec81}),
\[\overline{E}^2=\brackets{\frac{\partial\ln(Q)}{\partial\beta}}_{\alpha,X}^2\]

luego, tendremos que calcular $\overline{E^2}$, partiendo de la definición de valor medio,
\[\overline{E^2}=\sum_{N=0}^{\infty}\int dqdp H_N^2(q,p)\rho(N,q,p)\overset{(\ref{ec77})}{=}\sum_{N=0}^{\infty}\int dqdp H_N^2(q,p)\frac{1}{Q}\frac{1}{h^fN!}e^{-\beta H_N-\alpha N}\]
\[\overline{E^2}=\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp H_N^2 e^{-\beta H_N}\overset{\curlybraces{He^{-\beta H}=-\frac{\partial e^{-\beta H}}{\partial\beta}}}{=}\frac{1}{Q}\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp H_N\frac{\partial e^{-\beta H_N}}{\partial\beta}\]

\[\overline{E^2}=-\frac{1}{Q}\frac{\partial}{\partial\beta}\brackets{\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp H_Ne^{-\beta H_N}}\overset{\curlybraces{He^{-\beta H}=-\frac{\partial e^{-\beta H}}{\partial\beta}}}{=}\cancel{-}\frac{1}{Q}\frac{\partial}{\partial\beta}\brackets{\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp \frac{\cancel{-}\partial e^{-\beta H_N}}{\partial\beta}}\]

\[\overline{E^2}=\frac{1}{Q}\frac{\partial}{\partial\beta}\curlybraces{\frac{\partial}{\partial\beta}\brackets{\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdp e^{-\beta H_N}}}\overset{(\ref{ec76})}{=}\frac{1}{Q}\frac{\partial}{\partial\beta}\brackets{\frac{\partial Q}{\partial\beta}}_{\alpha,X}\]
Luego,
\begin{equation}
    \overline{E^2}=\frac{1}{Q}\brackets{\frac{\partial^2 Q}{\partial\beta^2}}_{\alpha,X}
\end{equation}
Por tanto,
\[\overline{(\Delta E)^2}=\overline{E^2}-\overline{E}^2=\frac{1}{Q}\brackets{\frac{\partial^2 Q}{\partial\beta^2}}_{\alpha,X}-\brackets{\frac{\partial\ln(Q)}{\partial\beta}}_{\alpha,X}^2=\left(\frac{\partial^2\ln(Q)}{\partial\beta^2}\right)_{\alpha,X}=-\left(\frac{\partial\overline{E}}{\partial\beta}\right)_{\alpha,X}\]
Así,
\begin{equation}
    \overline{(\Delta E)^2}=-\left(\frac{\partial\overline{E}}{\partial\beta}\right)_{\alpha,X}
\end{equation}

\item Dispersión del número de partículas:
\\ \\
Vamos a calcular $\overline{(\Delta N)^2}$, que se define como $\overline{(\Delta N)^2}=\overline{N^2}-\overline{N}^2$. Sabemos por la ecuación (\ref{ec83}),
\[\overline{N}^2=\brackets{\frac{\partial\ln(Q)}{\partial\alpha}}^2_{\beta,X}\]
luego, tendremos que calcular $\overline{N^2}$, partiendo de la definición de valor medio,

\[\overline{N^2}=\sum_{N=0}^{\infty}\int dqdpN^2\rho(N,q,p)\overset{(\ref{ec77})}{=}\sum_{N=0}^{\infty}\int dqdp\frac{1}{Q}\frac{e^{-\alpha N-\beta H}}{h^fN!}N^2=\frac{1}{Q}\sum_{N=0}^{\infty}\frac{1}{h^fN!}\int dqdp e^{-\alpha N-\beta H}N^2\]
\[\overline{N^2}\overset{\curlybraces{Ne^{-\alpha N}=-\frac{\partial e^{-\alpha N}}{\partial\alpha}}}{=}\frac{1}{Q}\sum_{N=0}^{\infty}\frac{1}{h^fN!}\int dqdpN\brackets{-\frac{\partial e^{-\alpha N}}{\partial\alpha}}=-\frac{1}{Q}\frac{\partial}{\partial\alpha}\brackets{\sum_{N=0}^{\infty}\frac{1}{h^fN!}\int dqdpNe^{-\alpha N-\beta H}}\]
\[\overline{N^2}\overset{\curlybraces{Ne^{-\alpha N}=-\frac{\partial e^{-\alpha N}}{\partial\alpha}}}{=}-\frac{1}{Q}\frac{\partial}{\partial\alpha}\brackets{\sum_{N=0}^{\infty}\frac{1}{h^fN!}\int dqdp\left(-\frac{\partial e^{-\alpha N}}{\partial\alpha}\right)}=\frac{1}{Q}\frac{\partial^2}{\partial\alpha^2}\brackets{\sum_{N=0}^{\infty}\frac{1}{h^fN!}\int dqdp e^{-\alpha N-\beta H}}\]
Luego,
\begin{equation}
\overline{N^2}\overset{(\ref{ec76})}{=}\frac{1}{Q}\brackets{\frac{\partial^2\ln(Q)}{\partial\alpha^2}}_{\beta,X}
\end{equation}
Por tanto,
\[\overline{(\Delta N)^2}=\overline{N^2}-\overline{N}^2=\frac{1}{Q}\left(\frac{\partial^2Q}{\partial\alpha^2}\right)_{\beta,X}-\left(\frac{\partial\ln(Q)}{\partial\alpha}\right)^2_{\beta,X}=\left(\frac{\partial^2\ln(Q)}{\partial\alpha^2}\right)_{\beta,X}=-\left(\frac{\partial\overline{N}}{\partial\alpha}\right)^2_{\beta,X}\]
Así,
\begin{equation}
    \overline{(\Delta N)^2}=-\left(\frac{\partial\overline{N}}{\partial\alpha}\right)^2_{\beta,X}
\end{equation}
\end{enumerate}

\subsubsection*{Definición de entropía}

Partiendo de $Q=Q(\beta,X_{\alpha},\alpha_i)$ calcularemos la entropía $S$. Para simplificar los cálculos, usaremos el $\ln(Q)$. Comenzamos diferenciando el $\ln(Q)$,

\[d\ln(Q)=\left(\frac{\partial\ln(Q)}{\partial\beta}\right)_{\alpha,X}d\beta+\left(\frac{\partial\ln(Q)}{\partial X}\right)_{\beta,X}dX+\left(\frac{\partial\ln(Q)}{\partial\alpha}\right)_{\beta,X}d\alpha\]
Usando las ecuaciones (\ref{ec81}), (\ref{ec83}) y (\ref{ec86}), calculadas antes, tenemos
\[d\ln(Q)=-\overline{E}d\beta+\beta\overline{Y}dX-\overline{N}d\alpha\]

Usando que,
\[\curlybraces{\begin{matrix}
d(\beta\overline{E})=\beta d\overline{E}+\overline{E}d\beta & \Longrightarrow & \overline{E}d\beta=d(\beta\overline{E})-\beta d\overline{E} \\ \\
d(\alpha\overline{N})=\alpha\overline{N}+\overline{N}d\alpha & \Longrightarrow & \overline{N}d\alpha=d(\alpha\overline{N})-\alpha d\overline{N}
\end{matrix}}\]
tenemos
\[d\ln(Q)=-d(\beta\overline{E})+\beta d\overline{E}+\beta\overline{Y}dX-d(\alpha\overline{N})+\alpha d\overline{N}\]
\[d\ln(Q)+d(\beta\overline{E})+d(\alpha\overline{N})=\beta d\overline{E}+\beta\overline{Y}dX+\alpha d\overline{N}\]
\[d(\ln(Q)+\beta\overline{E}+\alpha\overline{N})=\cancelto{\frac{1}{kT}}{\beta}(d\overline{E}+\overline{Y}dX-\mu d\overline{N})\]
Por termodinámica, sabemos que 
\begin{equation}
    d\mathcal{Q}=d\overline{E}+\overline{Y}dX-\mu d\overline{N}
\end{equation}
siendo $\mathcal{Q}$ el calor. Por tanto,
\[kd(\ln(Q)+\beta\overline{E}+\alpha\overline{N})=\frac{d\mathcal{Q}}{T}\]
\[d(k\brackets{\ln(Q)+\beta\overline{E}+\alpha\overline{N}})=\frac{d\mathcal{Q}}{T}\]
También sabemos que
\begin{equation}
    dS=\frac{d\mathcal{Q}}{T}
\end{equation}
Por tanto,
\begin{equation}
    S=k(\ln(Q)+\beta\overline{E}+\alpha\overline{N})
\end{equation}

\subsubsection*{Potencial termodinámico $B$}

Se define como
\begin{equation}
    B=\overline{E}-TS-\sum_{i}\overline{\mu}_i\overline{N}_i
\end{equation}
con
\begin{equation}
    S=-\left(\frac{\partial B}{\partial T}\right)_{X_{k},\mu_i}\hspace{10mm}\overline{Y}_k=-\left(\frac{\partial B}{\partial X_k}\right)_{T,X_{j\neq k},\mu_i}\hspace{10mm}\overline{N}_i=-\left(\frac{\partial B}{\partial\mu_i}\right)_{T,X_k,\mu_{j\neq i}}
\end{equation}

Vamos a demostrar que,
\begin{equation}
    B=-kT\ln(Q)
\end{equation}
\[S=k(\ln(Q)+\beta\overline{E}+\alpha\overline{N})\overset{\alpha=-\beta\mu}{=}k(\ln(Q)+\beta\overline{E}-\beta\mu\overline{N})\]
\[B=\overline{E}-T\brackets{k(\ln(Q)+\beta\overline{E}-\beta\mu\overline{N})}-\mu\overline{N}\]
\[B=\cancel{\overline{E}}-kT\ln(Q)-\frac{\cancel{kT}}{\cancel{kT}}\cancel{\overline{E}}+\frac{\cancel{kT}}{\cancel{kT}}\cancel{\mu\overline{N}}-\cancel{\mu\overline{N}}\]
Luego,
\[B=-kT\ln(Q)\qed\]

Por tanto,

\[\ln(Q)=-\frac{B}{kT}\Longrightarrow Q=e^{-\frac{B}{kT}}\]
El equivalente a este potencial en el colectivo canónico es el potencial de Helmzholtz ($F$).

\subsubsection*{Energía libre de Gibbs}

Se define como

\begin{equation}
    G=\overline{E}-TS+\overline{Y}_{\alpha}X_{\alpha}
\end{equation}

Calculamos el diferencial de $G$,

\[dG=d\overline{E}-d(TS)+d(\overline{Y}_{\alpha}X_{\alpha})=d\overline{E}-TdS-SdT+\overline{Y}_{\alpha}dX_{\alpha}+X_{\alpha}d\overline{Y}_{\alpha}\]
Usando que
\[TdS=d\overline{E}-\mu d\overline{N}+\overline{Y}_{\alpha}dX_{\alpha}\]
tenemos,
\[dG=\cancel{d\overline{E}}-\cancel{d\overline{E}}+\mu d\overline{N}-\cancel{\overline{Y}_{\alpha}dX_{\alpha}}-SdT+\cancel{\overline{Y}_{\alpha}dX_{\alpha}}+X_{\alpha}d\overline{Y}_{\alpha}=-SdT+X_{\alpha  }d\overline{Y}_{\alpha}+\mu d\overline{N}\]
Luego, $G=G(\overline{N},T,\overline{Y}_{\alpha})$
\[dG=\frac{\partial G}{\partial\overline{N}}d\overline{N}+\frac{\partial G}{\partial T}dT+\frac{\partial G}{\partial\overline{Y}_{\alpha}}d\overline{Y}_{\alpha}\]
Por tanto, comparando,
\[\mu=\frac{\partial G}{\partial\overline{N}}\hspace{10mm}-S=\frac{\partial G}{\partial T}\hspace{10mm}X_{\alpha}=\frac{\partial G}{\partial\overline{Y}_{\alpha}}\]
Siendo $G$ una función extensiva. \\ \\
Las funciones extensivas tienen la característica de ser una función homogénea de grado 1 en las variables de las que depende, pues las funciones homogéneas cumplen que, $f(\lambda x)=\lambda^kf(x)$.\\ \\
El Teorema de Euler de las funciones homogéneas nos dice que $x\frac{\partial f}{\partial x}=kf$.\\ \\
Como $\overline{N}$ es extensible, entonces $G$ debe ser una función extensible de grado 1 para $\overline{N}$ y entonces,
\[\overline{N}\frac{\partial G}{\partial\overline{N}}=1\cdot G\]
así,
\begin{equation}
    G=\mu\overline{N}
\end{equation}
por tanto,
\begin{equation}
    \mu\overline{N}=\overline{E}-TS+\overline{Y}_{\alpha}X_{\alpha}
\end{equation}

Usando que $S=k(\ln(Q)+\beta\overline{E}-\mu\beta\overline{N})$, luego, sustituyendo,

\[\cancel{\mu\overline{N}}=\overline{E}-kT(\ln(Q)+\beta\overline{E}-\beta\mu\overline{N})+\overline{Y}_{\alpha}X_{\alpha}=\cancel{\overline{E}}-kT\ln(Q)-\frac{\cancel{kT}}{\cancel{kT}}\cancel{\overline{E}}+\frac{\cancel{kT}}{\cancel{kT}}\cancel{\mu\overline{N}}+\overline{Y}_{\alpha}X_{\alpha}\]
\[\overline{Y}_{\alpha}X_{\alpha}-kT\ln(Q)=0\]
Luego,
\begin{equation}
    \overline{Y}_{\alpha}X_{\alpha}=kT\ln(Q)
\end{equation}
No hay número de moléculas, pues para $H$ solo usamos la energía cinética y si hubiera moléculas, debería haber energías de enlace, etc.\\ \\
Si $X_{\alpha}=V$, entonces $\overline{Y}_{\alpha}=\overline{P}$ y entonces 
\begin{equation}
    \overline{P}V=kT\ln(Q)
\end{equation}
Para un gas ideal monoatómico, se tiene que $\overline{N}=\ln(Q)$, así, la ecuación e los gases ideales será
\begin{equation}
    \overline{P}V=\overline{N}kT
\end{equation}

\subsubsection*{Dispersión del número de partículas}

Sabemos que

\[\left(\frac{\partial\overline{N}}{\partial\mu}\right)_{\beta,V}=\left(\frac{\partial\overline{N}}{\partial\overline{P}}\right)_{T,V}=\left(\frac{\partial\overline{N}}{\partial\overline{P}}\right)_{T,V}\left(\frac{\partial\overline{P}}{\partial\mu}\right)_{T,V}\]

Por el Teorema de Schwarz establece que

\[\frac{\partial^2B}{\partial V\partial\mu}=\frac{\partial^2B}{\partial\mu\partial V}\]

Por tanto,

\[\frac{\partial}{\partial V}\brackets{\frac{\partial B}{\partial\mu}}=\frac{\partial}{\partial\mu}\brackets{\frac{\partial B}{\partial V}}\]

Luego, usando que $\overline{N}=-\frac{\partial B}{\partial\mu}$ y $\overline{P}=-\frac{\partial B}{\partial V}$, tenemos

\[\left(\frac{\partial\overline{N}}{\partial V}\right)_{T,\mu}=\left(\frac{\partial\overline{P}}{\partial\mu}\right)_{T,V}\]

\[\left(\frac{\partial\overline{N}}{\partial\mu}\right)_{\beta,V}=\left(\frac{\partial\overline{N}}{\partial\mu}\right)_{T,V}=\left(\frac{\partial\overline{N}}{\partial\overline{P}}\right)_{T,V}\left(\frac{\partial\overline{N}}{\partial V}\right)_{T,\mu}\overset{\overline{N}=nV}{=}\frac{\overline{N}}{V}\left(\frac{\partial\overline{N}}{\partial\overline{P}}\right)_{T,V}\]

\[\left(\frac{\partial\overline{N}}{\partial\mu}\right)_{\beta,V}=\overline{N}\left(\frac{\partial\frac{\overline{N}}{V}}{\partial\overline{P}}\right)_{T,V}=\overline{N}\left(\frac{\partial n}{\partial\overline{P}}\right)_{T,V}=\overline{N}\left(\frac{\partial n}{\partial\overline{P}}\right)_{T,\overline{N}}=\overline{N}^2\left(\frac{\partial\frac{1}{V}}{\partial\overline{P}}\right)_{T,\overline{N}}=-\frac{\overline{N}^2}{V^2}\left(\frac{\partial V}{\partial\overline{P}}\right)_{T,\overline{N}}\]
\\ \\
Por tanto,

\[(\overline{\Delta N})^2=\overline{N^2}-\overline{N}^2=kT\left(\frac{\partial\overline{N}}{\partial\mu}\right)_{\beta,V}=-kT\frac{\overline{N^2}}{V^2}\left(\frac{\partial V}{\partial\overline{P}}\right)_{T,\overline{N}}\]

El coeficiente de compresibilidad isotermo es $k_T=-\frac{1}{V}\left(\frac{\partial V}{\partial\overline{P}}\right)_{T,\overline{N}}$, luego

\[(\overline{\Delta N})^2=\frac{kT}{V}\overline{N^2}k_T\]

Por tanto,

\begin{equation}
    \frac{(\overline{\Delta N})^2}{\overline{N}^2}=\frac{kT}{\overline{N}}nk_T
\end{equation}

\chapter{ TEMA 3: Termodinámica estadística de sistemas ideales y sistemas débilmente interactivos}
\section{Gases ideales}

A fin de aclarar ideas, vamos obtener las funciones más relevantes para los gases ideales monoatómicos en los distintos colectivos vistos en el Tema 2. Consideraremos en todos los casos un sistema formado por $N$ partículas iguales que no interaccionan entre sí y están encerradas en un recinto de volumen $V$.
\subsubsection*{Colectivo microcanónico}
Vamos a calcular el volumen fásico $\Gamma(E)$.\\
Usando la definición,
\begin{equation}
    \Gamma(E)=\int_{E_0}^{E}dE'\Omega(E')
\end{equation}
sabemos además que
\begin{equation}
    \Omega(E)=\int dqdp\delta\curlybraces{E-H(q,p)}
\end{equation}
Luego, sustituyendo
\[\Gamma(E)=\int_{E_0}^{E}dE'\Omega(E')\int dqdp\delta\curlybraces{E-H(q,p)}=\int_{E_0\leq H\leq E}dqdp\]
Calculamos el Hamiltoniano, usando que
\begin{equation}
    H=\sum_{i=1}^{N}\sum_{j=1}^{N}\frac{p_{ij}^2}{2m}+U(\vec{r})
\end{equation}
con
\begin{equation}
    U(\vec{r})=\left\lbrace\begin{matrix}
        0 & \vec{r}_i\in V,\hspace{2mm} \forall i\\
        \infty & \text{en otro caso}
    \end{matrix}\right.
\end{equation}
Además, tomaremos $E_0=0$, pues cuando todas las partículas están paradas la energía cinética es cero y la potencial también lo es, pues no interaccionan entre ellas.\\
Luego,
\[\Gamma(E)=\int dqdp_{E_0\leq H(q,p)\leq E}=\int_{0\leq H(q,p)\leq E}d^3\vec{r}_1,\dots,d^3\vec{r}_N,d^3\vec{p}_1,\dots,d^3\vec{p}_N\]
Como el Hamiltoniano no depende de la posición, hacemos 
\[\int d^3\vec{r}_i=\int dxdydz=V\]
Así,
\[\Gamma(E)=V^N\int_{0\leq H(q,p)\leq E}d^3\vec{p}_1,\dots,d^3\vec{p}_N\]
Para calcular la integral resultante, vemos cómo se comporta geométricamente para los distintos rangos de energía.\\ \\
\begin{tabular}{c|}
    $H=0$ \\ \hline
\end{tabular}
\[\frac{p_{ix}^2}{2m}+\frac{p_{iy}^2}{2m}+\frac{p_{iz}^2}{2m}=0\]
\[p_{ix}^2+p_{iy}^2+p_{iz}^2=0\]
Vemos que esta ecuación tiene la forma de un punto centrado en el origen y de radio cero.\\ \\
\begin{tabular}{c|}
     $H=E$ \\ \hline
\end{tabular}
\[\frac{p_{ix}^2}{2m}+\frac{p_{iy}^2}{2m}+\frac{p_{iz}^2}{2m}=E\]
\[p_{ix}^2+p_{iy}^2+p_{iz}^2=2mE\]
Vemos que esta ecuación tiene la forma de una hipersuperficie esférica de radio $r=\sqrt{2mE}$. Entonces, tendremos que calcular el volumen de una hipersuperficie esférica de radio $(2mE)^{1/2}$ de dimensión $3N$.\\ \\
Sabemos que el hipervolumen de la esfera será constante por un radio elevado a su dimensión, tal que
\[\Gamma(E)=V^NC(2mE)^{3N/2}\]
Para calcular $C$, cogemos una integral que conozcamos su resultado, después, la transformamos en esféricas y calculamos la zona radial y el ángulo,
\[J=\int_{-\infty}^{+\infty}dx_1dx_2\dots dx_fe^{-(x_1^2x_2^2\dots x_f^2)}=\int_{-\infty}^{+\infty}dx_1e^{-x_1^2}\int_{-\infty}^{+\infty}dx_2e^{-x_2^2}\dots\int_{-\infty}^{+\infty}dx_Ne^{-x_N^2}=\]
\[=\curlybraces{\int_{-\infty}^{\infty}dxe^{-x^2}}^f=(\sqrt{\pi})^f=\pi^{f/2}\]
Pasamos a esféricas para sacar los ángulos,
\[J=\int_{0}^{+\infty}r^{f-1}drd\Omega e^{-r^2}\]
donde el $d\Omega$ representa el ángulo sólido, tal que $\int_{0}^{+\infty}d\Omega=S_f$, luego,
\[J=S_f\int_{0}^{+\infty}r^{f-1}e^{-r^2}dr=S_f\frac{\Gamma(f/2)}{2\cdot 1^{f/2}}=S_f\frac{\Gamma(f/2)}{2}\]
Igualando ambos resultados,
\[\pi^{f/2}=S_f\frac{\Gamma(f/2)}{2}\Rightarrow S_f=\frac{2\pi^{f/2}}{\Gamma(f/2)}\overset{\curlybraces{f=3N}}{=}\frac{2\pi^{\frac{3N}{2}}}{\Gamma\left(\frac{3N}{2}\right)}\approx\frac{2\pi^{\frac{3N}{2}}}{\frac{3N}{2}!}\]
Luego, tomamos $C=S_f$, y así nos queda el \textbf{volumen fásico de un gas ideal monoatómico en el colectivo microcanónico}
\begin{equation}
    \Gamma(E)=\frac{V^N\pi^{\frac{3n}{2}}}{\frac{3N}{2}!}(2mE)^{\frac{3N}{2}}
\end{equation}
Para obtener $\Omega(E)$, despejamos de la ecuación (1), aplicando Leibtniz, 
\begin{equation}
    \Omega(E)=\frac{\partial\Gamma(E)}{\partial E}
\end{equation}
que queda
\begin{equation}
    \Omega(E)=\frac{V^N\pi^{\frac{3N}{2}}}{\frac{3N}{2}!}\frac{3N}{2}2m(2mE)^{\frac{3N}{2}}=\frac{2mV^N\pi^{\frac{3N}{2}}}{\left(\frac{3N}{2}-1\right)!}(2mE)^{\frac{3N}{2}-1}
\end{equation}
\subsubsection*{Colectivo canónico}
Vamos a calcular la función de partición, considerando que el gas monoatómico está a temperatura $T$.\\
Primero, calculamos el Hamiltoniano del sistema, tal que
\[H(q,p)=\sum_{i=1}^N\frac{p_i^2}{2m}\]
La función de partición del sistema la calculamos partiendo de la definición,
\begin{equation}
    Z=\frac{1}{h^{3N}N!}\int d\vec{r}_1\dots d\vec{r}_Nd\vec{p}_1\dots d\vec{p}_N e^{-\beta H(\vec{r}_1,\dots,\vec{r}_N,\vec{p}_1,\dots,\vec{p}_N)}
\end{equation}
Tal que
\[Z=\frac{1}{h^{3N}N!}\int d^3\vec{r}_1\dots d^3\vec{r}_Nd^3\vec{p}_1\dots d^3\vec{p}_N e^{-\beta\brackets{\frac{\vec{p_1}^2+\dots+\vec{p_N}^2}{2m}}}=\]
\[=\frac{1}{h^{3N}N!}\int d^3\vec{p}_1e^{-(\beta/2m)\vec{p_1}^2}\dots\int d^3\vec{p}_N e^{-(\beta/2m)\vec{p_N}^2}\int d^3\vec{r}_1\dots d^3\vec{r}_N\]
donde usamos que 
 \[\int d^3\vec{r}_1\dots d^3\vec{r}_N=\int d^3\vec{r}_1\dots \int d^3\vec{r}_N=V^N\]
y las integrales de $p$ son todas idénticas, luego es un producto de $N$ integrales iguales, definiendo
\begin{equation}
\xi=\frac{V}{h^3}\int d^3\vec{p}e^{-(-\beta/2m)\vec{p}^2}
\end{equation}
que podemos resolver la integral,
\[\int d^3\vec{p}e^{-(-\beta/2m)p^2}=\int\int\int_{-\infty}^{+\infty}dp_xdp_ydp_ze^{-(\beta/2m)(p_x^2+p_y^2+p_z^2)}=\brackets{\int_{-\infty}^{+\infty}dpe^{-(\beta/2m)p^2}}^3=\left(\frac{2\pi m}{\beta}\right)^{\frac{3}{2}}\]
Así queda,
\begin{equation}
    Z=\frac{\xi^N}{N!}
\end{equation}
con
\begin{equation}
    \xi=\frac{V}{h^3}\left(\frac{2\pi m}{\beta}\right)^{\frac{3}{2}}
\end{equation}

\subsubsection*{Colectivo macrocanónico}
Vamos a obtener la gran función de partición del colectivo macrocanónico para gases ideales.\\ 
Partimos de su definición,
\begin{equation}
    Q=\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}\int dqdpe^{-\beta H_N}=\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{h^fN!}Z_N
\end{equation}
con
\begin{equation}
    Z_N=\int dqdpe^{-\beta H_N}
\end{equation}
Calculamos $Z_N$,
\[Z_N=\int d^3\vec{r}_1\dots d^3\vec{r}_N d^3\vec{p}_1\dots d^3\vec{p}_N e^{-\beta\sum_{i=0}^N\frac{\vec{p}_i^2}{2m}-\beta\sum_{i=0}^NU(\vec{r}_i)}\]

\[Z_N=\int d^3\vec{r}_1\dots d^3\vec{r}_N e^{-\beta U(\vec{r}_1)-\dots-\beta U(\vec{r}_N)}\int d^3\vec{p}_1\dots d^3\vec{p}_N e^{-\beta\frac{\vec{p}_1^2}{2m}-\dots-\beta\frac{\vec{p}_N^2}{2m}}\]
donde vemos que la integral de $r$ es la misma multiplicada $N$ veces y la de $p$ igual, así queda
\[Z_N=\brackets{\int d^3\vec{r} e^{-\beta U(\vec{r})}}^N\brackets{\int d^3\vec{p} e^{-\beta\frac{\vec{p}^2}{2m}}}^N=\brackets{\int d^3\vec{r} e^{-\beta U(\vec{r})}}^N\left(\frac{2\pi m}{\beta}\right)^{\frac{3N}{2}}\]
Usando que el potencial es
\[U(\vec{r})=\left\lbrace\begin{matrix}
        0 & \vec{r}_i\in V,\hspace{2mm} \forall i\\
        \infty & \text{en otro caso}
    \end{matrix}\right.\]
tenemos,
\[Z_N=\brackets{\int_{V}d^3\vec{r}}^N\left(\frac{2\pi m}{\beta}\right)^{\frac{3N}{2}}=\left(\frac{2\pi m}{\beta}\right)^{\frac{3N}{2}}V^N=\curlybraces{\sum_{N=0}^{\infty}\frac{e^{-\alpha }}{N!}\left(\frac{2\pi m}{h^2\beta}\right)^{\frac{3}{2}}V}^N\]
Así, queda
\[Q=\sum_{N=0}^{\infty}\frac{e^{-\alpha N}}{N!}\left(\frac{2\pi m}{h^2\beta}\right)^{\frac{3N}{2}}V^N\]
como $e^x\approx\sum_{n=0}^{\infty}\frac{x^n}{n!}$, tenemos que
\[Q\approx e^{e^{-\alpha }\left(\frac{2\pi m}{h^2\beta}\right)^{\frac{3}{2}}V}\]
Luego, tenemos
\begin{equation}
    \ln{Q}=e^{-\alpha}\left(\frac{2\pi m}{h^2\beta}\right)^{\frac{3}{2}}V
\end{equation}
\section{Teorema de equipartición}
El teorema de equipartición es una fórmula general que relaciona la temperatura de un sistema con su energía media. Para demostrarlo partimos del colectivo canónico y calculamos $\overline{x_i\frac{\partial H}{\partial x_j}}$ (solo es aplicable si las coordendas $q$ son lineales). Para ello,
\[\overline{x_i\frac{\partial H}{\partial x_j}}=\int dqdp x_i\frac{\partial H}{\partial x_j}\rho(q,p)=\int dqdp x_i\frac{\partial H}{\partial x_j}\frac{1}{Z}e^{-\beta H}\]
usando que
\[\frac{\partial e^{-\beta H}}{\partial x_j}=e^{-\beta H}\left(-\beta\frac{\partial H}{\partial x_j}\right)=-\beta\frac{\partial H}{\partial x_j}e^{-\beta H}\]
entonces
\[\frac{\partial H}{\partial x_j}e^{-\beta H}=-\frac{1}{\beta}\frac{\partial e^{-\beta H}}{\partial x_j}\]
luego,
\[\overline{x_i\frac{\partial H}{\partial x_j}}=\int dx_1\dots dx_{2f}\cdot x_i\frac{1}{Z}\left(-\frac{1}{\beta}\frac{\partial e^{-\beta H}}{\partial x_j}\right)=-\frac{kT}{Z}\int dx_1\dots dx_{2f}\cdot x_i\frac{\partial e^{-\beta H}}{\partial x_j}\]
Como tenemos que hacer todas las integrales, vamos a empezar la correspondiente a $x_j$, pues tenemos la derivada respecto a $x_j$,
\[\int_{-\infty}^{+\infty}dx_j\cdot x_i\frac{\partial e^{-\beta H}}{\partial x_j}=\curlybraces{\begin{matrix}
    u=x_i & du=\frac{\partial x_i}{\partial x_j}dx_j\\
    dv=\frac{\partial e^{-\beta H}}{\partial x_j} & v=e^{-\beta H}
\end{matrix}}=\]
\[=\cancelto{0}{\left.x_i e^{-\beta H}\right|_{-\infty}^{+\infty}}-\int_{-\infty}^{+\infty}e^{-\beta H}\frac{\partial x_i}{\partial x_j}dx_j=-\int_{-\infty}^{+\infty}e^{-\beta H}\delta_{ij}dx_j\]
donde hemos hecho cero el primer término para que tengamos definidas las magnitudes físicas. Así,
\[-\frac{kT}{Z}\int dx_1\dots dx_{2f}\cdot x_i\frac{\partial e^{-\beta H}}{\partial x_j}=\frac{kT}{Z}\int dx_1\dots dx_{2f}\delta_{ij}e^{-\beta H}=kT\delta_{ij}\int dx_1\dots dx_{2f} \frac{e^{-\beta H}}{Z}=\]
\[=kT\delta_{ij}\cancelto{1}{\int dx_1\dots dx_{2f} \rho(x_1,\dots,x_{2f})}=kT\delta_{ij}\]
donde hemos cancelado la integral, pues la densidad de probabilidad debe estar normalizada.\\
Así, el \textbf{Teorema de equipartición generalizado} queda
\begin{equation}
    \overline{x_i\frac{\partial H}{\partial x_j}}=kT\delta_{ij}
\end{equation}
- Si hacemos $x_i=x_j=q_i$, obtenemos el \textbf{Teorema del Virial},
\begin{equation}
    \overline{q_i\frac{\partial H}{\partial q_i}}=kT
\end{equation}
- Si hacemos $x_i=x_j=p_i$, obtenemos el \textbf{Teorema de equipartición},
\begin{equation}
    \overline{p_i\frac{\partial H}{\partial p_i}}=kT
\end{equation}
\subsubsection*{Nota}
Sabemos que 
\[H=T+U=\sum^fp\frac{\partial H}{\partial p}-T+U\]
entonces, $2T=\sum^fp\frac{\partial H}{\partial p}$, luego,
\[\overline{T}=\overline{\frac{1}{2}\sum^fp\frac{\partial H}{\partial p}}=\frac{1}{2}\sum^f\overline{p\frac{\partial H}{\partial p}}=\frac{1}{2}fkT\]
usando el Teorema de equipartición. En el caso de un gas ideal, su energía es solo energía cinética, por tanto,
\[\overline{E_m}=\overline{T+\cancelto{0}{U}}=\overline{T}=\frac{1}{2}fkT\]
\[\overline{T}=\overline{\frac{1}{2}mv^2}=\frac{1}{2}m\overline{v}^2\]
Luego, podemos relacionar $\overline{v}$ con $T$, de la forma
\begin{equation}
    \overline{v}=\sqrt{\frac{fkT}{m}}
\end{equation}
\section{Teoría cinética de gases ideales en equilibrio}
Vamos a introducir la notación que usaremos,
\[\vec{r}\equiv\text{posición del centro de masas de una partícula}\]
\[\vec{p}\equiv\text{cantidad de movimiento del centro de masas de una partícula}\]
\[q_{gint},p_{gint}\equiv\text{coordenadas y momentos generalizados asociados con los grados internos de libertad}\]
El Hamiltoniano de una partícula del gas será,
\[H_i=\frac{\vec{p}_i^2}{2m}+H_{gint}(q_{gint},p_{gint})\]
Tendremos que limitar el sistema con un volumen dado y vamos a considerar que está en contacto con un foco térmico, es decir, estamos en el colectivo canónico, con $\beta=\frac{1}{kT}$.\\ \\
Consideramos que nuestro sistema está formado por una partícula del gas y que el resto de partículas constituye el foco térmico que está en contacto con la partícula que queremos estudiar. El foco térmico tendrá una temperatura $T$.\\ \\
Entonces, la probabilidad de encontrar la partícula en el estado dado por
\[\begin{matrix}
    \vec{r}\in\brackets{\vec{r},\vec{r}+d\vec{r}}; & q_{gint}\in\brackets{q_{gint},q_{gint}+dq_{gint}}\\
    \vec{p}\in\brackets{\vec{p},\vec{p}+d\vec{p}}; & p_{gint}\in\brackets{p_{gint},p_{gint}+dp_{gint}}
\end{matrix}\]
Para ello, tendremos una densidad dada por
\[\rho(\vec{r},\vec{p},q_{gint},p_{gint})\]
Así, la probabilidad será
\[\rho(\vec{r},\vec{p},q_{gint},p_{gint})d^3\vec{r}d^3\vec{p}dq_{gint}dp_{gint}\]

Como estamos en el colectivo canónico, esta probabilidad es proporcional a $e^{-\beta H}$, sustituyendo $H$ por el de una partícula del gas, tal que
\[\rho(\vec{r},\vec{p},q_{gint},p_{gint})d^3\vec{r}d^3\vec{p}dq_{gint}dp_{gint}\propto e^{-\beta\brackets{\frac{\vec{p}^2}{2m}+H_{gint}(q_{gint},p_{gint})}}d^3\vec{r}d^3\vec{p}dq_{gint}dp_{gint}\]
La probabilidad de encontrar la partícula en el estado dado por $\vec{r}\in\brackets{\vec{r},\vec{r}+d\vec{r}}$ y $\vec{p}\in\brackets{\vec{p},\vec{p}+d\vec{p}}$, sin importar el estado interno de la partícula, será
\[\rho(\vec{r},\vec{p})d^3\vec{r}d^3\vec{p}\propto\brackets{\int e^{-\beta H_{gint}(q_{gint},p_{gint})}dq_{gint}dp_{gint}}e^{-\beta\frac{\vec{p}^2}{2m}}d^3\vec{r}d^3\vec{p}\]
como la integral es un número, tendremos
\[\rho(\vec{r},\vec{p})d^3\vec{r}d^3\vec{p}\propto e^{-\beta\frac{\vec{p}^2}{2m}}d^3\vec{r}d^3\vec{p}\]
por tanto,
\[\rho(\vec{r},\vec{p})d^3\vec{r}d^3\vec{p}=Ce^{-\beta\frac{\vec{p}^2}{2m}}d^3\vec{r}d^3\vec{p}\]
Multiplicando por $N$, tenemos el número medio de partículas cuyo centro de masas está entre $\vec{r}\in\brackets{\vec{r},\vec{r}+d\vec{r}}$ y $\vec{p}\in\brackets{\vec{p},\vec{p}+d\vec{p}}$.

\subsection{Función de distribución de velocidades de Maxwell}

La función de distribución de velocidades de Maxwell será el número medio de partículas cuyo centro de masas está en una posición $\vec{r}\in\brackets{\vec{r},\vec{r}+d\vec{r}}$ y tiene un velocidad $\vec{v}\in\brackets{\vec{v},\vec{v}+d\vec{v}}$.\\ \\
Para ello, usamos que $\vec{p}=m\vec{v}$, entonces la función de distribución de velocidades será,
\[f(\vec{r},\vec{v})d^3\vec{r}d^3\vec{v}=CNe^{-\beta\frac{mv^2}{2}}d^3\vec{r}d^3\vec{v}\]
Para obtener la constante $C$, deberemos aplicar la condición de normalización para el número medio de partículas, por tanto, deberá ser igual a $N$, tal que
\[N=\int f(\vec{r},\vec{v})d^3\vec{r}d^3\vec{v}\]
luego,
\[C\cancel{N}\cancelto{V}{\int d^3\vec{r}}\int d^3\vec{v}e^{-\beta\frac{mv^2}{2}}=\cancel{N}\]
Luego,
\[\frac{1}{C}=V\int dv_xdv_ydv_z e^{-\beta\frac{m(v_x^2+v_y^2+v_z^2)}{2}}=V\brackets{\int_{-\infty}^{+\infty}dv e^{-\beta\frac{mv^2}{2}}}^3=V\brackets{\frac{\Gamma(1/2)}{\left(\frac{m\beta}{2}\right)^{1/2}}}^3\]
Por tanto,
\[C=\frac{1}{V}\left(\frac{m\beta}{2\pi}\right)^{3/2}\]
Entonces, la \textbf{función de distribución de velocidades de Maxwell} será,
\begin{equation}
    f(\vec{r},\vec{v})=n\left(\frac{m\beta}{2\pi}\right)^{3/2}e^{-\beta\frac{mv^2}{2}}
\end{equation}
usando que $n=\frac{N}{V}$.
\\
Como vemos, la función de distribución no depende de $\vec{r}$, por tanto el sistema es homogéneo, y tampoco depende de $\vec{v}$, por lo que el sistema también será isótropo. Estamos trabajando en seis dimensiones.

\subsection{Función de distribución de una componente de la velocidad}

El número de partículas por unidad de volumen que tiene una componente $i$ de la velocidad en el intervalo $\brackets{v_i,v_i+dv_i}$ sin importar el valor de las otras componentes es
\[g(v_x)dv_x=\frac{1}{V}\int d^3\vec{r}\int_{v_y}\int_{v_z}f(\vec{v})d^3\vec{v}\]
Al aplicarle la distribución de Maxwell queda
\begin{equation}
    g(v_x)dv_x=n\left(\frac{m\beta}{2\pi}\right)^{1/2}e^{-\beta\frac{mv_x^2}{2}}dv_x
\end{equation}
Vamos a calcular los siguientes valores medios,

\[\overline{v_x}=\int d^3\vec{r}d^3\vec{v}v_xf(\vec{r},v)\frac{1}{N}=\frac{1}{\cancel{N}}\int d^3\vec{r}\int d^3\vec{v} v_x \frac{\cancel{N}}{V}\left(\frac{\beta m}{2\pi}\right)^{3/2}e^{-\beta\frac{mv^2}{2}}=\]
\[=\frac{1}{\cancel{V}}\left(\frac{\beta m}{2\pi}\right)^{3/2}\cancel{V}\int dv_xdv_ydv_z v_x e^{-\beta\frac{mv^2}{2}}=\left(\frac{\beta m}{2\pi}\right)^{3/2}\int_{-\infty}^{+\infty}dv_ye^{-\beta\frac{mv^2}{2}}\int_{-\infty}^{+\infty}dv_ze^{-\beta\frac{mv^2}{2}}\int_{-\infty}^{+\infty}dv_x v_xe^{-\beta\frac{mv^2}{2}}\]
pero sabemos que al ser una función impar en un intervalo par, se nos anulará la integral,
\[\int_{-\infty}^{+\infty}dv_x v_xe^{-\beta\frac{mv^2}{2}}=0\]
por tanto,
\begin{equation}
    \overline{v_x}=0
\end{equation}
Ahora calcularemos el valor medio $\overline{(\Delta v_x)^2}$, tal que
\[\overline{(\Delta v_x)^2}=\sqrt{(\overline{v_x^2}-\overline{v_x}^2)^2}=\overline{v_x^2}-\cancelto{0}{\overline{v_x}^2}=\overline{v_x^2}\]
luego, calculamos
\[\overline{v_x^2}=\int d^3\vec{r}d^3\vec{v}v_x^2\frac{f(\vec{r},v)}{N}=\frac{1}{\cancel{N}}\int d^3\vec{r}\int d^3\vec{v} v_x^2\frac{\cancel{N}}{V}\left(\frac{\beta m}{2\pi}\right)^{3/2}e^{-\beta\frac{mv^2}{2}}=\]
\[=\frac{1}{\cancel{V}}\left(\frac{\beta m}{2\pi}\right)^{3/2}\cancel{V}\int dv_xdv_ydv_z v_x^2e^{-\beta\frac{mv^2}{2}}=\left(\frac{\beta m}{2\pi}\right)^{3/2}\int_{-\infty}^{+\infty}dv_ye^{-\beta\frac{m v^2}{2}}\int_{-\infty}^{+\infty}dv_ze^{-\beta\frac{m v^2}{2}}\int_{-\infty}^{+\infty}dv_x v_x^2e^{-\beta\frac{m v^2}{2}}=\]
\[=\left(\frac{\beta m}{2\pi}\right)^{3/2}\left(\frac{2\Gamma(1/2)}{2\left(\frac{\beta m}{2}\right)^{1/2}}\right)^2\left(2\frac{\Gamma(3/2)}{2\left(\frac{\beta m}{2}\right)^2{3/2}}\right)=\frac{1}{2}\frac{2}{\beta m}=\frac{kT}{m}\]
Por tanto,
\begin{equation}
    \overline{(\Delta v_x)^2}=\frac{kT}{m}
\end{equation}

\subsection{Función de distribución del módulo de la velocidad}

Ahora hablaremos de la función de distribución del módulo de la velocidad. El número de partículas por unidad de volumen con una velocidad cuyo módulo $v$ está en el intervalo $\brackets{v,v+dv}$ es

\[F(v)dv=\frac{1}{V}\int d^3\vec{r}\int_{v<\abs{\vec{v}}<v+dv}f(\vec{v})d^3\vec{v}\]
Usamos coordenadas esféricas, para tener por un lado el módulo $r$ y por otro lado las direcciones $\theta$ y $\varphi$. Así,
\[d^3\vec{v}=\sin\theta d\theta d\varphi v^2 dv\]
Tal que,
\[F(v)=\frac{1}{V}V\cancelto{4\pi}{\int d\Omega} \int n\left(\frac{\beta m}{2\pi}\right)^{3/2}e^{-\beta\frac{mv^2}{2}}v^2dv=4\pi n\left(\frac{m\beta}{2\pi}\right)^{3/2}\int v^2 e^{-\beta\frac{mv^2}{2}} dv \]
Luego,
\begin{equation}
    F(v)dv=4\pi n\left(\frac{m\beta}{2\pi}\right)^{3/2} v^2 e^{-\beta\frac{mv^2}{2}} dv
\end{equation}

Vamos a calcular los siguientes valores,\\
-Valor más probable $\tilde{v}$, que lo calculamos derivando $F(v)$ respecto de $v$ e igualando a cero, tal que

\[0=\frac{F(v)}{dv}=\frac{d}{dv}\left(4\pi n\left(\frac{m\beta}{2\pi}\right)^{3/2} v^2 e^{-\beta\frac{mv^2}{2}}\right)=\cancel{4\pi n\left(\frac{m\beta}{2\pi}\right)^{3/2}}\brackets{2\cancel{v} \cancel{e^{-\beta\frac{mv^2}{2}}}-\beta mv^{\cancel{3}} \cancel{e^{-\beta\frac{mv^2}{2}}}}\]
Luego,
\[-\beta mv^2+2=0\Rightarrow v^2=\frac{2}{m\beta}=\frac{2kT}{m}\]
Por tanto, el valor más probable será
\begin{equation}
    \tilde{v}=\sqrt{\frac{2kT}{m}}
\end{equation}
-Valor medio $\overline{v}$, aplicando la definición de valor medio,
\[\overline{v}=\frac{1}{N}\int vF(v)dv=4\pi n\left(\frac{m\beta}{2\pi}\right)^{3/2}\int_{0}^{+\infty} v^3 e^{-\beta\frac{mv^2}{2}}dv\]
Usando que 
\[\int_{0}^{+\infty}x^me^{-ax^2}dx=\frac{\Gamma\left(\frac{m+1}{2}\right)}{2a^{\frac{m+1}{2}}}\]
tenemos que
\[\overline{v}=\cancel{4}\pi\left(\frac{m\beta}{2\pi}\right)^{3/2}\frac{\Gamma(2)}{\cancel{2}\left(\frac{\beta m}{2}\right)^2}=\frac{\cancel{4}\cancel{\pi}}{\cancel{2}}\frac{1}{\pi^{\cancel{3/2}}}\frac{1}{\left(\frac{\beta m}{2}\right)^{1/2}}=\frac{2\sqrt{2}}{\sqrt{\pi}}\left(\frac{kT}{m}\right)^{1/2}=\sqrt{\frac{8kT}{m\pi}}\]
Por tanto,
\begin{equation}
    \overline{v}=\sqrt{\frac{8kT}{m\pi}}
    \label{ec25}
\end{equation}
-Velocidad cuadrática media $\sqrt{(\overline{\Delta v})^2}$, que la calculamos usando,
\[\sqrt{(\overline{\Delta v})^2}=\overline{v^2}-\overline{v}^2\]
Ya sabemos que
\[\overline{v}=\sqrt{\frac{8kT}{m\pi}}\Rightarrow\overline{v}^2=\frac{8kT}{m\pi}\]
Además,
\[\overline{v^2}=\overline{v_x^2+v_y^2+v_z^2}=\overline{v_x^2}+\overline{v_y^2}+\overline{v_z^2}\]
que sabiendo de antes que
\[\overline{v_x^2}=\frac{kT}{m}\]
tenemos
\[\overline{v^2}=\frac{kT}{m}+\frac{kT}{m}+\frac{kT}{m}=\frac{3kT}{m}\]
Por tanto,
\[\sqrt{(\overline{\Delta v})^2}=\overline{v^2}-\overline{v}^2=\frac{3kT}{m}-\frac{8kT}{m\pi}\]
Luego,
\begin{equation}
    \sqrt{(\overline{\Delta v})^2}=\frac{kT}{m}\brackets{3-\frac{8}{\pi}}
\end{equation}

\subsection{Número de colisiones contra una superficie $dS$}

\begin{multicols}{2}

Consideraremos un gas ideal encerrado en un cierto recinto. Vamos a calcular el número de moléculas que por unidad de tiempo chocan contra un elemento de superficie $dS$ de dicho recinto. Para ello, vamos a escoger el eje $z$ normal al elemento de superficie considerado y dirigido hacia el exterior del recinto, como se muestra en la Figura \ref{fig:algo}
\\ \\
Consideraremos una partícula que se mueve a velocidad $\vec{v}$, tal que $\vec{v}$ apunta hacia el origen de coordenadas.
\begin{Figura}
    \centering
    \includegraphics[width=1\textwidth]{image (2).png}
    \captionof{figure}{Cilindro auxiliar para el cálculo del número de moléculas de velocidad $\vec{v}$ que en un intervalo de tiempo $dt$ chocarán con el elemento de superficie $dS$.}
    \label{fig:algo}
\end{Figura}

\end{multicols}

El número de partículas contenidas en el volumen del cilindro de base $dS$ y eje $vdt$, será
\begin{equation}
    f(\vec{v})d^3\vec{v}(dSvdt\cos\theta)
\end{equation}

Definimos $\Phi(\vec{v})d^3\vec{v}$ como el número de partículas con velocidad $\vec{v}\in\brackets{\vec{v},\vec{v}+d\vec{v}}$ que colisionan con la superficie $dS$ por unidad de tiempo y unidad de área, tal que
\begin{equation}
    \Phi(\vec{v})d^3\vec{v}=\frac{f(\vec{v})d^3\vec{v}(\cancel{dS}v\cancel{dt}\cos\theta)}{\cancel{dS}\cancel{dt}}=f(\vec{v})v\cos\theta d^3\vec{v}
\end{equation}

Para calcular el número total de partículas que chocan con las paredes por unidad de tiempo y unidad de área, definimos
\begin{equation}
    \Phi_0=\int_{v_z>0}\Phi(\vec{v})d^3\vec{v}
\end{equation}
imponiendo que $v_z>0$ porque queremos que la partícula vaya hacia la superficie $dS$. Lo calculamos,
\[\Phi_0=\int_{v_z>0}f(\vec{v})v\cos\theta d^3\vec{v}=\int_{v_z>0}f(\vec{v})v\cos\theta v^2\sin\theta d\theta d\varphi dv=\int_{0}^{+\infty}F(v)v^3dv\int_{0}^{\pi/2}\cos\theta\sin\theta d\theta\int_{0}^{2\pi} d\varphi\]
\[\Phi_0=\brackets{\frac{\sin^2\theta}{2}}_{0}^{\pi/2}\brackets{\varphi}_{0}^{2\pi}\int_{0}^{+\infty}f(v)v^3dv=\pi\int_{0}^{+\infty}F(v)v^3dv\]
Recordando que
\[\overline{v}=\frac{1}{n}\int f(\vec{v})vd^3\vec{v}=\frac{1}{n}\int_{0}^{+\infty}F(v)v^3dv\int_{0}^{\pi}\sin\theta d\theta\int_{0}^{2\pi}d\varphi=\frac{4\pi}{n}\int_{0}^{+\infty}F(v)v^3dv\]
Despejando la integral,
\[\int_{0}^{+\infty}F(v)v^3dv=\frac{\overline{v} n}{4\pi}\]
Igualando con $\Phi_0$,
\begin{equation}
    \Phi_0=\cancel{\pi}\brackets{\frac{\overline{v} n}{4\cancel{\pi}}}=\frac{1}{4}n\overline{v}
\end{equation}
Este resultado siempre será válido para cualquier función de distribución $F(v)$ que esté normalizada, pero sí tiene una restricción y es que $F$ debe depender únicamente del módulo $v$.\\ \\

Vamos a obtener el valor d $\Phi_0$ para la distribución de velocidades de Maxwell, sabiendo que el valor medio de $v$ vale, (ec. \ref{ec25})

\[\overline{v}=\sqrt{\frac{8kT}{m\pi}}\]

sustituimos,

\[\Phi_0=\frac{n}{4}\sqrt{\frac{8kT}{m\pi}}=\sqrt{\frac{n^2kT}{2m\pi}}\]

sabiendo que la ecuación de estado de los gases ideales es $\overline{P}V=NkT$, o lo que es lo mismo, $\overline{P}=nkT$, tenemos que

\begin{equation}
    \Phi_0=\frac{\overline{P}}{\sqrt{2m\pi kT}}
\end{equation}

\section{Gases reales diluidos}

Son gases con baja densidad. En los gases reales sí hay interacciones entre las moléculas del gas.\\ \\
Para estudiar un sistema físico real, analizamos un modelo microscópico para calcular $Z$ y así, analizar las propiedades físicas. Si el modelo es correcto lo dirán los experimentos y en la mayoría de los casos tendremos hipótesis para llegar a calcular las propiedades físicas.\\ \\
En este apartado, nuestro sistema será un gas monoatómico. Supondremos que son partículas sin carga eléctrica en el seno de un potencial central $U(\vec{r}_1,\vec{r}_2)=U(\abs{\vec{r}_1-\vec{r}_2})$, así, los potenciales solo dependen de la distancia de separación entre las partículas.
\subsection{Función de partición configuracional}
Una vez planteado el problema, sacamos la función de partición configuracional $Z_U$, partiendo del Hamiltoniano,
\[H(q,p)=\sum_{i=1}^{N}\frac{\vec{p}_i^2}{2m}+\sum_{i=1}^{N}\sum_{j>i}^{N}U(\abs{\vec{r}_i-\vec{r}_j})\equiv\sum_{i=1}^{N}\frac{\vec{p}_i^2}{2m}+\sum_{i=1}^{N}\sum_{j=i+1}^{N}U(\abs{\vec{r}_i-\vec{r}_j})\equiv\sum_{i=1}^{N}\frac{\vec{p}_i^2}{2m}+\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}U(\abs{\vec{r}_i-\vec{r}_j})=K+U\]

Aplicamos la definición de $Z$,
\[Z=\frac{1}{h^{3N}N!}\int dqdp e^{-\beta H(q,p)}=\frac{1}{h^{3N}N!}\int d^3\vec{r}_1\dots d^3\vec{r}_Nd^3\vec{p}_1\dots d^3\vec{p}_Ne^{-\beta(K+U)}=Z_TZ_U\]
tal que
\begin{equation}
Z_T=\frac{1}{h^{3N}N!}\int d^3\vec{p}_1\dots d\vec{p}_Ne^{-\beta K}
\end{equation}
y la función de partición configuracional será,
\begin{equation}
Z_U=\int d^3\vec{r}_1\dots d^3\vec{r}_Ne^{-\beta U}
\end{equation}

Para calcular $Z_U$, hacemos un desarrollo en la densidad, tal que, partimos de un sistema de $N$ partículas y suponemos que $U_0\lll m\frac{\overline{v_0}}{2}$ y cogemos $T\ggg$ para que no se formen \textit{agregados de partículas}.
\\ \\
\subsection{Desarollo en la densidad}
Sabemos que para que dos partículas interaccionen entre sí, con un potencial como el de la Figura \ref{fig:algos2}, su distancia de separación debe ser del orden de $r_0$.
\begin{Figura}
        \centering
        \includegraphics[width=0.5\textwidth]{dd.png}
        \captionof{figure}{Potencial del tipo 'Morse'}
        \label{fig:algos2}
    \end{Figura}    

Por tanto, la probabilidad de que la partícula $j$ interaccione con la partícula $i$, se puede reducir a la probabilidad de que $j$ esté a una distancia menor que $r_0$ de $i$, que es la probabilidad de que $j$ esté en una esfera de radio $r_0$ y centro en la partícula $i$, siendo el volumen de esta esfera del orden de $r_0^3$ y como el volumen total del sistema es $V$, la probabilidad será del orden $\backsim\frac{r_0^3}{V}$ (casos favorables partido casos totales). Vamos a calcular la probabilidad de forma inductiva:
\\ \\
- \underline{Probabilidad de colisión de 2 partículas}\\ \\

El número de colisiones binarias es el número de partículas por probabilidad de una colisión, tal que
\[\frac{N!}{(N-2)!2!}\frac{r_0^3}{V}=\frac{N(N-1)}{2}\frac{r_0^3}{V}\overset{\curlybraces{\begin{matrix}
    N\ggg\\
    N\approx N-1
\end{matrix}}}{\approx}\frac{N^2}{2}\frac{r_0^3}{V}\overset{\curlybraces{n=\frac{N}{V}}}{=}\frac{N}{2}nr_0^3\]

- \underline{Probabilidad de colisión de 3 partículas}\\ \\

Primero vemos cuál es la probabilidad de interacción de tres partículas, que será que al menos una de las tres esté dentro de alguna de las esferas de radio $r_0$ de las otras dos, es decir, será 
\[\frac{r_0^3}{V}\frac{r_0^3}{V}=\left(\frac{r_0^3}{V}\right)^2\]
Ahora, vemos que le número de tríos es
\[\frac{N!}{(N-3)3!}\]
Por último, el número de tríos por la probabilidad de una colisión será,
\[\frac{N!}{(N-3)!3!}\left(\frac{r_0^3}{V}\right)^2=\frac{N(N-1)(N-2)}{3!}\left(\frac{r_0^3}{V}\right)^2\overset{\curlybraces{\begin{matrix}
    N\ggg\\
    N\approx N-1\\
    N\approx N-2
\end{matrix}}}{\approx}\frac{N^3}{3!}\left(\frac{r_0^3}{V}\right)^2\overset{\curlybraces{n=\frac{N}{V}}}{=}\frac{N}{3!}(nr_0^3)^2\]

- \underline{Probabilidad de colisión de $p$ partículas}\\ \\

De forma análoga a las anteriores hacemos:
\begin{enumerate}[1º)]
    \item Vemos que  la probabilidad de interacción entre $p$ partículas es \[\left(\frac{r_0^3}{V}\right)^{p-1}\]
    \item Vemos que el número de grupos es
    \[\frac{N!}{(N-p)!p!}\]
    \item Vemos que el número medio de colisiones entre $p$ partículas es
    \[\frac{N!}{(N-p)!p!}\left(\frac{r_0^3}{V}\right)^{p-1}\approx\frac{N}{p!}(nr_0^3)^{p-1}\]
\end{enumerate}

Si el número de partículas en el volumen $r_0^3$, es decir, $nr_0^3$, es mucho menor que 1, entonces la probabilidad de que dos partículas interaccionen será muy pequeña. Así,
\[nr_0^^3\lll1\Rightarrow\frac{N}{V}r_0^3\lll1\Rightarrow\frac{V}{N}\ggg r_0^3\]
siendo esta la ecuación del gas diluido. Como el volumen de cada partícula es mucho mayor que la distancia de interacción, entonces no hay apenas interacciones; predominan las colisiones binarias.

\subsection{Función de Mayer}

Entonces la función de partición configuracional se queda,

\begin{equation*}
    Z_U=\int d^3\vec{r}_1\dots d^3\vec{r}_N e^{-\beta U}=\int d^3\vec{r}_1\dots d^3\vec{r}_N e^{-\beta\sum_{1\leq i\leq j\leq N}}u(r_{ij})=\int d^3\vec{r}_1\dots d^3\vec{r}_N\prod_{1\leq i\leq j\leq N}e^{-\beta u(r_{ij}}
\end{equation*}
Para seguir desarrollando $Z_U$ definimos la función de Mayer $f(r)$ como
\begin{equation}
    f(r)=e^{-\beta u(r)}-1\hspace{4mm}\begin{matrix}
        f(r)=0 & si & u(r)=0\\
        f(r)\approx0 & si & r>r_0
    \end{matrix}
\end{equation}
\\ \\
Visualmente, esa función será la apreciada en la Figura \ref{fig:enter-label3},
\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{jj.png}
    \captionof{figure}{Función de Mayer frente a un potencial central.}
    \label{fig:enter-label3}
\end{Figura}

Por tanto, 
\[Z_U=\int d^3\vec{r}_1\dots d^3\vec{r}_N\prod_{1\leq i\leq j\leq N}(1+f(r_{ij}))\]
Desarrollamos el productorio,
\[\prod_{1\leq i\leq j\leq N}(1+f(r_{ij}))=(1+f(r_{12}))(1+f(r_{13}))\dots(1+f(r_{1N}))(1+f(r_{23}))\dots(1+f(r_{N-1N}))=\]
\[=1+f(r_{12})+f(r_{13})+\dot+f(r_{N-1N})+f(r_{12})f(r_{13})+\dots=1+\sum_{1\leq i\leq j\leq N}f(r_{ij})+\sum_{1\leq i\leq j\leq N}\sum_{1\leq l\leq m\leq N}f(r_{ij})f(r_{lm})+\dots\]
Entonces,
\[Z_U=\int d^3\vec{r}_1\dots d^3\vec{r}_N\brackets{1+\sum_{1\leq i\leq j\leq N}f(r_{ij})+\sum_{1\leq i\leq j\leq N}\sum_{1\leq l\leq m\leq N}f(r_{ij})f(r_{lm})+\dots}\]
\subsection{Segundo coeficiente del Virial}
\[Z_U=\int d^3\vec{r}_1\dots d^3\vec{r}_N+\sum_{1\leq i\leq j\leq N}\int d^3\vec{r}_1\dots d^3\vec{r}_Nf(r_{ij})+\cancelto{0}{\sum_{1\leq i\leq j\leq N}\sum_{1\leq l\leq m\leq N}\int d^3\vec{r}_1\dots d\vec{r}_Nf(r_{ij})f(r_{lm})+\dots}\]

despreciamos los términos de orden superior. Así,

\[Z_U=\cancelto{V^N}{\int d^3\vec{r}_1\dots d^3\vec{r}_N}+\sum_{1\leq i\leq j\leq N}\int d^3\vec{r}_1\dots d^3\vec{r}_Nf(r_{ij})=V^N+\sum_{1\leq i\leq j\leq N}\int d^3\vec{r}_1\dots d^3\vec{r}_i\dots d^3\vec{r}_j\dots d^3\vec{r}_Nf(r_{ij})\]
Hacemos las integrales que no dependan de $i$ ni de $j$ y tenemos,
\[Z_U=V^N+\sum_{1\leq i\leq j\leq N}V^{N-2}\int d^3\vec{r}_i d^3\vec{r}_j f(r_{ij})\]
Como todas estas integrales son iguales, tendremos $\binom{N}{2}=\frac{N!}{(N-2)!2!}$ integrales, tal que
\[Z_U=V^N+\frac{N!}{(N-2)!2!}V^{N-2}\int d^3\vec{r}_1d^3\vec{r}_2 f(r_{12})\]
Calculamos esta integral usando que $f(r_{12})$ depende solo de la distancia de $\vec{r}_1$ con $\vec{r}_2$, pasando a coordenadas de centro de masas y relativas, $\vec{R}=\frac{\vec{r}_1-\vec{r}_2}{2}$ y $\vec{r}=\vec{r}_1-\vec{r}_2$. Luego,
\[\int d^3\vec{r}_1 d^3\vec{r}_2 f(r_{12})=\int d^3\vec{R}d^3\vec{r}f(r)=\cancelto{V}{\int d^3\vec{R}}\int d^3\vec{r} f(r)=V\int d^3\vec{r}f(r)=\]
Pasamos a coordenadas esféricas,
\[=V\int r^2\sin\theta d\theta d\varphi f(r)dr=V\int_{0}^{\pi}\sin\theta d\theta\int_{0}^{2\pi} d\varphi\int r^2f(r)dr=4\pi V\int r^2f(r)dr\]
Por tanto,
\begin{equation}
Z_U=V^N+\frac{N(N-1)}{2}V^{N-1}\int 4\pi r^2f(r)dr=V^N\brackets{1+\frac{N(N-1)}{2V}\int 4\pi r^2f(r)dr}
\end{equation}
En conjunto, la función de partición quedará como,
\begin{equation}
    Z=Z_UZ_T=\frac{(2\pi mkT)^{3/2}V^N}{h^{3N}N!}\brackets{1+\frac{N(N-1)}{2V}\int 4\pi r^2f(r)dr}=Z_{\text{gas ideal}}\brackets{1+\frac{N(N-1)}{2V}\int 4\pi r^2f(r)dr}
\end{equation}
pues efectivamente, vemos que el primer producto es la función de partición de un gas ideal, cosa que tiene sentido, pues en el límite cuando no haya interacción, es decir, $f(r)=0$, el gas se deberá comportar como un gas ideal.
\\ \\
Calculamos el logaritmo neperiano de $Z$, tal que
\[\ln{Z}=\ln{Z_{gi}}+\ln\brackets{1+\frac{N(N-1)}{2V}\int 4\pi r^2f(r)dr}\]
Usando Taylor, $\ln{(1+x)}\approx x$ y también aproximamos $N(N-1)\approx N^2$, luego
\[\ln{Z}\approx\ln{Z_{gi}}+\frac{N^2}{2V}\int 4\pi r^2 f(r)dr\]
Una vez que tenemos la función de partición, podemos calcular las diferentes magnitudes físicas, como $\overline{P}$,
\[\overline{P}=\frac{1}{\beta}\left.\frac{\partial\ln{Z}}{\partial V}\right|_{\beta}=\frac{1}{\beta}\left.\frac{\partial \ln{Z_{gi}}}{\partial V}\right|_{\beta}+\frac{1}{\beta}\frac{\partial}{\partial V}\left(\frac{N^2}{2V}\int 4\pi r^2f(r)dr\right)_{\beta}=\frac{\overline{N}kT}{V}-\frac{N^2kT}{2V^2}\int 4\pi r^2f(r)dr\]
Entonces, llamaremos
\begin{equation}
    \frac{\overline{P}V}{\overline{N}kT}=1+B_2(T)\frac{N}{V}
\end{equation}
siendo el \textbf{desarrollo del Virial} y con
\begin{equation}
    B_2(T)=-2\pi\int r^2f(r)dr
\end{equation}
que es el \textbf{segundo coeficiente del Virial}.\\ \\
Según el potencial que cojamos nos dará un valor u otro, y esto servirá para ver si el potencial elegido es adecuado o no.

\chapter{TEMA 4: Fundamentos de la\\ Física Estadística Cuántica y Operador Densidad}
\section{Repaso de nociones elementales de
Mecánica Cuántica}

Como no hemos visto mecánica cuántica, vamos a hacer un repaso de las nociones que nos serán útiles para la asignatura.
\\ \\
En clásica, la descripción del estado de un sistema físico lo hacíamos con el espacio fásico y la representación de las magnitudes las hacíamos como funciones de las coordenadas generalizadas del espacio fásico. No había restricciones en cuanto a la energía, cantidad de movimiento, etc. Para ver la evolución temporal, se usaban las ecuaciones de Hamilton. Vamos a ver sus análogos en Mecánica Cuántica.

\subsection{Descripción del estado de un sistema
físico}

El estado de un sistema físico con $f$ grados de libertad está descrito por una función de las coordenadas generalizadas y del tiempo, que se denomina función de onda, $\Psi(q_1,q_2,\dots,q_f;t)\in\mathbb{C}$. Su significado físico será que el módulo al cuadrado de la función de oda determina la distribución de probabilidad de que una medida realizada sobre el sistema resulte en valores de las $q_i$ comprendidos entre $q_i$ y $q_i+dq_i$, tal que
\begin{equation}
    \left|\Psi\right|^2dq_1dq_2\dots dq_f=\Psi^*\Psi dq_1dq_2\dots dq_f
\end{equation}

\subsection{Representación de las magnitudes
físicas}

A toda magnitud física medible en un sistema cuántico, le corresponde un operador. Así, dada una magnitud clásica que, en general tendrá la forma $A(\vec{r},\vec{p},t)$, el operador asociado a la correspondiente magnitud cuántica tiene la forma,
\begin{equation}
    \hat{A}=A(\hat{r},-i\hbar\vec{\nabla},t)
\end{equation}

\subsection{Medida de las magnitudes físicas}

Los únicos resultados posibles al medir una magnitud física, en un sistema, representada por el operador $\hat{A}$, son los autovalores del mismo.\\ \\
Estos autovalores deberán ser reales, pues representan una medida. Si la función de onda que representa el estado del sistema es función propia
del operador que representa la magnitud física que se mide, el resultado de
la medida es con toda certeza el autovalor a que corresponde la función de
onda.\\ \\
Si en un sistema representado por la función de onda $\Psi$ medimos la variable
física $A$, el resultado de la medida responde a una distribución de
probabilidades, y su valor medio es:
\begin{equation}
    \overline{a_{\Psi}}=\int\Psi^*\hat{A}\Psi dq_1dq_2\dots dq_f
    \label{ec4-3}
\end{equation}
En resumen, si tenemos un estado del sistema $\Psi(q,t)$ y una magnitud física $A$, relacionada con un operador $\hat{A}=A(\hat{r},-i\hbar\vec{\nabla},t)$, al hacer la medida de $A$, si $\Psi$ es autofunción de $\hat{A}$, entonces $a$ es un autovalor tal que $\hat{A}x=ax$ y si $\Psi$ no es autofunción, tendremos la ecuación \ref{ec4-3}.

\subsection{Evolución temporal de un sistema físico}

La función de onda de un sistema físico cuántico evoluciona en el tiempo de
acuerdo con la ecuación de Schrödinger, que es el análogo a la ecuación de Hamilton en clásica:
\begin{equation}
    i\hbar\frac{\partial\Psi}{\partial t}=\hat{H}\Psi
\end{equation}
siendo $\hat{H}$ el operador asociado al Hamiltoniano.
\subsubsection*{Ejemplo}
Si tenemos una partícula libre, su Hamiltoniano es
\[H=\frac{p^2}{2m}\]
pero como estamos en cuántica, tenemos que $\vec{p}=-i\hbar\vec{\nabla}$, luego,
\[\left|p\right|^2=(-i\hbar\vec{\nabla})(-i\hbar\vec{\nabla})=-\hbar^2\left(\frac{\partial}{\partial x},\frac{\partial}{\partial y},\frac{\partial}{\partial z}\right)\left(\frac{\partial}{\partial x},\frac{\partial}{\partial y},\frac{\partial}{\partial z}\right)=\]
\[=-\hbar^2\brackets{\frac{\partial}{\partial x}\frac{\partial}{\partial x}+\frac{\partial}{\partial y}\frac{\partial}{\partial y}+\frac{\partial}{\partial z}\frac{\partial}{\partial z}}=-\hbar^2\brackets{\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}}=-\hbar^2\nabla^2\]
Luego, el operador Hamiltoniano para una partícula libre será
\[\hat{H}=-\frac{\hbar^2}{2m}\nabla^2\]
tal que
\[\hat{H}\Psi=E\Psi\hspace{5mm}\left\lbrace\begin{matrix}
    \Psi(\vec{r},t)=Ae^{i(\vec{k}\cdot\vec{r}-Et)}\\
    E=\frac{\hbar k^2}{2m}
\end{matrix}\right.\]
donde $k$ puede ser cualquier valor, $k\in\mathbb{R}$.\\
Así, $E$ puede tomar cualquier valor, es decir, la energía tendrá un espectro continuo.

\subsubsection*{Ejemplo}

Si tenemos una partícula libre encerrada en una caja, determinada por un pozo de potencial infinito, cuyo Hamiltoniano es
\[H=\frac{p^2}{2m}+U(\vec{r})\hspace{10mm}U(\vec{r})=\left\lbrace\begin{matrix}
    0 & si \left\lbrace\begin{matrix}
        -a<x<a\\
        -b<x<b\\
        -c<x<c
    \end{matrix}\right.\\ \\
    +\infty & \text{en otro caso}
\end{matrix}\right.\]

Entonces, para $\hat{H}\Psi=E\Psi$, tendremos que
\[\Psi(\vec{r},t)=\sqrt{\frac{8}{abc}}\sin(\frac{\pi n_x}{a}x)\sin(\frac{\pi n_y}{b}y)\sin(\frac{\pi n_z}{c}z)\]
Por tanto, la energía será
\[E_{n_x,n_y,n_z}=\frac{\pi^2\hbar^2}{2m}\left(\frac{n_x^2}{a^2}+\frac{n_y^2}{b^2}+\frac{n_z^2}{c^2}\right)\hspace{7mm}n_x,n_y,n_z\in\mathbb{N}\]
Como estamos en una caja, $a=b=c=L$, así,
\[E_{n_x,n_y,n_z}=\frac{\pi^2\hbar^2}{2mL^2}\brackets{n_x^2+n_y^2+n_z^2}\]
Así, vemos que la energía toma valores discretos, es decir, tiene un espectro discreto. Es más, vemos que no importa el orden de los $n_i$, pues obtenemos siempre el mismo resultado, lo que nos indica que tiene un espectro \textit{degenerado}. 
\subsubsection*{Ejemplo}
Si tenemos un oscilador armónico en una dimensión, tendremos
\[\hat{H}=\frac{\hat{p}^2}{2m}+\frac{m\omega x^2}{2}\]
con 
\[E_n=\left(n+\frac{1}{2}\right)\hbar\omega\hspace{7mm} n\in\mathbb{N}\cup\curlybraces{0}\]
es decir, la energía tiene un espectro discreto.\\ \\
Si tenemos un oscilador armónico en tres dimensiones, tendremos
\[\hat{H}=\frac{\hat{p}^2}{2m}+\frac{m\omega r^2}{2}\]
con
\[E_n=(n_x+n_y+n_z+\frac{3}{2})\hbar\omega\]
es decir, la energía tiene un espectro discreto y degenerado.

\subsection{Momento angular orbital}

Sabemos que el momento angular es
\[\vec{L}=\vec{r}\times\vec{p}=(yp_z-zp_y)\hat{e}_x+(zp_x-xp_z)\hat{e_y}+(xp_y-yp_x)\hat{e}_z\]
Si tenemos dos operadores $\hat{A}$ y $\hat{B}$, definimos el conmutador de operadores como
\begin{equation}
    \brackets{\hat{A},\hat{B}}=\hat{A}\hat{B}-\hat{B}\hat{A}
\end{equation}
tal que
\[\brackets{\hat{A},\hat{B}}\Psi=\hat{A}\hat{B}\Psi-\hat{B}\hat{A}\Psi\neq0\]
en la mayoría de situaciones. Que no conmuten los operadores implica que sus magnitudes asociadas no se puedan medir simultáneamente (Principio de incertidumbre).\\ \\
Además, se puede comprobar que las componentes del momento angular no conmutan, luego, no podemos conocer al mismo tiempo $L_x$, $L_y$ y $L_z$. Pero $\hat{L^2}$ es un operador que sí conmuta con las componentes, entonces, para describir el momento angular en cuántica se usa $\hat{L^2}$ y una de las componentes de $\vec{L}$ (normalmente $L_z$). Se tiene,
\[\hat{L^2}\Psi=l(l+1)\hbar^2\Psi\hspace{7mm}l=0,1,2,3,\dots\]
\[\hat{L_z}\Psi=m\hbar\Psi\hspace{5mm}m=-l,-l+1,\dots,0,\dots,l-1,l\]

Para cada valor de $l$, tendremos $2l+1$ valores de $m$ posibles.\\ \\

La composición de momentos angulares se hace tal que

\[\begin{matrix}
    \hat{L_1^2}\Psi_1=l_1(l_1+1)\hbar^2\Psi_1; & \hat{L}=\hat{L_1}+\hat{L_2}\\ \\
    \hat{L_2^2}\Psi_2=l_2(l_2+1)\hbar^2\Psi_2; & l=l_1+l_2,l_1+l_2-1,\dots,\abs{l_1-l_2}
\end{matrix}\]
\subsubsection*{Nota}
Si nos dicen 'tenemos un sistema con momento angular igual a dos/tres/...', se refiere a $l=2,3,...$

\subsection{El espín}

No tiene análogo en clásica. El espín es el momento angular intrínseco de las partículas, es decir, es una propiedad de las partículas como lo son la masa, carga, etc. y esta propiedad se comporta como un momento angular.\\ \\
Es distinto al momento angular orbital, pues este solo surge cuando la partícula está en movimiento, mientras que el espín está siempre.\\ \\
Lo representamos como $\hat{s}$ y no conmuta. Tenemos,
\[\hat{s^2}\Psi=s(s+1)\hbar^2\Psi\]
\[\hat{s_z}\Psi=s_z\hbar\Psi\]
así, todas las propiedades del momento angular orbital son aplicables con el espín.\\ \\
Los posibles valores son $s=0,\frac{1}{2},1,\frac{3}{2},\dots$ que son resultados experimentales. Siempre que combinemos partículas con espín cero, la resultante tendrá espín cero.\\ \\
El momento angular total será
\begin{equation}
    \hat{J}=\hat{L}+\hat{s}
\end{equation}

\section{Sistemas de partículas idénticas}
\subsubsection*{Principio de identidad de partículas}
'En un sistema formado por partículas idénticas, sólo son posibles aquellos
estados del sistema que no cambian cuando se intercambian entre si dos
partículas idénticas.'
\\ \\
Por tanto, no importa qué partículas está en cada estado, sino cuántas hay en ese estado. Luego, $\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N)$ y $\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_i,\dots,\vec{r}_j,\dots\vec{r}_N)$ deberán ser idénticos, pero la función de onda no está bien definida, pues si tomamos
\[\Psi_1=\Psi\rightarrow\abs{\Psi_1}^2=\Psi^*\Psi\]
\[\Psi_2=\Psi e^{i\alpha}\rightarrow\abs{\Psi_2}^2=\Psi^*\cancel{e^{-i\alpha}}\Psi\cancel{e^{i\alpha}}=\Pi^*\Psi=\abs{\Psi_1}^2!!!!\]
Así, vemos que tenemos funciones de onda distintas, pero llevan a la misma física, por esto, la función de onda está mal definida, pues le falta un factor de fase. Tomaremos,

\[\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N,t)=e^{i\alpha}\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_i,\dots,\vec{r}_j,\dots\vec{r}_N,t)\]
Si volvemos a intercambiar,
\[\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N,t)=e^{2i\alpha}\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N,t)\]
pero como hemos llegado a la posición original, entonces $e^{2i\alpha}=1$, por tanto,
\begin{equation}
    e^{i\alpha}=\pm1
\end{equation}
El hecho de que $e^{i\alpha}=1$ ó $e^{i\alpha}=-1$ nos lleva a pensar que debemos de tener dos tipos de partículas, es decir,
\begin{itemize}
    \item Si $e^{i\alpha}=1$,
    \begin{enumerate}[(i)]
        \item $\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N,t)=\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_i,\dots,\vec{r}_j,\dots\vec{r}_N,t)$
        \item La función de onda es simétrica respecto al intercambio de partículas.
        \item Estas partículas son los \textbf{Bosones}.
        \item Tienen espín entero.
    \end{enumerate}
    \item Si $e^{i\alpha}=-1$,
    \begin{enumerate}[(i)]
        \item $\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_j,\dots,\vec{r}_i,\dots\vec{r}_N,t)=-\Psi(\vec{r}_1,\vec{r}_2,\dots,\vec{r}_i,\dots,\vec{r}_j,\dots\vec{r}_N,t)$
        \item La función de onda es antisimétrica respecto al intercambio de partículas.
        \item Estas partículas son los \textbf{Fermiones}.
        \item Tienen espín semientero.
    \end{enumerate}
\end{itemize}

Los Fermiones nos llevan al Principio de Exclusión de Pauli, que nos dice que en un mismo orbital no podemos tener dos electrones con números cuánticos iguales.

\subsection{Las colectividades de equilibrio en Mecánica Estadística Cuántica}

El estado de nuestro sistema viene descrito por la función de onda $\Psi(\vec{r}_1,\dots,\vec{r}_N,t)$ (puede haber factor de fase). Entonces, si conocemos $\Psi$ conocemos el macroestado de nuestro sistema, pero al revés no es cierto.\\ \\
Para ello, introducimos el conjunto de sistemas en estados cuánticos compatibles con el macroestado y una distribución de probabilidad para esos estados.\\ \\
El carácter probabilístico en Mecánica Estadística Cuántica tiene dos caracteres, el propio carácter cuántico, pues aún conociendo la función de onda no tenemos asegurado el resultado de una medida salvo que $\Psi$ sea autofunción del operador asociado a la magnitud a medir, y el carácter estadístico.

\subsection{Colectividad micocanónica}

Tenemos un sistema aislado con energía comprendida entre $E$ y $E+dE$. Así, los microestados (funciones de onda) deberán ser un conjunto de autoestados de $H$, independientes entre sí, con $\hat{H}\Psi=E\Psi$.\\ \\
Tenemos el \textbf{Principio de igualdad de probabilidades a priori}:\\ \\
'Un sistema aislado en equilibrio puede encontrarse con la
misma probabilidad en cualquiera de los estados estacionarios
independientes que le son accesibles.'\\ \\

Así, aplicando el principio de igualdad de probabilidades a priori en nuestro sistema, puede encontrarse con la misma probabilidad en cualquiera de los estados estacionarios independientes que le son accesibles, tal que
\[\mathcal{P}(R)=\left\lbrace\begin{matrix}
    \frac{1}{\Omega(E)} & si & E<E_R<E+dE\\
    0 & si & E_R\notin\brackets{E,E+dE}
\end{matrix}\right.\]
siendo $R$ un estado cuántico y $\Omega(E)$ es el número de estados estacionarios independientes del sistema en el intervalo de energía $E$ y $E+dE$. Entonces, como en clásica, tendremos
\begin{equation}
    \beta=\frac{1}{kT}=\left.\frac{\partial\ln{\Omega}}{\partial E}\right|_{X_{\alpha}}
\end{equation}
\begin{equation}
    \overline{Y}_{\alpha}=\frac{1}{\beta}\left.\frac{\partial\ln{\Omega}}{\partial X}\right|_{E}
\end{equation}
\begin{equation}
    \mu=-kT\left.\frac{\partial\ln{\Omega}}{\partial N}\right|_{E,X_{\alpha}}
\end{equation}
\begin{equation}
    S(E,X_{\alpha})=k\ln{\Omega}(E,X_{\alpha})
\end{equation}
que está bien definida.

\subsection{Colectividad canónica}

Tenemos un sistema aislado compuesto por dos subsistemas $A_1$ y $A_2$, tal que \[\hat{H}=\hat{H}_1+\hat{H}_2+\hat{H}_{12}\approx\hat{H}_1+\hat{H}_2\]
donde $\hat{H}_1$ es el operador correspondiente al Hamiltoniano de $A_1$, $\hat{H}_2$, el correspondiente a $A_2$ y $\hat{H}_{12}$, el correspondiente a la interacción entre $A_1$ y $A_2$, y vamos a considerar que $\hat{H}_1,\hat{H}_2\gg\hat{H}_{12}$.
\\ \\
Los niveles de energía vendrán dados por $\hat{H}\Psi=E\Psi$ y como $\hat{H}=\hat{H}_1+\hat{H}_2$, tendremos que
\[\left\lbrace\begin{matrix}
\Psi=\varphi_1\varphi_2 & \hat{H}_1\varphi_1=E_1\varphi_1\\
E=E_1+E_2 & \hat{H}_2\varphi_2=E_2\varphi_2
\end{matrix}\right.
\]
Al igual que en clásica, planteamos que $A_2\gg A_1$, siendo $A_2$ un foco térmico, obteniendo así la probabilidad
\begin{equation}
    \mathcal{P}(R)=\frac{\Omega_2(E-E_1,R)}{\Omega(E)}=\frac{e^{-\beta E_R}}{\sum_R e^{-\beta E_R}}
\end{equation}
siendo el $\sum_R$ una suma para todos los estados cuánticos.
\\ \\
Definimos la función de partición cuántica del colectivo canónico como
\begin{equation}
    Z=\sum_R e^{-\beta E_R}
\end{equation}
que sería una suma para estados, o bien como
\begin{equation}
    Z=\sum_{E_R}g(E_R)e^{-\beta E_R}
\end{equation}
que sería una suma para energías, donde hemos introducido $g(E_R)$ como un factor de degeneración, pues podremos tener varios estados en la misma energía y sin este factor, al hacer el sumatorio perderíamos información.
\subsubsection*{Aclaración}
Estamos usando sumatorios y no integrales para describir las energías, porque en cuántica la energía no es continua como ocurre en clásica, por lo que al ser discreta, discretamos la integral clásica, pasándola a sumatorio. 
\\ \\
En este colectivo obtenemos todas las expresiones de la colectividad canónica clásica, pues todas son válidas, tal que
\begin{equation}
    \overline{Y}_{\alpha}=\frac{1}{\beta}\left.\frac{\partial\ln{Z}}{\partial X_{\alpha}}\right|_{\beta}
\end{equation}
\begin{equation}
    \overline{E}=-\left.\frac{\partial\ln{Z}}{\partial \beta}\right|_{X_{\alpha}}
\end{equation}
\begin{equation}
    k(\ln{Z}+\beta\overline{E})
\end{equation}
\subsection{Colectividad canónica generalizada}
Tendremos una probabilidad de los estados cuánticos tal que
\begin{equation}
    \mathcal{P}(R)=\frac{1}{Q}e^{-\alpha N_R-\beta E_R}
\end{equation}
donde $Q$ es la gran función de partición cuántica, descrita por
\begin{equation}
    Q=\sum_{N=0}^{\infty}e^{-\alpha N_R}\sum_{R}e^{-\beta E_R}
\end{equation}
cuyas ecuaciones son
\begin{equation}
    \overline{E}=-\left.\frac{\partial\ln{Z}}{\partial \beta}\right|_{\alpha,X}
\end{equation}
\begin{equation}
    \overline{N}=-\left.\frac{\partial\ln{Q}}{\partial \alpha}\right|_{\beta,X}
\end{equation}
\begin{equation}
    \overline{Y}=\frac{1}{\beta}\left.\frac{\partial\ln{Q}}{\partial X}\right|_{\beta,\alpha,X}
\end{equation}
\begin{equation}
    S=k(\ln{Q}+\beta\overline{E}+\sum_i\alpha_i\overline{N}_i)
\end{equation}
\begin{equation}
    \overline{P}V=kT\ln{Q}
\end{equation}

\section{Operador densidad}

Vamos a estudiar un sistema cuántico formado por $N$ partículas, suponiendo que estamos en el estado puro, es decir, conocemos la función de onda completamente, $\Psi(q_1,q_2,\dots,q_f,t)=\Psi(\xi_1,\xi_2,\dots,\xi_N)$, tal que al resolver la ecuación de Schrödinger, tenemos
\[i\frac{\partial\Psi}{\partial t}=\hat{H}\Psi\rightarrow\Psi(\xi,t)=\sum_nc_n(t)\varphi_n(\xi)\]
Esto quiere decir que cuando tenemos un sistema $\hat{H}\varphi_n=E_n\varphi_n$, al resolverlo, obtenemos los autovalores $E_n$ que le corresponderán a los autovectores $\varphi_n$, por tanto, el conjunto $\curlybraces{\varphi_n}$ será una base de autovectores del espacio de funciones de onda. 
\\ \\
Así, una función de onda la podemos expresar como
\begin{equation}
    \Psi=\sum_nc_n(t)\varphi_n
\end{equation}
Una vez conocido el sistema, podemos calcular el valor de una magnitud física $'a'$, que tendrá asociada un operador $\hat{A}$, así, para calcular esta magnitud, calculamos su valor medio
\[\overline{a}=\int d\xi\Psi^*\hat{A}\Psi\]
sustituimos $\Psi=\sum_nc_n(t)\varphi_n$ y $\Psi^*=\sum_mc_m^*(t)\varphi_m^*$
\[\overline{a}=\int d\xi\sum_mc_m^*(t)\varphi_m^*\hat{A}\sum_nc_n(t)\varphi_n=\sum_n\sum_m\int d\xi c_m^*\varphi^*\hat{A}c_n\varphi_n\]
para calcularlo, calculamos lo de dentro de la integral primero,
\[c_m^*\varphi_m^*\hat{A}c_n\varphi_n=c_m^*\varphi_m^*c_n\hat{A}\varphi_n=c_m^*c_n\varphi_m^*\hat{A}\varphi_n\]
Luego,
\[\overline{a}=\sum_n\sum_mc_m^*c_n\int d\xi\varphi_m^*\hat{A}\varphi_n=\sum_n\sum_Nc_m^*c_na_{mn}\]
donde hemos sacado $c_m^*$ y $c_n$ de la integral, pues solo dependen de $t$ y $a_{mn}$ es la descripción matricial de un operador cuyos elementos son los coeficientes de $\hat{A}$.
\\ \\
Podemos interpretar los elementos de $B=\curlybraces{\varphi_n}$, que puede ser de dimensión infinita, como vectores, tales que
\[\begin{matrix}
    \varphi_1=(1,0,0,\dots)\\
    \varphi_2=(0,1,0,\dots)\\
    \vdots
\end{matrix}\]
y hemos definido $a_{mn}$ como los elementos que representan el operador $\hat{A}$, tal que
\[\hat{A}=\begin{pmatrix}
    a_{11} & a_{12} & a_{13} & \dots\\
    a_{21} & a_{22} & a_{23} & \dots\\
    \vdots & \vdots & \vdots & \ddots
\end{pmatrix}\]
\subsubsection*{Notación de Dirac}
En la notación de Dirac, podemos representar las funciones de onda como 'vectores', 
\[\begin{matrix}
    \Psi & \rightarrow & \ket{\Psi}\\
    \Psi^* & \rightarrow & \bra{\Psi}\\
    \hat{A}\Psi & \rightarrow & \hat{A}\ket{\Psi}\\
    \int\Psi^*\hat{A}\Psi & \rightarrow & \bra{\Psi}\hat{A}\ket{\Psi}
\end{matrix}\]
tal que $\ket{\Psi}$ se denomina 'ket' y $\bra{\Psi}$ se denomina 'bra', tal que los vectores ket son vectores columna y los bra, son fila. En mecánica cuántica, puede escribirse todo de forma matricial.
\\ \\
Seguimos con nuestro sistema aislado cuántico formado por $N$ partículas y supondremos que tenemos un estado mezcla, es decir, conocemos la probabilidad $\gamma_i$ de que, en un instante determinado ($t=0$), el sistema está en el estado $\Psi^{(i)}(\xi,t=0)$. Entonces, tenemos un \textbf{colectivo de Gibbs} en mecánica cuántica, donde al macroestado le corresponden microestados compatibles con el macroestado. Como a cada estado del sistema le corresponde una probabilidad $\gamma_i$, tendremos
\[\begin{matrix}
    \Psi^{(1)}(\xi,0) & \gamma_1\\
    \Psi^{(2)}(\xi,0) & \gamma_2\\
    \vdots & \vdots\\
    \Psi^{(k)}(\xi,0) & \gamma_k\\
\end{matrix}\]
\[\gamma_i\geq0\Rightarrow \sum_i\gamma_i=1\]
Teniendo $i-$ecuaciones de Schrödinger, tales que
\[i\frac{\partial\Psi^{(i)}}{\partial t}=\hat{H}\Psi^{(i)}\hspace{10mm}\Psi^{(i)}=\sum_nc_n^{(i)}(t)\varphi_n(\xi)\]
Como no sabemos en qué estado se encuentra el sistema, necesitamos calcular el valor medio de la magnitud que queramos calcular, que será el conjunto de todos los $i-$valores medios,
\[\overline{a^{(i)}}=\int d\xi\Psi^{*(i)}\hat{A}\Psi^{(i)}=\sum_n\sum_mc_m^{*(i)}c_n^{(i)}a_{mn}
\]
Luego, el conjunto será la suma de todos los valores medios por la probabilidad de obtener cada valor,
\[\overline{a}=\sum_i\gamma_i\overline{a^{(i)}}=\sum_i\gamma_i\sum_n\sum_mc_m^{*(i)}c_n^{(i)}a_{mn}=\sum_n\sum_ma_{mn}\rho_{nm}\]
donde definimos el operador densidad,
\begin{equation}
    \rho_{nm}=\sum_i\gamma_ic_m^{*(i)}c_n^{(i)}
\end{equation}
Esta expresión del valor medio se asemeja con la  de clásica, pues tenemos $a_{mn}\rho_{nm}\sim A\rho$, donde en clásica usamos integrales y en cuántica, sumatorios.\\ \\
Vamos a ver que la traza va a ser un invariante, pues
\[\overline{a}=\sum_m\sum_na_{mn}\rho_{nm}=\sum_m(a_{m1}\rho_{1m}+a_{m2}\rho_{2m}+\dots)\]
donde nos damos cuenta que el paréntesis es la suma de los elementos de la $m-$fila de $\hat{A}$ y los elementos de la $m-$columna de $\hat{\rho}$, entonces tenemos un producto de matrices, tales que
\[\overline{a}=\begin{pmatrix}
    & \dots & \\
    a_{m1} & a_{m2} & \dots\\
    & \dots &
\end{pmatrix}\begin{pmatrix}
    & \rho_{1m} & \\
    \vdots & \rho_{2m} & \vdots\\
    & \vdots &
\end{pmatrix}=\begin{pmatrix}
    a'_{11} & & & \\
     & \ddots & & \\
     & & a'_{mm} & \\
     & & & \ddots
\end{pmatrix}\]
es decir,
\begin{equation}
    \overline{a}=Tr(\hat{A}\hat{\rho})
\end{equation}
siendo la traza un invariante ante los cambios de base y por tanto, el valor medio será un invariante ante los cambios de base.
\subsubsection*{Nota}
Los $c_n$ son los representantes en la base del vector $\Psi$, pero estas componentes las podemos representar haciendo el producto escalar, tal que
\[c_n^{(i)}=\bra{\Psi}\ket{\varphi_n}\equiv(0,0,\dots,0,1,0,\dots,0,0,\dots)\]
siendo la posición $n$-ésima donde está el 1.\\ \\
Vamos a darle sentido físico al operador densidad, que viene dado por
\[\rho_{nm}=\sum_i\gamma_ic_m^{*(i)}c_n^{(i)}=\sum_i\gamma_i\bra{\varphi_m}\ket{\Psi^{(i)}}\bra{\Psi^{(i}}\ket{\varphi_n}\]
donde para saber la componente $i$-ésima del vector, hemos hecho el producto escalar con el vector $i$-ésimo de la base. Luego,
\[\rho_{mn}=\bra{\varphi_m}\hat{\rho}\ket{\varphi_n}=\bra{\varphi_m}\brackets{\sum_i\gamma_i\ket{\Psi^{(i)}}\bra{\Psi^{(i)}}}\ket{\varphi_n}\]
Por tanto, el operador densidad queda como
\begin{equation}
    \hat{\rho}=\sum_i\gamma_i\ket{\Psi^{(i)}}\bra{\Psi^{(i)}}
\end{equation}
el elemento $\ket{\Psi^{(i)}}\bra{\Psi^{(i)}}$ es lo que se conoce como \textit{proyector}, el cuál se puede usar para obtener la componente de un vector en la dirección de otro, básicamente como cuando proyectamos vectores, por ejemplo, con este proyector,  $(\ket{\Psi^{(i)}}\bra{\Psi^{(i)}})\ket{\varphi}$, podemos obtener la componente de $\varphi$ en la dirección de $\Psi^{(i)}$.\\ \\
\subsubsection*{Nota}
La evolución temporal del operador densidad será,
\[i\frac{d\hat{\rho}}{dt}=\hat{H}\hat{\rho}-\hat{\rho}\hat{H}=\brackets{\hat{H},\hat{\rho}}\]
siendo $\brackets{\hat{H},\hat{\rho}}$ el conmutador de $\hat{H}$ con $\hat{\rho}$, luego
\begin{equation}
    i\frac{d\hat{\rho}}{dt}=\brackets{\hat{H},\hat{\rho}}
\end{equation}
siendo la ecuación de \textbf{Von Neumann}, la cuál se asemeja a la ecuación de Liouville de clásica.\\ \\
Como queremos estudiar sistemas en equilibrio, esto implica que las magnitudes no cambian con el tiempo, entonces la derivada temporal de $\hat{\rho}$ en un sistema en equilibrio debe ser cero, y esto se consigue haciendo que $\brackets{\hat{H},\hat{\rho}}=0$, es decir, que $\hat{H}$ conmute con $\hat{\rho}$. Por tanto, usaremos operadores de densidad $\hat{\rho}$ que dependan de $\hat{H}$.

\chapter{TEMA 5: Estadística Cuántica de los Gases Ideales}

\section{Función de partición de un gas ideal cuántico}
Un gas ideal cuántico es un sistema de $N$ partículas idénticas que no interaccionan entre sí.\\ \\
Como estamos en cuántica, tenemos dos tipos de partículas, los fermiones y los bosones.\\ \\
Vamos a comenzar haciendo una descripción generalizada y luego introducimos la distinción entre fermión y bosón cuando sea necesario. Usaremos la nomenclatura siguiente,\\ \\
\begin{tabular}{|c|c|c|c|}
 \hline
     & \textbf{Estado cuántico} & \textbf{Energía} & \textbf{Número de partículas} \\ \hline
   \textbf{Partícula simple}  & $r$ & $\epsilon_r$ & $n_r$ \\ \hline
   \textbf{Sistema completo} & $R$ & $E_R$ & $N_R$ \\ \hline
\end{tabular}
\\ \\
Se deberá cumplir que
\begin{equation}
    E=\sum_{j=1}^{N_R}\epsilon_j=\sum_rn_r\epsilon_r
\end{equation}
donde el primer sumatorio indica que hay varias partículas.
\begin{equation}
\Psi_R(\xi_1,\dots,\xi_{N_R})=\prod_{j=1}^{N_R}\Psi_{r_j}(\xi_j)
\end{equation}
\subsubsection*{Nota}
El \textbf{número de ocupación} es el número de partículas que hay por energía, $R=\curlybraces{n_{r,R}}$.\\ \\
Un estado cuántico $R$ de un sistema de partículas idénticas queda totalmente
determinado si se conoce el número de partículas que se encuentran en cada
estado de partícula simple $r$ ya que no es posible especificar cuáles son las
partículas concretas en cada estado.
\\ \\
En cuántica, la función de partición viene dada por
\begin{equation}
    Z_R=\sum_Re^{-\beta E_R}
\end{equation}
que se puede desarrollar,
\[Z_R=\sum_{n_1,n_2,\dots}e^{-\beta(n_1\epsilon_1+n_2\epsilon_2+\dots)}=\sum_{n_1+n_2+\dots+n_r+\dots=N}e^{-\beta(n_1\epsilon_1+n_2\epsilon_2+\dots+n_r\epsilon_r+\dots)}\]
tal que $N=n_1+n_2+\dots+n_r+\dots$, siendo esto todas las posibles combinaciones de $n_r$, con $N\approx N_A$, cosa absurda de calcular. Para solucionarlo, necesitamos eliminar las restricciones del número de partículas y para ello, usamos el colectivo canónico generalizado.
\\ \\
En este colectivo, la gran función de partición es,
\begin{equation}
    Q_R=\sum_{N_R=0}^{\infty}e^{-\alpha N_R}\sum_R^{(N_R)}e^{-\beta E_R}=\sum_{N_R,R}e^{-\alpha N_R-\beta E_R}
\end{equation}
con $\alpha=-\beta\mu$ y que podemos desarrollar un poco
\[Q_R=\sum_{N_R,R}e^{-\alpha N_R-\beta E_R}=\sum_{N_R,R}e^{-\alpha\sum_rn_{r,R}-\beta\sum_rn_{r,R}\epsilon_r}=\sum_{N_R,R}\prod_re^{-\alpha n_{r,R}-\beta n_{r,R}\epsilon_r}=\sum_{N_R,R}\prod_re^{-(\alpha+\beta\epsilon_r)n_{r,R}}\]
que puede reordenarse como
\[Q=\sum_{n_1,n_2,\dots}\prod_re^{-(\alpha+\beta\epsilon_r)n_{r,R}}=\sum_{n_1=0}^{n_1^{max}}\sum_{n_2=0}^{n_2^{max}}\dots\prod_re^{-(\alpha+\beta\epsilon_r)n_{r,R}}\]
desarrollamos el productorio,
\[Q=\sum_{n_1=0}^{n_1^{max}}\sum_{n_2=0}^{n_2^{max}}\dots e^{-(\alpha+\beta\epsilon_0)n_0}e^{-(\alpha+\beta\epsilon_1)n_1}e^{-(\alpha+\beta\epsilon_2)n_2}\dots\]
\[Q=e^{-(\alpha+\beta\epsilon_0)n_0}\sum_{n_1=0}^{n_1^{max}}e^{-(\alpha+\beta\epsilon_1)n_1}\sum_{n_2=0}^{-(\alpha+\beta\epsilon_2)n_2}\dots\]
desarrollamos los sumatorios,
\[Q=e^{-(\alpha+\beta\epsilon_0)n_0}\brackets{1+e^{-(\alpha+\beta\epsilon_1)}+\dots+e^{-(\alpha+\beta\epsilon_1)n_1^{max}}}\brackets{1+e^{-(\alpha+\beta\epsilon_2)}+\dots e^{-(\alpha+\beta\epsilon_2)n_2^{max}}}\dots\]
que pueden reordenarse como
\begin{equation}
    Q=\prod_r\sum_{r=0}^{n_r^{max}}e^{-(\alpha+\beta\epsilon_r)n_r}
\end{equation}
donde los $n_r$ son el número de partículas que pueden haber en cada estado. Esto dependerá de si las partículas son fermiones o son bosones, pues
\[\left\lbrace\begin{matrix}
    \text{Bosones}: & n_r^{max}=\infty\\
    \text{Fermiones}: & n_r^{max}=1
\end{matrix}\right.\]
esto quiere decir que para los fermiones, $n_r\in\brackets{0,1}$, mientras que para los bosones, $n_r\in\brackets{0\infty}$.\\ \\
Una vez obtenida la función de partición, podemos obtener los diferentes valores medios.
\[\overline{n_r}=\frac{\sum_{N_R,R}n_{r,R}e^{-\alpha N_R-\beta E_R}}{Q}=\frac{1}{Q}\sum_{N_R,R}n_{r,R}e^{-\alpha\sum_jn_{j,R}-\beta\sum_jn_{j,R}\epsilon_j}\]
donde $r$ es fijo. Buscamos la derivada para que nos quede $n_r$ multiplicando la exponencial, luego derivamos $Q$ respecto $\epsilon_r$, 
\[\frac{\partial Q}{\partial\epsilon_r}=\frac{\partial}{\partial \epsilon_r}\brackets{\prod_r\sum_{r=0}^{n_r^{max}}e^{-(\alpha+\beta\epsilon_r)n_r}}=-\beta \sum_{N_R,R}n_{r,R}e^{-\alpha N_R-\beta E_R}\]
luego tenemos,
\[\overline{n_r}=-\frac{1}{\beta}\frac{1}{Q}\frac{\partial Q}{\partial\epsilon_r}\]
que usando que $\frac{\partial\ln{f(x)}}{\partial x}=\frac{1}{f(x)}\frac{\partial f(x)}{\partial x}$, tenemos
\begin{equation}
    \overline{n_r}=-\frac{1}{\beta}\frac{\partial\ln{Q}}{\partial\epsilon_r}
    \label{ec6}
\end{equation}
esta ecuación sirve tanto para fermiones como para bosones.

Calculamos la presión, partiendo de su definición en el colectivo canónico generalizado
\[\overline{P}=\frac{1}{\beta}\left.\frac{\partial\ln{Q}}{\partial V}\right|_{\alpha,\beta}=\frac{1}{\beta}\frac{1}{Q}\left.\frac{\partial Q}{\partial V}\right|_{\alpha,\beta}=\frac{1}{\beta Q}\frac{\partial}{\partial V}\brackets{\sum_{N_R,R}e^{-\alpha\sum_jn_{j,R}-\beta\sum_jn_{j,R}\epsilon_j}}_{\alpha,\beta}\]
Como la única dependencia de $V$ estará en $\epsilon_j$, tenemos
\[\overline{P}=\frac{1}{\cancel{\beta}}\frac{1}{Q}\sum_{N_R,R}\brackets{-\cancel{\beta}\sum_jn_{j,R}\frac{\partial\epsilon_j}{\partial V}}e^{-\alpha\sum_jn_{j,R}-\beta\sum_jn_{j,R}\epsilon_j}\]
\[\overline{P}=-\frac{1}{Q}\sum_j\frac{\partial\epsilon_j}{\partial V}\sum_{N_R,R}n_{j,R}e^{-\alpha\sum_j n_{j,R}-\beta\sum_j n_{j,R}\epsilon_j}\]
Usando la expresión de $\overline{n_r}$ tenemos
\begin{equation}
    \overline{P}=-\sum_j\left(\frac{\partial\epsilon_j}{\partial V}\right)\overline{n_j}
    \label{ec7}
\end{equation}
Esta relación toma una forma especialmente útil en el caso particular de un sistema de partículas ideales encerradas en un volumen $V=L_xL_yL_z$, de forma que la energía de las partículas sea únicamente de traslación, y cuyos niveles energéticos vienen dados por\footnote{La demostración de esta expresión está en el Apéndice B del Brey, no veía necesaria ponerla aquí.}
\begin{equation}
    \epsilon_r=\frac{\hbar^2\pi^2}{2m}\brackets{\left(\frac{n_x}{L_x}\right)^2+\left(\frac{n_y}{L_y}\right)^2+\left(\frac{n_z}{L_z}\right)^2}
\end{equation}
Supongamos que $L=Lx=L_y=L_z$, así
\[\epsilon_r=\frac{\hbar^2\pi^2}{2mL^2}\brackets{n_x^2+n_y^2+n_z^2}\]
Calculamos $\overline{P}$ con la ecuación $\ref{ec7}$ y hacemos la derivada, tal que
\[\overline{P}=-\sum_r\left(\frac{\partial\epsilon_r}{\partial V}\right)\overline{n_r}\]
usamos que $V=L^3$, entonces $L=V^{1/3}$, entonces
\[\epsilon_r=\frac{\hbar^2\pi^2}{2mV^{2/3}}\brackets{n_x^2+n_y^2+n_z^2}\]
así,
\[\frac{\partial\epsilon_r}{\partial V}=\frac{\partial}{\partial V}\brackets{\frac{\hbar^2\pi^2}{2mV^{2/3}}\brackets{n_x^2+n_y^2+n_z^2}}=\frac{\hbar^2\pi^2}{2m}\brackets{n_x^2+n_y^2+n_z^2}\frac{\partial (1/V^{2/3})}{\partial V}=-\frac{2}{3}\frac{\hbar^2\pi^2}{2m}V^{-\frac{2}{3}-1}\brackets{n_x^2+n_y^2+n_z^2}\]
Por tanto,
\[\overline{P}=\sum_r\frac{2\hbar^2\pi^2}{2\cdot 3mV^{2/3}V}\brackets{n_x^2+n_y^2+n_z^2}\overline{n_r}=\frac{2}{3V}\sum_r\cancelto{\epsilon_r}{\frac{\hbar^2\pi^2}{2 mV^{2/3}}\brackets{n_x^2+n_y^2+n_z^2}}\overline{n_r}=\frac{2}{3V}\cancelto{\overline{E}}{\sum_r\epsilon_r\overline{n_r}}=\frac{2\overline{E}}{3V}\]
Luego, la ecuación de estado es
\begin{equation}
    \overline{P}V=\frac{2}{3}\overline{E}
\end{equation}
\subsection{Función de partición $Q$ para fermiones}
Podemos particularizar la función de partición obtenida anteriormente para los fermiones, recordando que para los fermiones $n_r^{max}=1$. Entonces, partiendo de la ecuación de $Q$,
\[Q=\prod_r\sum_{n_r=0}^{n_r=1}e^{-(\alpha+\beta\epsilon_r)n_r}=\prod\brackets{1+e^{-(\alpha+\beta\epsilon_r)}}
\]
Por tanto, usando la definición de logaritmo,
\begin{equation}
    \ln{Q}=\sum_r\ln(1+e^{-(\alpha+\beta\epsilon_r)})
    \label{ec10}
\end{equation}
Podemos calcular el número medio de partículas, partiendo de su definición en e canónico generalizado,

\[\overline{N}=-\frac{\partial\ln{Q}}{\partial\alpha}=-\frac{ \partial}{\partial\alpha}\brackets{\sum_r\ln(1+e^{-(\alpha+\beta\epsilon_r)})}=-\sum_r\brackets{\frac{\partial}{\partial\alpha}\ln(1+e^{-(\alpha+\beta\epsilon_r)})}=\cancel{-}\sum_r\cancel{-}\frac{e^{-(\alpha+\beta\epsilon_r)}}{1+e^{-(\alpha+\beta\epsilon_r)}}\]
luego,
\begin{equation}
    \overline{N}=\sum_r\frac{1}{1+e^{\alpha+\beta\epsilon_r}}
\end{equation}
También podemos calcular el número medio de partículas para un estado $r$, partiendo de la ecuación \ref{ec6},
\[\overline{n_r}=-\frac{1}{\beta}\frac{\partial\ln{Q}}{\partial\epsilon_r}=-\frac{1}{\beta}\frac{\partial}{\partial\epsilon_r}\brackets{\sum_j\ln(1+e^{-(\alpha+\beta\epsilon_j)})}=-\frac{1}{\beta}\frac{\partial}{\partial\epsilon_r}\ln(1+e^{-(\alpha+\beta\epsilon_r)})=\cancel{-}\frac{1}{\cancel{\beta}}\frac{\cancel{-}\cancel{\beta}e^{-(\alpha+\beta\epsilon_r)}}{1+e^{-(\alpha+\beta\epsilon_r)}}\]
luego,
\begin{equation}
    \overline{n_r}=\frac{1}{1+e^{\alpha+\beta\epsilon_r}}
    \label{ec12}
\end{equation}
denominada \textbf{Distribución de Fermi} y es directo ver que
\begin{equation}
    \overline{N}=\sum_r\overline{n_r}
\end{equation}
siendo el número medio de partículas igual a la suma de los valores medios de las partículas en un estado $r$. \\ \\
Además, al conjunto de las ecuaciones \ref{ec12} y \ref{ec10} se le denomina \textbf{Estadística de Fermi}.
Tenemos por definición que $\alpha=-\beta\mu$, denominándose $\mu$ como \textbf{Nivel de Fermi}, que depende de la temperatura, teniendo que $\mu(T=0)=\mu_0$, denominando a $\mu_0$ como \textbf{Energía de Fermi}.
\\ \\
Representamos la función de distribución de Fermi frente a $\epsilon_r$, tal que
\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{image (1).png}
    \captionof{figure}{Representación de la función de distribución de Fermi frente a $\epsilon_r$ para  distintas temperaturas.}
    \label{fig:5-algos}
\end{Figura}

Vemos que todas las curvas cortan en un único punto a una altura de $\overline{n_r}=1/2$ y $\epsilon_r=\mu$. 
\\
Para $T=0$, tenemos que $\beta=\frac{1}{kT}=\infty$, luego tendremos que 
\[\overline{n_r}=\left\lbrace\begin{matrix}
    1 & si & \epsilon_r<\mu\\
    0 & si & \epsilon>\mu
\end{matrix}\right.\]
Para $T\rightarrow\infty$, $\beta\rightarrow0$, luego
\[\lim_{\beta\rightarrow0}\overline{n_r}=\lim_{\beta\rightarrow0}\frac{1}{1+e^{\alpha+\beta\epsilon_r}}=\lim_{\beta\rightarrow0}\frac{1}{1+e^{\beta(\epsilon_r-\mu)}}=\frac{1}{1+e^0}=\frac{1}{2}\]
\subsection{Función de partición $Q$ para los bosones}
Podemos particularizar la función de partición obtenida anteriormente para los fermiones, recordando que para los fermiones $n_r^{max}=\infty$. Entonces, partiendo de la ecuación de $Q$,
\[Q=\prod_r\sum_{n_r=0}^{n_r=\infty}e^{-(\alpha+\beta\epsilon_r)n_r}=\prod\brackets{1+e^{-(\alpha+\beta\epsilon_r)}+e^{-2(\alpha+\beta\epsilon_R)}+\dots}
\]
Vemos que podemos obtener el elemento siguiente a partir del anterior multiplicando por $e^{-(\alpha+\beta\epsilon_r)}$, entonces tendremos una \textit{Serie Geométrica}, que converge cuando $e^{-\beta(\epsilon_r-\mu)}<1$ y su suma vale $\sum_{n=0}^{\infty}a_nr^n=\frac{a_1}{1-r}$, luego,
\[Q=\prod_r\frac{1}{1-e^{-\beta(\epsilon_r-\mu)}}\]
Aplicando la definición de logaritmo,
\begin{equation}
    \ln{Q}=-\sum_r\ln(1-e^{-\beta(\epsilon_r-\mu)})=-\sum_r\ln(1-e^{-(\alpha+\beta\epsilon_r)})
    \label{ec5-14}
\end{equation}

Podemos calcular el número medio de partículas, partiendo de su definición en e canónico generalizado,

\[\overline{N}=-\frac{\partial\ln{Q}}{\partial\alpha}=-\frac{ \partial}{\partial\alpha}\brackets{-\sum_r\ln(1-e^{-(\alpha+\beta\epsilon_r)})}=\sum_r\brackets{\frac{\partial}{\partial\alpha}\ln(1-e^{-(\alpha+\beta\epsilon_r)})}=\sum_r-\frac{e^{-(\alpha+\beta\epsilon_r)}}{1-e^{-(\alpha+\beta\epsilon_r)}}\]
luego,
\begin{equation}
    \overline{N}=\sum_r\frac{1}{e^{\alpha+\beta\epsilon_r}-1}
\end{equation}
También podemos calcular el número medio de partículas para un estado $r$, partiendo de la ecuación \ref{ec6},
\[\overline{n_r}=-\frac{1}{\beta}\frac{\partial\ln{Q}}{\partial\epsilon_r}=-\frac{1}{\beta}\frac{\partial}{\partial\epsilon_r}\brackets{-\sum_j\ln(1-e^{-(\alpha+\beta\epsilon_j)})}=\frac{1}{\beta}\frac{\partial}{\partial\epsilon_r}\ln(1-e^{-(\alpha+\beta\epsilon_r)})=-\frac{1}{\cancel{\beta}}\frac{\cancel{\beta}e^{-(\alpha+\beta\epsilon_r)}}{1-e^{-(\alpha+\beta\epsilon_r)}}\]
luego,
\begin{equation}
    \overline{n_r}=\frac{1}{e^{\alpha+\beta\epsilon_r}-1}
    \label{ec16}
\end{equation}
denominada \textbf{Distribución de Bose} y es directo ver que
\begin{equation}
    \overline{N}=\sum_r\overline{n_r}
\end{equation}
siendo el número medio de partículas igual a la suma de los valores medios de las partículas en un estado $r$. \\ \\
Además, al conjunto de las ecuaciones \ref{ec16} y \ref{ec5-14} se le denomina \textbf{Estadística de Bose-Einstein}.

Representamos la función de distribución de Bose frente a $\epsilon_r$, tal que
\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{fff.png}
    \captionof{figure}{Representación de la función de distribución de Bose frente a $\epsilon_r$ para  distintas temperaturas.}
    \label{fig:5-algos2}
\end{Figura}
Vemos que todas las temperaturas tienden a cero.
\\ \\
La función de estado del gas ideal cuántico es
\begin{equation}
    \overline{P}V=\pm kT\sum_r\ln(1\pm e^{-(\beta(\epsilon_r-\mu))})
\end{equation}
donde para calcular $\mu$ usamos $\overline{N}=\sum_r\overline{n_r}$ y el signo $(-)$ es para los bosones, mientras que el signo $(+)$ es para los fermiones.

\section{Límite clásico: Estadística de Maxwell-Boltzmann.}

Tomamos el límite clásico cuando:
\begin{enumerate}[(a)]
    \item Consideremos que estamos a $T=cte$ y en el límite de bajas densidades, es decir, que $\frac{N}{V}\ll\frac{r}{V}$, que el número de niveles accesibles sea mucho mayor que el número de partículas. Así, en promedio, habrá pocas partículas por nivel. Luego,
    \[\frac{1}{e^{\alpha+\beta\epsilon_r}\pm1}\ll1\Rightarrow e^{\alpha+\beta\epsilon_r}\pm1\gg1\Rightarrow e^{\alpha+\beta\epsilon_r}\gg1\]
    \item Consideremos una densidad fija, pero con el límite de altas temperaturas, es decir, $T\rightarrow\infty\Rightarrow\beta\rightarrow0$. Como $\overline{N}$ es fijo, al disminuir $\beta$, $\overline{N}$ cambiará, pero como es constante, deberá haber más sumandos que contribuyan a que $\overline{N}=cte$. Por tanto, al haber cada vez más sumandos que puedan contribuir, tendremos que
    \[\frac{1}{e^{\alpha+\beta\epsilon_r}\pm1}\ll1\Rightarrow e^{\alpha+\beta\epsilon_r}\pm1\gg1\Rightarrow e^{\alpha+\beta\epsilon_r}\gg1\]
\end{enumerate}
Luego, tendremos en general,
\begin{equation}
    \left.\begin{matrix}
    \overline{n_r}=\frac{1}{e^{\alpha+\beta\epsilon_r}\pm1}\\
    e^{\alpha+\beta\epsilon_r}\gg1
\end{matrix}\right\rbrace\Rightarrow\overline{n_r}\approx\frac{1}{e^{\alpha+\beta\epsilon_r}}=e^{-\alpha-\beta\epsilon_r}=e^{-(\alpha+\beta\epsilon_r)}
\label{ec19}
\end{equation}
Por tanto, en el límite clásico, vemos que para $\overline{n_r}$ no hay distinciones entre bosones y fermiones. Luego,
\[\overline{N}=\sum_r\overline{n_r}=\sum_re^{-(\alpha+\beta\epsilon_r)}=e^{-\alpha}\sum_re^{-\beta\epsilon_r}\]
entonces,
\begin{equation}
    e^{-\alpha}=\frac{\overline{N}}{\sum_re^{-\beta\epsilon_r}}
\end{equation}
Considerando que $\sum_re^{-\beta\epsilon_r}$ sea la función de partición cuántica canónica para una sola partícula, tenemos
\begin{equation}
    \overline{n_r}=\frac{\overline{N}}{\sum_je^{-\beta\epsilon_j}}e^{-\beta\epsilon_r}
\end{equation}
que se denomina \textbf{Distribución de Maxwell-Boltzmann}, que representa al número medio de partículas que tienen una velocidad entre $v$ y $v+dv$.
\\ \\
Al no haber distinción, la ecuación de estado es
\begin{equation}
    \overline{P}V=kT\ln{Q}=kT\overline{N}
\end{equation}
pues
\[\ln{Q}=\pm\sum_r\ln(1\pm e^{-(\alpha+\beta\epsilon_r)})\]
con $e^{\alpha+\beta\mu}\gg1$, entonces $e^{-(\alpha+\beta\mu)}\ll1$, por tanto, podemos desarrollar el logaritmo con Taylor, usando que $ln(1\pm x)\approx\pm x$, así
\[\ln{Q}\approx \cancel{\pm}\sum_r\cancel{\pm} e^{-(\alpha+\beta\epsilon_r)}=\sum_re^{-(\alpha+\beta\mu)}\overset{(\ref{ec19})}{=}\sum_r\overline{n_r}=\overline{N}\]
Además, la función de partición canónica la podemos expresar como
\[Q=\sum_{N=0}^{\infty}e^{-\alpha N}\sum_R^{(N_R)}e^{-\beta E_R}=\sum_{N=0}^{\infty}e^{-\alpha N}Z(N)
\]
Además, como $\ln{Q}\approx\overline{N}$,
\[Q=e^{\overline{N}}\]
usando que
\[\xi\equiv e^{-\alpha}=\frac{\overline{N}}{\zeta\equiv\sum_re^{-\beta\epsilon_r}}\Rightarrow\overline{N}=\xi\cdot\zeta\]
entonces, sustituimos,
\[Q=e^{\xi\cdot\zeta}\]
Usando el desarrollo de MacLaurin de la exponencial $e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$, tenemos
\[Q=\sum_{N=0}^{\infty}\frac{(\xi\cdot\zeta)^N}{N!}=\sum_{N=0}^{\infty}\xi^N\frac{\zeta^N}{N!}=\sum_{N=0}^{\infty}(e^{-\alpha})^N\frac{\zeta^N}{N!}=\sum_{N=0}^{\infty}e^{-\alpha N}\frac{\zeta^N}{N!}=\sum_{N=0}^{\infty}e^{-\alpha N}\frac{\brackets{\sum_re^{-\beta\epsilon_r}}^N}{N!}\]
Igualamos las $Q$,
\[\sum_{N=0}^{\infty}e^{-\alpha N}Z(N)=\sum_{N=0}^{\infty}e^{-\alpha N}\frac{\brackets{\sum_re^{-\beta\epsilon_r}}^N}{N!}\]
Por comparación,
\begin{equation}
    Z(N)=\frac{\brackets{\sum_{N=0}^{\infty}e^{-\alpha N}\frac{\brackets{\sum_re^{-\beta\epsilon_r}}^N}{N!}}^N}{N!}
\end{equation}
siendo esta la función de partición canónica para un gas ideal cuántico.
\\ \\
A modo de resumen tenemos,
\begin{tcolorbox}[title=\textbf{Gas ideal cuántico}]
\begin{itemize}
    \item Gas no degenerado:
    \begin{itemize}
        \item Estadística de Maxwell-Boltzmann
         \[\overline{n_r}=e^{-(\alpha+\beta\epsilon_r)}\hspace{15mm}\ln{Q}=\overline{N}\]
    \end{itemize}
    \item Gas degenerado:
    \begin{itemize}
        \item Estadística de Fermi-Dirac
        \[\overline{n_r}=\frac{1}{e^{\alpha+\beta\epsilon_r}+1}\hspace{15mm}\ln{Q}=\sum_r\ln(1+e^{-(\alpha+\beta\epsilon_r)})\]
        \item Estadística de Bose-Einstein
        \[\overline{n_r}=\frac{1}{e^{\alpha+\beta\epsilon_r}-1}\hspace{15mm}\ln{Q}=-\sum_r\ln{1-e^{-(\alpha+\beta\epsilon_r)}}\]
    \end{itemize}
\end{itemize}
    
\end{tcolorbox}
\section{Gas ideal cuántico en el límite clásico (gas no degenerado)}

Es el estudio del gas ideal desde un punto de vista cuántico, pero en el margen del límite clásico.\\ \\
Suponemos que tenemos un gas ideal compuesto de $N$ partículas. Al tenderlo al límite clásico, seguirá una estadística de Maxwell-Boltzmann. Así, tendremos un gas ideal no degenerado, cuyos grados de libertad son únicamente de traslación. La función de partición del sistema es
\[Z(N)=\frac{\brackets{\sum_re^{-\beta\epsilon_r}}^N}{N!}=\frac{\zeta^N}{N!}\]
Así, la función de partición de cada partícula será,
\begin{equation}
    \zeta=\sum_re^{-\beta\epsilon_r}
\end{equation}
con 
\[\epsilon_r=\epsilon_{n_x, n_y, n_z}=\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_x^2}{L_x^2}+\frac{n_y^2}{L_y^2}+\frac{n_z^2}{L_z^2}}\]
siempre que tengamos que el recipiente en el que están encerradas las partículas es una caja. Por tanto, sustituyéndolo en $\zeta$,
\[\zeta=\sum_{n_x=1}^{\infty}\sum_{n_y=1}^{\infty}\sum_{n_z=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_x^2}{L_x^2}+\frac{n_y^2}{L_y^2}+\frac{n_z^2}{L_z^2}}}\sum_{n_x=1}^{\infty}\sum_{n_y=1}^{\infty}\sum_{n_z=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_x}{L_x}}^2}+e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_y}{L_y}}^2}+e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_z}{L_z}}^2}\]
\[\zeta=\brackets{\sum_{n_x=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_x}{L_x}}^2}}\brackets{\sum_{n_y=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_y}{L_y}}^2}}\brackets{\sum_{n_z=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n_z}{L_z}}^2}}\]
Como los tres sumatorios tienen la misma estructura, hacemos solo uno de ellos,
\[\sum_{n=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}\]
Para ver cómo distan los valores, hacemos 

\[\abs{\frac{s_{n+1}-s_n}{s_{n+1}}}=\abs{\frac{e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n+1}{L}}^2}-e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}}{e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n+1}{L}}^2}}}=\abs{\frac{\cancel{e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n+1}{L}^2}}}\brackets{1-e^{-\beta\frac{\hbar^2\pi^2}{2m}\frac{n^2-(n+1)^2}{L^2}}}}{\cancel{e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}}}}\]
\[\abs{\frac{s_{n+1}-s_n}{s_{n+1}}}=\abs{1-e^{-\beta\frac{\hbar^2\pi^2}{2m}\frac{\cancel{n^2}- \cancel{n^2}-2n-1}{L^2}}}=\abs{1-e^{\beta\frac{\hbar^2\pi^2}{2m}\frac{2n+1}{L^2}}}\]
Usando la aproximación de Taylor de las exponencial $e^x\approx1+x$, tenemos
\[\abs{\frac{s_{n+1}-s_n}{s_{n+1}}}\approx\abs{\cancel{1}-\beta\frac{\hbar^2\pi^2}{2m}\frac{2n+1}{L^2}-\cancel{1}}=\beta\frac{\hbar^2\pi^2}{2m}\frac{2n+1}{L^2}=\frac{\hbar^2\pi^2}{2mL^2kT}(2n+1)\]
Usando los valores típicos $\curlybraces{\begin{matrix}
    m=10^{-20}Kg\\
    T=300K\\
    L=10^{-2}m
\end{matrix}}$
 tenemos que la distancia es aproximadamente de $1.3\cdot 10^{-23}(2n+1)$. Este número es pequeño dependiendo del valor de $n$, pero al estar con $T=300K$, la probabilidad de que $n\gg$ es muy baja y por tanto, aporta muy poco al sumatorio. Por esto, al tener valores que distan muy poco entre ellos, podremos aproximar el sumatorio a una integral, tal que

\[\sum_{n=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}\approx\int_{n=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}dn\approx\int_{n=0}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}dn\]
Usamos que $\int_{0}^{\infty}x^me^{-ax^2}=\frac{\Gamma\left(\frac{m+1}{2}\right)}{2a^{\frac{m+1}{2}}}$, tenemos
\[\sum_{n=1}^{\infty}e^{-\beta\frac{\hbar^2\pi^2}{2m}\brackets{\frac{n}{L}}^2}\approx\frac{\Gamma(1/2)}{2\left(\beta\frac{\hbar^2\pi^2}{2mL^2}\right)^{1/2}}=\frac{\sqrt{\pi}(2mL^2kT)^{1/2}}{2(\hbar^2\pi^2)^{1/2}}=\frac{L\sqrt{\pi}(2mkT)^{1/2}}{2\hbar\pi}=\frac{L}{h}(2m\pi kT)^{1/2}\]
Por tanto,
\[\zeta=\frac{L_x}{h}(2m\pi kT)^{1/2}\cdot\frac{L_y}{h}(2m\pi kT)^{1/2}\cdot\frac{L_z}{h}(2m\pi kT)^{1/2}=\frac{L_xL_yL_z}{h^3}(2m\pi kT)^{3/2}=\frac{V}{h^3}(2m\pi kT)^{3/2}\]
Así,
\begin{equation}
    \zeta=\frac{V}{h^3}(2m\pi kT)^{3/2}
\end{equation}
Luego, la función de partición de un gas ideal cuántico en el límite clásico será,
\begin{equation}
    Z=\frac{1}{h^{3N}}\frac{V^N}{N!}(2m\pi kT)^{3N/2}
\end{equation}
El límite clásico se da cuando $e^{\alpha+\beta\epsilon_r}\gg1$, como $e^{\beta\epsilon_r}>0$, se cumple también que $e^{\alpha}\gg1$, luego $e^{-\alpha\ll1}$. Tenemos que,
\[e^{-\alpha}=\frac{\overline{N}}{\sum_re^{-\beta\epsilon_r}}=\frac{\overline{N}}{\zeta}=\frac{\overline{N}}{\frac{V}{h^3}(2m\pi kT)^{3/2}}=\frac{\overline{N}}{V}\brackets{\frac{h^2}{2m\pi kT}}^{3/2}=n\lambda^3\]
 con $\lambda=\frac{h}{(2m\pi kT)^{1/2}}$. \\ \\
 Entonces, para que el límite sea clásico, se debe cumplir que
 \begin{equation}
     n\lambda^3\ll1\Rightarrow\left\lbrace\begin{matrix}
         n\ll1 & & & \leftarrow & \text{Bajas densidades}\\
         \lambda\ll1 & \leftarrow & \lambda\propto\frac{1}{\sqrt{T}} & \leftarrow & \text{Temperaturas altas}
     \end{matrix}\right.
 \end{equation}
Por esto decimos que el límite clásico se da para bajas densidades y/o temperaturas altas.

\section{Grados internos de libertad: rotación, vibración y electrónico}
\subsection{Gas ideal cuántico en el límite clásico (no degenerado)}
Vamos a estudiar los gases, considerando que habrán grados de libertad de rotación, vibración, traslación y electrónicos.\\ \\
Como estamos en el límite clásico, sabemos el valor de su función de partición, que es
\[Z(N)=\frac{\zeta^N}{N!};\hspace{10mm}\zeta=\sum_re^{-\beta\epsilon_r}\]
El Hamiltoniano se podrá descomponer como:
\begin{equation}
    \hat{H}=\hat{H}_{\text{traslación}}(\zeta_{\text{traslación}})+\hat{H}_{\text{rotación}}(\zeta_{\text{rotación}})+\hat{H}_{\text{vibración}}(\zeta_{\text{vibración}})+\hat{H}_{\text{electrónico}}(\zeta_{\text{electrónico}})
\end{equation}
que reescribimos como
\[\hat{H}=\hat{H}_{\text{trasl}}(\zeta_{\text{trasl}})+\hat{H}_{\text{rot}}(\zeta_{\text{rot}})+\hat{H}_{\text{vib}}(\zeta_{\text{vib}})+\hat{H}_{\text{elec}}(\zeta_{\text{elec}})\]
donde hemos supuesto que estos Hamiltonianos son independientes unos de otros.\\ \\
Sabiendo que la ecuación de Schrödinger es
\begin{equation}
    \hat{H}\varphi_r(\zeta))=\epsilon_r\varphi_r(\zeta)
\end{equation}
tal que
\[\begin{matrix}
    \epsilon_r=\epsilon_{\text{trasl}}+\epsilon_{\text{rot}}+\epsilon_{\text{vib}}+\epsilon_{\text{elec}}\\
    \varphi_r=\varphi_{\text{trasl}}+\varphi_{\text{rot}}+\varphi_{\text{vib}}+\varphi_{\text{elec}}
\end{matrix}\Rightarrow\left\lbrace\begin{matrix}
    \hat{H}_{\text{trasl}}\varphi_{\text{trasl}}=\epsilon_{\text{trasl}}\varphi_{\text{trasl}}\\
    \hat{H}_{\text{rot}}\varphi_{\text{rot}}=\epsilon_{\text{rot}}\varphi_{\text{rot}}\\
    \hat{H}_{\text{vib}}\varphi_{\text{vib}}=\epsilon_{\text{vib}}\varphi_{\text{vib}}\\
    \hat{H}_{\text{elec}}\varphi_{\text{elec}}=\epsilon_{\text{elec}}\varphi_{\text{elec}}
\end{matrix}\right.\]

Entonces,

\begin{equation*}
    \zeta=\sum_{\text{trasl}}\sum_{\text{rot}}\sum_{\text{vib}}\sum_{\text{elec}}e^{-\beta(\epsilon_{\text{trasl}}+\epsilon_{\text{rot}}+\epsilon_{\text{vib}}+\epsilon_{\text{elec}})}=
\end{equation*}
 \[=\left(\sum_{\text{trasl}}e^{-\beta\epsilon_{\text{trasl}}}\right)+\left(\sum_{\text{rot}}e^{-\beta\epsilon_{\text{rot}}}\right)+\left(\sum_{\text{vib}}e^{-\beta\epsilon_{\text{vib}}}\right)+\left(\sum_{\text{elec}}e^{-\beta\epsilon_{\text{elec}}}\right)=\]
\begin{equation}
\zeta=\zeta^{\text{trasl}}+\zeta^{\text{rot}}+\zeta^{\text{vib}}+\zeta^{\text{elec}}
\end{equation}
Entonces, la función de partición queda
\begin{equation}
Z(N)=\frac{\left(\zeta^{\text{trasl}}\right)^N}{N!}+\left(\zeta^{\text{rot}}\right)^N+\left(\zeta^{\text{vib}}\right)^N+\left(\zeta^{\text{elec}}\right)^N
\end{equation}
Sabemos que
\begin{equation}
    \frac{\left(\zeta^{\text{trasl}}\right)^N}{N!}=\frac{1}{h^{3N}}\frac{V^N}{N!}(2m\pi kT)^{3N/2}
\end{equation}
pues esto lo calculamos antes, ya que supusimos que solo había traslación.Luego,
\[Z(N)=\frac{1}{h^{3N}}\frac{V^N}{N!}(2m\pi kT)^{3N/2}+\left(\zeta^{\text{rot}}\right)^N+\left(\zeta^{\text{vib}}\right)^N+\left(\zeta^{\text{elec}}\right)^N=Z_{\text{trasl}}+Z_{\text{rot}}+Z_{\text{vib}}+Z_{\text{elec}}\]
Vamos a ir analizando uno por uno:
\subsubsection{Rotación}
Tenemos que
\[\zeta^{\text{rot}}=\sum_{\text{rot}}e^{-\beta\epsilon_{\text{rot}}}\hspace{15mm}\hat{H}_{\text{rot}}\varphi_{\text{rot}}=\epsilon_{\text{rot}}\varphi_{\text{rot}}\]
El Hamiltoniano será el de un sólido rígido, tal que
\begin{equation}
    \hat{H}_{\text{rot}}=\hat{E}_{c_{\text{rot}}}=\frac{\hat{L}^2}{2I}
\end{equation}
Pues,
\[E_{c_{\text{rot}}}=\frac{1}{2}\overline{\omega}\cdot I\cdot\overline{\omega}=\frac{1}{2}I\omega^2\overset{\curlybraces{\vec{L}=I\vec{\omega}}}{=}\frac{1}{2}\cancel{I}\frac{L^2}{I^{\cancel{2}}}=\frac{L^2}{2I}\]
tomando operadores, tenemos que $\hat{E}_{c_{\text{rot}}}=\frac{\hat{L}^2}{2I}$.
\\ \\
Además, los autovalores serán
\[\begin{matrix}
    \hat{L}^2\Psi_{l,m}=l(l+1)\hbar^2\Psi_{l,m}\\ \\
    l=0,1,2,3,\dots\\ \\
    m=-l,-l+1,\dots,0,\dots,l-1,l
\end{matrix}\]
Luego,
\begin{equation}
    \epsilon_{\text{rot}}=\frac{l(l+1)\hbar^2}{2I}\equiv\epsilon_l
\end{equation}
que tendrá $2l+1$ degeneración de estados. Entonces,
\[\zeta^{\text{rot}}=\sum_{\text{rot}}e^{-\beta\epsilon_{\text{rot}}}=\sum_{l=0}^{\infty}\sum_{m=-l}^{m=+l}e^{-\beta\epsilon_l}=\sum_{l=0}^{\infty}(2l+1)e^{-\beta\epsilon_l}=\sum_{l=0}^{\infty}(2l+1)e^{-\beta\frac{l(l+1)\hbar^2}{2I}}\]
Denotando $\theta_{\text{rot}}=\frac{\hbar^2}{2Ik}$, que será la \textbf{temperatura de rotación}. Luego,
\begin{equation}
    \zeta^{\text{rot}}=\sum_{l=0}^{\infty}(2l+1)e^{-\frac{\theta_{rot}}{T}l(l+1)}
\end{equation}
A temperatura ambiente, unos $300$ K, tenemos que $\frac{\theta_{\text{rot}}}{T}\ll1$, entonces, el sumatorio podremos aproximarlo a una integral, pues tendremos una sucesión de números cada vez más pequeños, tal que
\[\zeta^{\text{rot}}=\sum_{l=0}^{\infty}(2l+1)e^{-\frac{\theta_{rot}}{T}l(l+1)}\approx\int_0^{\infty}(2l+1)e^{-\frac{\theta_{\text{rot}}}{T}l(l+1)}dl=-\frac{T}{\theta_{\text{rot}}}\brackets{e^{-\frac{\theta_{\text{rot}}}{T}}l(l+1)}_{0}^{\infty}=\frac{T}{\theta_{\text{rot}}}
\]
Así, a temperatura ambiente tenemos,
\begin{equation}
    \zeta^{\text{rot}}=\frac{T}{\theta_{\text{rot}}}
\end{equation}
\subsubsection*{Nota}
Además, si la molécula tiene simetría, tenemos
\begin{equation}
    \zeta^{\text{rot}}=\frac{T}{\theta_{\text{rot}}\sigma}
\end{equation}
siendo $\sigma$ un factor que depende de la simetría de la molécula, por ejemplo, si $\sigma=2$, tenemos una molécula diatómica homonuclear.

\subsubsection{Vibración}
Estudiamos la vibración, teniendo que
\[\zeta^{\text{vib}}=\sum_{\text{vib}}e^{-\beta\epsilon_{\text{vib}}}\hspace{15mm}\hat{H}_{\text{vib}}\varphi_{\text{vib}}=\epsilon_{\text{vib}}\varphi_{\text{vib}}\]
Tendremos los siguientes grados de libertad
\[\begin{matrix}
    & \text{\textbf{Grados de libertad}} & \text{\textbf{Trasl}} & \text{\textbf{Rot}} & \text{\textbf{Vib}}\\ 
    \text{\textbf{Molécula no lineal de }}j-\text{\textbf{átomos}} & 3j & 3 & 2 & 3j-5\\
    \text{\textbf{Molécula lineal de }}j-\text{\textbf{átomos}} & 3j & 3 & 3 & 3j-6\\
\end{matrix}\]
Los grados de libertad de vibración lo denotaremos por el número de modos normales de vibración, $f_v$, que son suma de oscilaciones armónicas; cada modo normal posee una frecuencia determinada. Así, para oscilaciones pequeñas, tenemos
\[\epsilon_{\text{vib}}=\epsilon_v=\sum_{k=1}^{f_v}\epsilon_{v,k}\]
luego,
\begin{equation}
    \zeta^{\text{vib}}=\prod_{k=1}^{f_v}\zeta_{v,k}
\end{equation}
Entonces,
\[\left.\begin{matrix}
    \zeta_{v,k}=\sum_{v,k}e^{-\beta\epsilon_{v,k}}\\
    \epsilon_{v,k}=\left(n+\frac{1}{2}\right)\hbar\omega_k\\
    n=0,1,2,\dots
\end{matrix}\right\rbrace\Rightarrow\zeta_{v,k}=\sum_{n=0}^{\infty}e^{-\beta\left(n+\frac{1}{2}\right)\hbar\omega_k}=e^{-\frac{\hbar\omega_k}{2kT}}\sum_{n=0}^{\infty}e^{-\frac{n\hbar\omega_k}{kT}}\]
Usando que $\theta_{v,k}=\frac{\hbar\omega_k}{k}$, que denominamos \textbf{temperatura de vibración}. Así,
\begin{equation}
    \zeta_{v,k}=e^{-\frac{\theta_{v,k}}{2T}}\sum_{n=0}^{\infty}e^{-\frac{\theta_{v,k}}{T}n}
\end{equation}

$\theta_{\text{rot}}$ y $\theta_{v,k}$ son parámetros característicos del sistema, que definimos como temperaturas características de alguna propiedad del sistema.\\
A temperatura ambiente, unos 300 K, tenemos $\frac{\theta_{v,j}}{T}\ll1$. Así, podemos desarrollar en serie la exponencial, tal que
\[\zeta_{v,k}=e^{-\frac{\theta_{v,k}}{2T}}\sum_{n=0}^{\infty}e^{-\frac{\theta_{v,k}}{T}}\approx e^{-\frac{\theta_{v,k}}{2T}}\brackets{1+e^{-\frac{\theta_{v,k}}{T}}+e^{-\frac{\theta_{v,k}}{t}2}+\dots}\]
Esta suma converge si $e^{-\frac{\theta_{v,k}}{T}}<1$, luego, como a temperatura ambiente $\frac{\theta_{v,j}}{T}\ll1$, entonces $e^{-\frac{\theta_{v,j}}{T}}\ll1$ y por tanto, converge. Usando $\sum_{n=0}^{\infty}a_nr^n=\frac{a_1}{1-r}$, tenemos
\[\zeta_{v,k}=e^{-\frac{\theta_{v,k}}{2T}}\frac{1}{1-e^{-\frac{\theta_{v,k}}{T}}}=\frac{1}{e^{\frac{\theta_{v,k}}{2T}}}\frac{1}{1-e^{-\frac{\theta_{v,k}}{T}}}=\frac{1}{e^{\frac{\theta_{v,k}}{2T}}-e^{-\frac{\theta_{v,k}}{2T}}}\]
Usando la definición de seno hiperbólico, tenemos
\begin{equation}
    \zeta_{v,k}=\frac{1}{2\sinh(\frac{\theta_{v,k}}{2T})}
\end{equation}
Así, 
\begin{equation}
    Z_{\text{vib}}=(\zeta_{\text{vib}})^N=\prod_{k=1}^{f_v}\zeta_{v,k}^N=\prod_{k=1}^{f_v}\left(\frac{1}{2\sinh(\frac{\theta_{v,k}}{2T})}\right)^N
\end{equation}

Luego, la función de partición total será,

\[Z=Z_{\text{tral}}Z_{\text{rot}}Z_{\text{vib}}Z_{\text{elect}}\]

con

\[\begin{matrix}
    Z_{\text{trasl}}=\frac{V^N}{h^{3N}N!}\left(2\pi m kT\right)^{3N/2} & Z_{\text{vib}}=\prod_{k=1}^{f_v}\left(\frac{1}{\sinh(\frac{\theta_{v,k}}{2T})}\right)^N \\ \\
    Z_{\text{rot}}=\left(\frac{T}{\theta_{\text{rot}}\sigma}\right)^N & Z_{\text{elect}}
\end{matrix}\]

\subsubsection*{Nota}
Puede haber electrones excitados, los cuales no se encuentren en el orbital correspondiente. Se definirá $\zeta_{\text{elect}}=\sum e^{-\beta\epsilon_{\text{elect}}}$. La definición de energía es mucho más grande que $kT$, por tanto, a temperatura ambiente, la mayoría estarán en el estado fundamental y puede que en primer estado también, tal que
\begin{equation}
    \zeta_{\text{elect}}\approx g_0e^{-\beta\epsilon_0}+g_1e^{-\beta\epsilon_1}=e^{-\beta\epsilon_0}\brackets{g_0+g_1e^{-\beta(\epsilon_1-\epsilon_0)}}
\end{equation}

donde $g_0$ y $g_1$ son los términos de la degeneración. Además, el primer término nos lo llevamos a los grados de libertad de traslación, pues

\[\sum_{\text{trasl}}e^{-\beta\epsilon_{\text{trasl}}}\dots\sum_{\text{elect}}=\sum_{\text{trasl}}e^{-\beta\epsilon_{\text{trasl}}e^{-\beta\epsilon_0}}\dots\]

La energía media y la capacidad calorífica de rotación y vibración son

\begin{equation}
    \begin{matrix}
        \overline{E}_r=NkT & C_{v,r}=Nk \\ \\
        \overline{E}_v=\frac{N}{2}k\theta_v+Nk\frac{\theta_v}{e^{\frac{\theta_v}{T}}-1} & C_{v,v}=Nk\frac{\theta_v^2}{T^2}\frac{e^{\frac{\theta_v}{T}}}{\left(e^{\frac{\theta_v}{T}}-1\right)^2}
    \end{matrix}
\end{equation}

Los calculamos:

\[\overline{E}_r=-\frac{\partial\ln Z_r}{\partial\beta}=-\frac{\partial T}{\partial\beta}\frac{\partial\ln Z_r}{\partial T}=-\frac{1}{\frac{\partial\beta}{\partial T}}\frac{\partial\ln Z_r}{\partial T}kT^2\frac{\partial\ln Z_r}{\partial T}\]

sabiendo que $Z_r=\left(\frac{T}{\theta_r}\right)^N$, entonces $\ln Z_r=N\ln(\frac{T}{\frac{\hbar^2}{2Ik}})=N\ln(\frac{2IkT}{\hbar^2})$, luego
\[\overline{E}_r=NkT^2\frac{\hbar^2}{2IkT}\frac{2IK}{\hbar^2}=NkT\qedh\]

\[C_{v,r}=\frac{\partial\overline{E_r}}{\partial T}=Nk\qedh\]

\[\overline{E}_v=-\frac{\partial\ln Z_v}{\partial\beta}=-\frac{1}{\frac{\partial\beta}{\partial T}}\frac{\partial\ln Z_v}{\partial T}\]
sabiendo que $Z_v=\brackets{\frac{e^{\frac{\theta_v}{T}}}{1-e^{\frac{-\theta_v}{T}}}}^N$, luego $\ln Z_v=N\ln(\frac{e^{\frac{\theta_v}{T}}}{1-e^{\frac{-\theta_v}{T}}})=-\frac{N\theta_v}{2T}-N\ln(1-e^{\frac{-\theta_v}{T}})$, entonces
\[\overline{E}_v=kT^2\brackets{\frac{N\theta_v}{2T^2}-N\frac{e^{-\frac{\theta_v}{T}}}{1-e^{-\frac{\theta_v}{T}}}\left(-\frac{\theta_v}{T^2}\right)}=\frac{N}{2}k\theta_v+Nk\frac{\theta_v}{e^{\frac{\theta_v}{T}}-1}\qedh\]

\[C_{v,v}=\frac{\partial\overline{E}_v}{\partial T}=Nk\frac{\theta_v^2}{T^2}\frac{e^{\frac{\theta_v}{T}}}{\left(e^{\frac{\theta_v}{T}}-1\right)^2}\qedh\]

\section{Gas ideal cuántico débilmente degenerado}

Al ser débilmente degenerado, significa que $n\lambda^3$ no es mucho menor que 1, sino que es un poco menor, es decir, $n\lambda^3<1$.\\ \\
Sabiendo que $\ln Q=\pm\sum_r\ln\left(1\pm e^{-(\alpha+\beta\epsilon_r)}\right)$ y hacemos el desarrollo en serie del logaritmo hasta el primer orden, tal que
\[\ln Q\approx\cancel{\pm}\sum_r\brackets{\cancel{\pm}e^{-(\alpha+\beta\epsilon_r)}-\frac{\cancel{\pm}\left(e^{-(\alpha+\beta \epsilon_r)}\right)^2}{2}+\cancel{\dots}}\]
\[\ln Q=\sum_r\brackets{e^{-(\alpha+\beta\epsilon_r)}\mp\frac{e^{-2(\alpha+\beta\epsilon_r)}}{2}}=\sum_re^{-(\alpha+\beta\epsilon_r)}\mp\frac{1}{2}\sum_re^{-2(\alpha+\beta\epsilon_r)}\]
\[\ln Q=\sum_re^{-\alpha}e^{-\beta\epsilon_r}\mp\frac{1}{2}\sum_re^{-2\alpha}e^{-2\beta\epsilon_r}=e^{-\alpha}\sum_re^{-\beta\epsilon_r}\mp\frac{1}{2}e^{-2\alpha}\sum_re^{-2\beta\epsilon_r}\]
Así,
\begin{equation}
    \ln Q=e^{-\alpha}\sum_re^{-\beta\epsilon_r}\mp\frac{1}{2}e^{-2\alpha}\sum_re^{-2\beta\epsilon_r}
\end{equation}
siendo el signo positivo para bosones y el negativo, para fermiones.
\\ \\
Sabemos que 
\[\zeta=\sum_re^{-\beta\epsilon_r}=V\left(\frac{2\pi mkT}{h^2}\right)^{3/2}\]
entonces, como en el sumatorio sale elevado al cuadrado y $(a+b+\dots)^2\neq a^2+b^2+\dots$, tenemos que
\[\zeta=\sum_re^{-2\beta\epsilon_r}=V\left(\frac{2\pi mkT}{2h^2}\right)^{3/2}\]
Por tanto,
\[\ln Q=e^{-\alpha}V\left(\frac{2\pi mkT}{h^2}\right)^{3/2}\mp\frac{1}{2}e^{-2\alpha}V\left(\frac{2\pi mkT}{2h^2}\right)^{3/2}=V\left(\frac{2\pi mkT}{h^2}\right)^{3/2}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}\]
Recordando que $\lambda=\frac{h}{\left(2\pi mkT\right)^{1/2}}$
\begin{equation}
    \ln Q=\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}
\end{equation}
siendo el signo positivo para bosones y el negativo para fermiones.\\ \\
Tenemos que la energía media y el número medio de partículas son
\begin{equation}
    \overline{E}=\frac{3}{2}kT\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}
\end{equation}
\begin{equation}
    \overline{N}=\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{3/2}}e^{-2\alpha}}
\end{equation}
Lo calculamos:
\[\overline{E}=-\frac{\partial\ln Q}{\partial\beta}=-\frac{\partial}{\partial\beta}\left(\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}\right)\]
usando que $\lambda=\frac{h}{\left(2\pi mkT\right)^{1/2}}$, entonces $\frac{\partial\lambda}{\partial\beta}=-\frac{3}{2}kT\frac{1}{\lambda^3}$, tenemos
\[\overline{E}=\frac{3}{2}kT\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}\qedh\]

\[\overline{N}=-\frac{\partial\ln Q}{\partial\alpha}=-\frac{\partial}{\partial\alpha}\left(\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{5/2}}e^{-2\alpha}}\right)=\frac{V}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{3/2}}e^{-2\alpha}}\qedh\]
Tendremos un desarrollo en serie de $e^{-\alpha}$ (fugacidad), para todas las magnitudes físicas. Pero la fugacidad no es muy intuitiva, así que vamos a buscar una relación con $e^{-\alpha}$, con la densidad y temperatura, incluyendo la temperatura, pues ésta es independiente en los gases clásicos, luego, vamos a usar $n\lambda^3$.\\ \\
La densidad viene dada por

\[n=\frac{\overline{N}}{V}=\frac{1}{\lambda^3}\brackets{e^{-\alpha}\mp\frac{1}{2^{3/2}}e^{-2\alpha}}\]

luego, debe existir una función tal que $f\left(e^{-\alpha}\right)=n$, y entonces existirá $e^{-\alpha}=g(n)$. Así, desarrollamos la exponencial,
\[e^{-\alpha}=\sum_{i=1}^{\infty}a_in^i=a_1n^1+a_2n^2+\dots\]
empezamos en $i=1$, pues para $a_0$, la fugacidad será cero, pues no habrá sistema.\\ \\
Sustituimos,
\[n=\frac{1}{\lambda^3}\brackets{\left(a_1n+a_2n^2+\cancel{\dots}\right)\mp\frac{1}{2^{3/2}}\left(a_1n+a_2n^2+\cancel{\dots}\right)^2}=\frac{1}{\lambda^3}\brackets{\left(a_1n+a_2n^2\right)\mp\frac{1}{2^{3/2}}\left(a_1^2n^2+\cancel{a_2^2n^4}\right)}\]
Nos quedamos con los términos de $n^2$, luego
\[n=\frac{1}{\lambda^3}\brackets{a_1n+a_2n^2\mp\frac{1}{2^{5/2}}a_1^2n^2}\]
Comparando,
\[\begin{matrix}
    n^1: & 1=\frac{a_1}{\lambda^3} & \Rightarrow & a_1=\lambda^3\\ \\
    n^2: & 0=\frac{1}{\lambda^3}\brackets{a_2\mp\frac{1}{2^{3/2}}a_1^2} & \Rightarrow & a_2=\pm\frac{\lambda^6}{2^{3/2}}
\end{matrix}\]

Luego,

\[e^{-\alpha}==a_1n+a_2n^2=\lambda^3n\pm\frac{\lambda^6}{2^{3/2}}n^2=n\lambda^3\brackets{1\pm\frac{\lambda^6}{2^{3/2}}}\]

Así, podemos expresar cualquier magnitud física como un desarrollo en serie de $n\lambda^3$, siendo el primer orden el límite clásico.

\section{Gas de Fermi: el gas de electrones}

Cuando $n\lambda^3>1$, debemos recurrir al límite cuántico, usando
\[\ln Q=\pm\sum_r\ln(1\pm e^{-(\alpha+\beta\epsilon_r)})\hspace{15mm}\overline{n}_r=\frac{1}{e^{(\alpha+\beta\epsilon_r)}\pm1}\]

Comenzamos estudiando los fermiones. Tomamos un gas ideal de electrones de conducción en un metal, siendo ideal, pues el promedio de los electrones en conducción experimenta un potencial constante por el apantallamiento. Además, algunas propiedades de este gas solo pueden estar determinadas por efectos cuánticos.\\ \\
Como estamos con fermiones, usaremos la estadística de Fermi-Dirac, así

\[\overline{n}_r=\frac{1}{e^{(\beta\epsilon_r+\alpha)}+1}\overset{\curlybraces{\alpha=-\beta\mu}}{=}\frac{1}{e^{\beta(\epsilon_r-\mu)}+1}\]
siendo $\mu$ el \textit{nivel de Fermi} y para $\mu(T=0)=\mu_0$.\\ \\
Debajo de la energía de Fermi, todos los niveles están ocupados, y por encima, están libres.\\ \\
Vamos a determinar una función de distribución para la energía de los electrones: número medio de los electrones que, en un volumen $V$, tienen una energía comprendida entre $\epsilon$ y $\epsilon+d\epsilon$. Definiendo $f(\epsilon)d\epsilon$ como el número de electrones que hay en un estado de energía $\epsilon$ por el número de estados con energía comprendida entre $\epsilon$ y $\epsilon+d\epsilon$.
\\ \\
Teniendo en cuenta que los electrones tienen espín 1/2 y -1/2, tendrá un número de degeneración y así, el número de estados será $2s+1$. Sabiendo que $\rho(\epsilon)d\epsilon$ es la densidad de probabilidad para una partícula encerrada en una caja, por tanto, al tener $2s+1$-números de estados, deberemos multiplicar $\rho(\epsilon)d\epsilon$ por $2s+1$. Así, usando que $\rho(\epsilon)d\epsilon=\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}d\epsilon$, tenemos

\[\rho(\epsilon)d\epsilon=(2s+1)\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}d\epsilon\]

Por tanto, $f(\epsilon)d\epsilon=\overline{n}_r\rho(\epsilon)d\epsilon$, luego

\[f(\epsilon)d\epsilon=\overline{n}_r\rho(\epsilon)d\epsilon=\frac{1}{e^{\beta(\epsilon_r-\mu)}+1}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}d\epsilon\]
Representamos $f(\epsilon)$,
\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{kk.png}
    \captionof{figure}{Representación de $f(\epsilon)$ para distintos $T$.}
    \label{fig:51-algos}
\end{Figura}
A partir de la función de distribución podemos calcular el valor medio de la energía y el número de partículas, tal que
\begin{equation}
    \overline{E}=\int_0^{\infty}\epsilon f(\epsilon)d\epsilon=\int_0^{\infty}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon\frac{\epsilon^{1/2}}{e^{\beta(\epsilon_r-\mu)}+1}d\epsilon=\int_0^{\infty}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\epsilon^{3/2}}{e^{\beta(\epsilon_r-\mu)}+1}d\epsilon
\end{equation}


\begin{equation}
    \overline{N}=\int_0^{\infty} f(\epsilon)d\epsilon=\int_0^{\infty}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\epsilon^{1/2}}{e^{\beta(\epsilon_r-\mu)}+1}d\epsilon
\end{equation}
tal que, en un sistema cerrado nos permite calcular el nivel de Fermi.\\ \\
\begin{tabular}{c|}
     Para $T=0$ K  \\ \hline
\end{tabular}
\\ \\
Como estamos a $T=0$ K, solo habrá un microestado posible, y la entropía será nula, tal que
\[S=k\brackets{\frac{\overline{P}V}{kT}+\beta\overline{E}-\beta\overline{N}\mu_0}=0\]
Luego,
\[\overline{N}=\int_0^{\infty}f(\epsilon)d\epsilon=\int_0^{\mu_0}f(\epsilon)d\epsilon+\cancelto{0}{\int_{\mu_0}^{\infty}f(\epsilon)d\epsilon}\]
\[\text{En }T=0\Rightarrow\overline{n}_r=\left\lbrace\begin{matrix}
    1 & \text{si} & \epsilon<\mu_0\\
    0 & \text{si} & \epsilon>\mu_0
\end{matrix}\right.\]
Luego,
\[\overline{N}=\int_0^{\mu_0}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}d\epsilon=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\int_{0}^{\mu_0}\epsilon^{1/2}d\epsilon\]
Entonces,
\begin{equation}
    \overline{N}=\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{3/2}
\end{equation}
Además, calculamos el valor medio de la energía,
\[\overline{E}=\int_0^{\infty}\epsilon f(\epsilon)d\epsilon=\int_0^{\mu_0}\epsilon f(\epsilon)d\epsilon+\cancelto{0}{\int_{\mu_0}^{\infty}\epsilon f(\epsilon)d\epsilon}\]
\[\text{En }T=0\Rightarrow\overline{n}_r=\left\lbrace\begin{matrix}
    1 & \text{si} & \epsilon<\mu_0\\
    0 & \text{si} & \epsilon>\mu_0
\end{matrix}\right.\]
Luego,
\[\overline{E}=\int_{0}^{\mu_0}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{3/2}d\epsilon=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\mu_0}\epsilon^{3/2}d\epsilon\]
Entonces,
\begin{equation}
    \overline{E}=\frac{16\pi V}{5h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}
\end{equation}

Usando la función de estado,

\begin{equation*}
    \overline{P}V=\frac{2}{3}\overline{E}=\frac{2}{3}\frac{16\pi V}{5h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}=\frac{2}{5}\overline{N}\mu_0
\end{equation*}
Por tanto,
\begin{equation}
    \overline{P}=\frac{2}{5}\frac{N}{V}\mu_0
\end{equation}
Comprobamos que $S=0$, así
\[S=k\brackets{\frac{\overline{P}V}{kT}+\beta\overline{E}-\beta\overline{N}\mu_0}=k\brackets{\beta\overline{P}V+\beta\overline{E}-\beta\overline{N}\mu_0}=k\beta\brackets{\frac{2}{3}\overline{E}+\overline{E}-\overline{N}\mu_0}=k\beta\brackets{\frac{5}{3}\overline{E}-\overline{N}\mu_0}\]
\[S=k\beta\brackets{\frac{\cancel{5}}{3}\frac{16\pi V}{\cancel{5}h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}-\mu_0\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{3/2}}\]
\[S=k\beta\brackets{\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}-\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}}=0\qedh\]
Calculamos la energía media por partícula,
\[\frac{\overline{E}}{\overline{N}}=\frac{\frac{\cancel{16}\cancel{\pi} \cancel{V}}{5\cancel{h^3}}\cancel{\left(2m^3\right)^{1/2}}\mu_0^{\cancel{5/2}}}{\frac{\cancel{16}\cancel{\pi} \cancel{V}}{3\cancel{h^3}}\cancel{\left(2m^3\right)^{1/2}}\cancel{\mu_0^{3/2}}}=\frac{3}{5}\mu_0\]
Luego,
\begin{equation}
    \overline{E}=\frac{3}{5}\overline{N}\mu_0
\end{equation}
Despejando $\mu_0$ de $\overline{N}$, tenemos
\begin{equation}
    \mu_0=\frac{h^2}{8m}\left(\frac{3}{\pi}\right)^{2/3}\left(\frac{N}{V}\right)^{2/3}
\end{equation}
 es decir, $\mu_0$ dependerá únicamente de $\frac{N}{V}$, que es la densidad del gas de electrones.\\ \\
 A partir de $\mu_0$ (energía de Fermi), podemos definir la temperatura de Fermi, dividiendo por la constante de Boltzmann, tal que 
 \begin{equation}
     T_F=\frac{\mu_0}{k}
 \end{equation}
siendo una característica de los metales ($\sim10^{4},\sim10^5$ K).
\subsubsection*{Nota}
El significado físico de la temperatura de Fermi es la temperatura que debe tener un gas clásico para que tenga la misma energía que la de un gas de electrones. Pues $\overline{E}=\frac{3}{5}\overline{N}\mu_0=\frac{3}{5}\overline{N}kT_F$, pues clásicamente $E_{clas}=\frac{3}{2}\overline{N}kT$.\\ \\
Ahora, si el gas de Fermi tiene una $T\neq0$, su diferencia con $T=0$ es bastante pequeña, luego deberán servir las mismas expresiones de $T=0$ para $T\neq0$, pues el error que estamos cometiendo es de aproximadamente 0.1, ya que será del oren $\frac{T}{T_F}$. Así,
\[\overline{E}(T)\approx\frac{3}{5}N\mu_0\]
pero entonces, tendríamos que la capacidad calorífica es
\[C_v=\frac{\partial\overline{E}}{\partial T}=\frac{\partial}{\partial T}\left(\frac{3}{5}N\mu_0\right)=0\]
pero experimentalmente se observa que $C_v\neq0$, luego, para calcular la capacidad calorífica NO podemos aproximar la distribución de Fermi por su valor para $T=0$ K.
\\ \\
Sabiendo que la función de distribución es $\frac{1}{e^{\beta(\epsilon-\mu)}+1}$ y la magnitud física será $\varphi(\epsilon)$, entonces el valor medio de la magnitud física será,
\[\overline{\varphi}(\epsilon)=\int_{0}^{\infty}\frac{1}{e^{\beta(\epsilon-\mu)}+1}\varphi(\epsilon)d\epsilon=\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon\Rightarrow\left\lbrace\begin{matrix}
    \overline{N} & \rightarrow & \varphi(\epsilon)=\rho(\epsilon)\\
    \overline{E} & \rightarrow & \varphi(\epsilon)=\epsilon\rho(\epsilon)
\end{matrix}\right.\]
Así, resolvemos esta integral de forma lo más general posible y solo tendremos que particularizar para los $\varphi(\epsilon)$ correspondientes.\\ \\
Para calcularlo, recurrimos al desarrollo de Sommerfeld, que consistirá en resolver la integral por partes,
\[\begin{matrix}
    u=\overline{n}(\epsilon) & \rightarrow & du=\overline{n}'(\epsilon)d\epsilon\\
    dv=\varphi(\epsilon)d\epsilon & \rightarrow & v=\int_{0}^{\epsilon}\varphi(\epsilon')d\epsilon'=\Psi(\epsilon)
\end{matrix}\]
\[\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=\left.\overline{n}(\epsilon)\Psi(\epsilon)\right|_0^{\infty}-\int_0^{\infty}\Psi(\epsilon)\overline{n}'(\epsilon)d\epsilon=\lim_{\epsilon\rightarrow\infty}(\overline{n}(\epsilon)\Psi(\epsilon))-\overline{n}(0)\Psi(0)-\int_0^{\infty}\Psi(\epsilon)\overline{n}'(\epsilon)d\epsilon\]

donde $\lim_{\epsilon\rightarrow\infty}\overline{n}(\epsilon)=0$ y $\lim_{\epsilon\rightarrow\infty}\Psi(\epsilon)=\infty$, teniendo una indeterminación de $0\cdot\infty$, pero en este caso, $\overline{n}(\epsilon)$ le gana al caer de forma exponencial, pues $\Psi(\epsilon)$ crece en forma de potencias y la exponencial siempre le gana. Además, tenemos que $\overline{n}(0)=1$, pues en el estado fundamental solo hay una partícula y $\Psi(\epsilon)=0$, pues $\Psi(\epsilon)$ va como potencias de base $\epsilon$ (una especie de polinomio sin el $a_0$). Luego,

\[\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=-\int_0^{\infty}\Psi(\epsilon)\overline{n}'(\epsilon)d\epsilon\]

Calculamos $\overline{n}'(\epsilon)$, tal que 

\[\overline{n}'(\epsilon)=\frac{d\overline{n}}{d\epsilon}=\frac{d}{d\epsilon}\left(\frac{1}{e^{\beta(\epsilon-\mu)}+1}\right)=-\frac{\beta e^{\beta(\epsilon-\mu)}}{\left(e^{\beta(\epsilon-\mu)}+1\right)^2}\]
Representamos la derivada,
\begin{Figura}
    \centering
    \includegraphics[width=0.5\textwidth]{jjjj.png}
    \captionof{figure}{Representación de $\frac{d\overline{n}}{d\epsilon}$}
    \label{fig:52-algos}
\end{Figura}
donde vemos que la derivada solamente es distinta de cero en un intervalo del orden $kT$ en torno al nivel de Fermi. Luego, la integral solamente contribuye a este intervalo del orden $kT$, por lo que haremos un desarrollo en serie de $\Psi(\epsilon)$ en torno al nivel de Fermi, tal que

\[\Psi(\epsilon)=\Psi(\mu)+\left.\frac{d\Psi}{d\epsilon}\right|_{\epsilon=\mu}(\epsilon-\mu)+\frac{1}{2}\left.\frac{d^2\Psi}{d\epsilon^2}\right|_{\epsilon=\mu}(\epsilon-\mu)^2+\dots=\sum_{m=0}^{\infty}\frac{1}{m!}\left.\frac{d^m\Psi}{d\epsilon^m}\right|_{\epsilon=\mu}(\epsilon-\mu)^m\]

Entonces, sustituyendo en la integral,

\[\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=-\int_0^{\infty}\overline{n}'(\epsilon)\brackets{\sum_{m=0}^{\infty}\frac{1}{m!}\left.\frac{d^m\Psi}{d\epsilon^m}\right|_{\epsilon=\mu}(\epsilon-\mu)^m}d\epsilon=\sum_{m=0}^{\infty}\frac{1}{m!}\left.\frac{d^m\Psi}{d\epsilon^m}\right|_{\epsilon=\mu}\int_0^{\infty}\overline{n}'(\epsilon)(\epsilon-\mu)^md\epsilon\]
Sustituímos la derivada,
\[\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=\sum_{m=0}^{\infty}\frac{1}{m!}\left.\frac{d^m\Psi}{d\epsilon^m}\right|_{\epsilon=\mu}\int_0^{\infty}\frac{\beta e^{\beta(\epsilon-\mu)}}{\left(e^{\beta(\epsilon-\mu)}+1\right)^2}(\epsilon-\mu)^md\epsilon\]

Calculamos la integral,

\[\int_0^{\infty}\frac{\beta e^{\beta(\epsilon-\mu)}}{\left(e^{\beta(\epsilon-\mu)}+1\right)^2}(\epsilon-\mu)^md\epsilon=\curlybraces{\begin{matrix}
    x=\beta(\epsilon-\mu)\\
    dx=\beta d\epsilon
\end{matrix}}=\int_{-\beta\mu}^{\infty}\frac{-\cancel{\beta}}{\left(e^x+1\right)^2}\frac{x^m}{\beta^m}\frac{dx}{\cancel{\beta}}=-\frac{1}{\beta^m}\int_{-\beta\mu}^{\infty}\frac{x^me^x}{\left(e^x+1\right)^2}dx\]
Usando que $\beta\mu\gg1$, podemos aproximar, tal que $\beta\mu=\frac{\beta}{kT}\approx\frac{\cancel{k}T_F}{\cancel{k}T}=\frac{T_F}{T}\gg1$ y entonces $-\beta\mu\ll-1\Rightarrow-\beta\mu\approx-\infty$. Así,
\[\int_0^{\infty}\frac{\beta e^{\beta(\epsilon-\mu)}}{\left(e^{\beta(\epsilon-\mu)}+1\right)^2}(\epsilon-\mu)^md\epsilon=-\frac{1}{\beta^m}\int_{-\infty}^{\infty}\frac{x^me^x}{\left(e^x+1\right)^2}dx=-(kT)^{m}\int_{-\infty}^{\infty}\frac{x^me^x}{\left(e^x+1\right)^2}dx=\]
\[=-(kT)^{m}\int_{-\infty}^{\infty}\frac{x^m}{e^{-x}\left(e^x+1\right)\left(e^x+1\right)}dx=-(kT)^{m}I_m\]
La integral $I_m$ dependerá de si $m$ es par o impar, pues si $m$ es par la función será par y si $m$ es impar, será impar. De esta forma, al ser el intervalo de integración un intervalo par, para $m$ impar tendremos que la integral es cero, pues tenemos una función impar en un intervalo par. De este modo, para no tener soluciones nulas, solo nos quedaremos con los $m$ pares, teniendo $I_0=1$, $I_2=\frac{\pi^2}{3}$, etc. Luego,
\[\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=\sum_{m=0}^{\infty}\frac{1}{m!}\left.\frac{d^m\Psi}{d\epsilon^m}\right|_{\epsilon=\mu}(kT)^mI_m\hspace{10mm}m\in2\mathbb{Z}^+\]
siendo $2\mathbb{Z}^+$ el conjunto de los números pares positivos, incluyendo el cero.\\ \\
Desarrollamos el sumatorio,
\begin{equation}
\overline{\varphi}(\epsilon)=\int_0^{\infty}\overline{n}(\epsilon)\varphi(\epsilon)d\epsilon=\Psi(\mu)+\frac{(kT)^2\pi^2}{6}\left.\frac{d^2\Psi}{d\epsilon^2}\right|_{\epsilon=\mu}+\dots
\label{ec56}
\end{equation}
Vamos a demostrar que el nivel de Fermi, le energía media y la capacidad calorífica vienen dadas por,
\begin{equation}
    \mu=\mu_0\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\hspace{10mm}\overline{E}=\frac{3}{5}N\mu_0\brackets{1+\frac{5\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\hspace{10mm}C_v=\frac{\pi^2}{2}Nk\frac{T}{T_F}
\end{equation}
\subsubsection*{Nivel de Fermi}
Comenzamos calculando $\overline{N}$, usando la ecuación (\ref{ec56}), tal que
\[\overline{N}=\int_0^{\infty}f(\epsilon)d\epsilon=\int_0^{\infty}\overline{n}(\epsilon)\rho(\epsilon)d\epsilon\overset{\curlybraces{\varphi(\epsilon)=\rho(\epsilon)}}{=}\Psi(\mu)+\frac{(kT)^2\pi^2}{6}\left.\frac{d^2\Psi}{d\epsilon^2}\right|_{\mu}+\dots\]
Buscamos $\Psi(\epsilon)$,
\[\Psi(\epsilon)=\int_0^{\epsilon}\rho(\epsilon')d\epsilon'=\int_0^{\epsilon}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon'^{1/2}d\epsilon'=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\epsilon^{3/2}}{3/2}=\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\epsilon^{3/2}\]
Calculamos las derivadas de $\Psi$,
\[\frac{d\Psi}{d\epsilon}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}e^{1/2}\]
\[\frac{d^2\Psi}{d\epsilon^2}=\frac{8\pi V}{2h^3}\left(2m^3\right)^{1/2}\epsilon^{-1/2}\]

Sustituimos,

\[\overline{N}=\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu^{3/2}+\frac{(kT)^2\pi^2}{6}\frac{8\pi V}{2h^3}\left(2m^3\right)^{1/2}\mu^{-1/2}\]

Sabemos además que

\[\overline{N}(T=0)=\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{3/2}\]

Igualamos,

\[\frac{\cancel{16}\pi V}{\cancel{3}h^3}\left(2m^3\right)^{1/2}\mu_0^{3/2}=\frac{\cancel{16}\pi V}{\cancel{3}h^3}\left(2m^3\right)^{1/2}\mu^{3/2}+\frac{(kT)^2\pi^2}{\cancelto{2}{6}}\frac{\cancelto{1/2}{8}\pi V}{2h^3}\left(2m^3\right)^{1/2}\mu^{-1/2}\]
\[\mu_0^{3/2}=\mu^{3/2}+\frac{(kT\pi)^2}{8}\mu^{-1/2}\]

Como en un sumando tenemos $T^2$, al quedarnos solo con los $T^2$, podemos aproxima este $\mu\approx\mu_0$, luego
\[\mu_0^{3/2}\approx\mu^{3/2}+\frac{(kT\pi)^2}{8}\mu_0^{-1/2}\]
Despejamos $\mu$,

\[\mu^{3/2}=\mu_0^{3/2}-\frac{(kT\pi)^2}{8}\mu_0^{-1/2}=\mu_0^{3/2}\brackets{1-\frac{(kT\pi)^2}{8\mu_0^{1/2}\mu_0^{3/2}}}=\mu_0{3/2}\brackets{1-\frac{(kT\pi)^2}{8\mu_0^2}}\overset{\curlybraces{T_F=\frac{\mu_0}{k}}}{=}\mu_0^{3/2}\brackets{1-\frac{\pi^2}{8}\left(\frac{T}{T_F}\right)^2}\]

Luego,

\[\mu=\mu_0\brackets{1-\frac{\pi^2}{8}\left(\frac{T}{T_F}\right)^2}^{2/3}\]
Como $\frac{T}{T_F}\ll1$, podemos hacer un desarrollo de Taylor, tal que $(1+x)^n\approx1+nx$, por tanto

\[\mu\approx\mu_0\brackets{1-\frac{2}{3}\frac{\pi^2}{8}\left(\frac{T}{T_F}\right)^2}=\mu_0\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\qedh\]

\subsubsection*{Energía media}

Tomamos,

\[\varphi(\epsilon)=\epsilon\rho(\epsilon)=\epsilon\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{3/2}\]

Luego, calculamos $\Psi(\epsilon)$,

\[\Psi(\epsilon)=\int_0^{\epsilon}\varphi(\epsilon')d\epsilon'=\int_0^{\epsilon}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon'^{3/2}d\epsilon'=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\epsilon}\epsilon'^{3/2}d\epsilon'=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\epsilon^{5/2}}{5/2}\]

Calculamos las derivadas,

\[\frac{d\Psi}{d\epsilon}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{3/2}\]

\[\frac{d^2\Psi}{d\epsilon^2}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{3}{2}\epsilon^{1/2}\]

Luego,

\[\overline{E}=\int_0^{\infty}\epsilon\overline{n}(\epsilon)\rho(\epsilon)d\epsilon=\Psi(\mu)+\frac{(kT)^2\pi^2}{6}\left.\frac{d^2\Psi}{d\epsilon^2}\right|_{\mu}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\mu^{5/2}}{5/2}+\frac{(kT)^2\pi^2}{6}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{3}{2}\mu^{1/2}\]

Para el sumatorio con $T^2$, aproximamos $\mu\approx\mu_0$,

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\mu^{5/2}}{5/2}+\frac{(kT)^2\pi^2}{6}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{3}{2}\mu_0^{1/2}\]

Sabemos que

\[\mu=\mu_0\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\]

Sustituyendo,

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{\mu_0^{5/2}\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}^{5/2}}{5/2}+\frac{(kT)^2\pi^2}{6}\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\frac{3}{2}\mu_0^{1/2}\]

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\brackets{\frac{2}{5}\mu_0^{5/2}\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}^{5/2}+\frac{(kT)^2\pi^2}{4}\mu_0^{1/2}}\]

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}\brackets{\frac{2}{5}\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}^{5/2}+\frac{(kT)^2\pi^2}{4}\frac{\mu_0^{1/2}}{\mu_0^{5/2}}}\]

Usando que $T_F=\frac{\mu_0}{k}$,

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}\frac{2}{5}\brackets{\brackets{1-\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2}^{5/2}+\frac{5}{2}\frac{\pi^2}{4}\left(\frac{T}{T_F}\right)^2}\]

Sabiendo que $\frac{T}{T_F}\ll1$, hacemos el desarrollo en serie de $(1+x)^n=1+nx$, tal que

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}\frac{2}{5}\brackets{1-\frac{5}{2}\frac{\pi^2}{12}\left(\frac{T}{T_F}\right)^2+\frac{5\pi^2}{8}\left(\frac{T}{T_F}\right)^2}\]

\[\overline{E}=\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}\mu_0^{5/2}\frac{2}{5}\brackets{1+\frac{5\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\]

Usando el valor de $\overline{N}$, 

\[\overline{N}=\frac{16\pi V}{3h^3}\left(2m^3\right)^{1/2}\mu_0^{3/2}\Rightarrow\frac{8\pi V}{h^3}\left(2m^3\right)^{1/2}=\frac{3\overline{N}}{2\mu_0^{3/2}}\]

Luego,

\[\overline{E}=\frac{3\overline{N}}{2\mu_0^{3/2}}\mu_0^{5/2}\frac{2}{5}\brackets{1+\frac{5\pi^2}{12}\left(\frac{T}{T_F}\right)^2}=\frac{3}{5}N\mu_0\brackets{1+\frac{5\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\qedh\]

\subsubsection*{Capacidad calorífica}

Sabemos que,

\[C_v=\frac{\partial\overline{E}}{\partial T}=\frac{\partial}{\partial T}\left(\frac{3}{5}N\mu_0\brackets{1+\frac{5\pi^2}{12}\left(\frac{T}{T_F}\right)^2}\right)=\frac{3}{5}N\mu_0\frac{5\pi^2}{12}2\frac{T}{T_F^2}\]

Usando que $\mu_0=T_Fk$

\[C_v=\frac{3}{6}NT_Fk\pi^2\frac{T}{T_F^2}=\frac{\pi^2}{2}Nk\frac{T}{T_F}\qedh\]

\section{Gas de Bose degenerado. Condensación de Bose-Einstein}

Tenemos un gas cuántico de bosones (no cumple el principio de Pauli). Tenemos,

\[\overline{n}_r=\frac{1}{e^{(\beta\epsilon_r+\alpha}-1}=\frac{1}{e^{\beta(\epsilon_r-\mu)}-1}\]
\[\overline{N}=\sum_r\frac{1}{e^{\beta(\epsilon_r-\mu)}-1}\]
siendo $\mu$ el potencial químico.\\ \\
Tenemos como hipótesis que en el nivel fundamental se cumple $\epsilon_0=0$, y por tanto $\mu\leq0$ necesariamente para que $\epsilon-\mu\geq$, pues $\epsilon>0$.\\ \\
Tendremos una función de distribución para energías de los bosones:
número medio de bosones que, en el volumen $V$, tienen una energía
comprendida entre $\epsilon$ y $\epsilon+d\epsilon$. Por tanto, calculamos la función d distribución como el número de bosones que hay en un estado de energía $\epsilon$ multiplicado por el número de estados con energía comprendida entre $\epsilon$ y $\epsilon+d\epsilon$, siendo $f(\epsilon)d\epsilon$.
\[\rho(\epsilon)d\epsilon=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}d\epsilon\hspace{10mm}g=2s+1\]
siendo $g$ la degeneración del spín. Luego,

\[f(\epsilon)d\epsilon=\overline{n}_r\rho(\epsilon)d\epsilon=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\epsilon^{1/2}\frac{1}{e^{\beta(\epsilon_r-\mu)}-1}d\epsilon\]
Tenemos 2 casos,
\begin{enumerate}[(a)]
    \item Supongamos que tenemos una densidad fija y vamos disminuyendo la temperatura tal que
    \[\frac{N}{V}=g\frac{4\pi}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{1/2}}{e^{\beta(\epsilon_r-\mu)}-1}d\epsilon\]
    Luego, si $T$ disminuye, entonces $\epsilon-\mu$ disminuirá, tal que $\epsilon-\mu=\epsilon+\abs{\mu}$ y entonces, $\mu$ irá aumentando hasta llegar a cero (pues $\mu\leq0$) y $\abs{\mu}$ irá disminuyendo hasta llegar a cero.\\
    Denotamos $T_0$ a la temperatura donde $\mu$ se hace cero, teniendo $\beta_0=\frac{1}{kT_0}$, luego,
    \[N=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{1/2}}{e^{\beta_0\epsilon_r}-1}d\epsilon\]
    pues estamos con $\epsilon\geq0$. Haciendo $z=\beta_0\epsilon\rightarrow dz=\beta_0d\epsilon$, 
    \[N=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{(z/\beta_0)^{1/2}}{e^z-1}\frac{dz}{\beta_0}=g\frac{4\pi V}{h^3\beta_0^{3/2}}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{z^{1/2}}{e^z-1}dz\]

Vemos que este tipo de integrales no son analíticas, y se debe recurrir a la Gamma de Euler y a la Zeta de Riemann, tal que

\[\int_0^{\infty}\frac{z^{x-1}}{e^z-1}dz=\Gamma(x)\zeta(x)\hspace{5mm}x\geq0\]

luego, como tenemos $x-1=\frac{1}{2}$, entonces $x=\frac{3}{2}$, sabiendo que $\Gamma\left(\frac{3}{2}\right)=\frac{\sqrt{\pi}}{2}$\footnote{Usando la propiedad $\Gamma(a+1)=a\Gamma(a)$}, tenemos

 \[N=g\frac{4\pi V}{\beta_0^{3/2}h^3}\frac{\sqrt{\pi}}{2}\zeta\left(\frac{3}{2}\right)=g\frac{4\pi V}{h^3}\frac{\sqrt{\pi}}{2}\zeta\left(\frac{3}{2}\right)(kT_0)^{3/2}\]

Despejando $T_0$,

\[T_0=\frac{1}{g^{2/3}}\frac{h^2}{(2m\pi k)^{3/2}}\frac{1}{\zeta(3/2)^{2/3}}\left(\frac{N}{V}\right)^{2/3}\]

Si tuviéramos $T<T_0$, no tendríamos sistema, pues obligaríamos necesariamente que $\mu>0$, cargándonos las condiciones iniciales.

\item Supongamos que tenemos temperatura fija y vamos aumentando $N$, tal que

\[\frac{N}{V}=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{1/2}}{e^{\beta(\epsilon-\mu)}-1}d\epsilon\]

Si $N$ aumenta, entonces $\epsilon-\mu$ disminuye, y al ser $\epsilon-\mu=\epsilon+\abs{\mu}$, entonces si $\abs{\mu}$ disminuye hasta anularse, $\mu$ aumenta hasta llegar a cero. 

\[N_{max}=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{1/2}}{e^{\beta\epsilon}-1}d\epsilon=\curlybraces{\begin{matrix}
    z=\beta\epsilon\\
    dz=\beta d\epsilon
\end{matrix}}=g\frac{4\pi V}{h^3\beta^{3/2}}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{z^{1/2}}{e^{z}-1}dz\]

Vemos que este tipo de integrales no son analíticas, y se debe recurrir a la Gamma de Euler y a la Zeta de Riemann, tal que

\[\int_0^{\infty}\frac{z^{x-1}}{e^z-1}dz=\Gamma(x)\zeta(x)\hspace{5mm}x\geq0\]

luego, como tenemos $x-1=\frac{1}{2}$, entonces $x=\frac{3}{2}$, sabiendo que $\Gamma\left(\frac{3}{2}\right)=\frac{\sqrt{\pi}}{2}$\footnote{Usando la propiedad $\Gamma(a+1)=a\Gamma(a)$}, tenemos

\[N_{max}=g\frac{(2\pi m)^{3/2}}{h^3}\zeta\left(\frac{3}{2}\right)(kT)^{3/2}\]

Observamos que si $T\rightarrow0$, el número máximo de partículas se anula, luego no tenemos partículas en el estado fundamental ($T=0$), por tanto, teóricamente no podemos tener gases de Bose a temperaturas muy bajas, pero experimentalmente se ha logrado tener gases de Bose a temperaturas extremadamente bajas, por tanto debe existir el estado fundamental.\\ \\
El fallo que se ha cometido ha sido al hace $\overline{N}$ y $\rho(\epsilon)d\epsilon$, pues no contempla la posibilidad de qué pasa $\epsilon=0$, es decir, no tienen en cuenta el estado fundamental. Esto se soluciona sacando del sumatorio de $\overline{N}$ el estado fundamental, tal que

\[\overline{N}=g\frac{1}{e^{-\beta}-1}+\sum_{\begin{matrix}
    r\\
    (\epsilon_r\neq0)
\end{matrix}}\frac{1}{e^{\beta(\epsilon_r-\mu)}-1}=N_0+\overline{N}'\]

donde $N_0$ se refiere a los bosones en el estado fundamental y $\overline{N}'$, a los bosones en los estados excitados. Luego,

\[\overline{N}\approx g\frac{1}{e^{-\beta}-1}+g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{1/2}}{e^{\beta(\epsilon_r-\mu)}-1}d\epsilon\]
Para resolver este tipo de integrales debemos recurrir a métodos numéricos, por lo que hacemos la siguiente comparación:\\ 
Para un $N$ dado, definimos $T_0$ con la fórmula anterior,
\[T_0=\frac{1}{g^{2/3}}\frac{h^2}{(2m\pi k)^{3/2}}\frac{1}{\zeta(3/2)^{2/3}}\left(\frac{N}{V}\right)^{2/3}\]
de donde despejamos $N$, tal que
\[N=g\frac{(2\pi m)^{3/2}}{h^3}\zeta\left(\frac{3}{2}\right)(kT_0)^{3/2}\]
siendo $\mu$ pequeño, pero no es cero. Luego, para una $T$ dada, el número máximo de partículas en estados excitados es,
\[N_{max}'=g\frac{(2\pi m)^{3/2}}{h^3}\zeta\left(\frac{3}{2}\right)(kT)^{3/2}\]
Para una $T$ dada, el número máximo de partículas en estados excitados respecto
al número total de partículas es:
\[\frac{N_{max}'}{N}=\left(\frac{T}{T_0}\right)^{3/2}\]
Tendremos varios casos,
\begin{itemize}
    \item Si $T\gg T_0$, entonces $N_{max}'\gg N$, por tanto $\overline{N}_0=g\frac{1}{e^{-\beta\epsilon}-1}\ll1$, con $\mu\ll1\neq0$
    \item Si $T\approx T_0$, entonces $N_{max}'\approx N$
    \item Si $T<T_0$, entonces $N_{max}'\ll N$, luego $\overline{N}_0=N-N'$ aumentará, pues como tenemos más partículas que estados máximos, entonces las partículas sobrantes van a estar en el estado fundamental.
    \item Si $T\rightarrow0$, tendremos todas las partículas en el estado fundamental y este estado es lo que se conoce como \textbf{Condensación de Bose-Einstein}.
\end{itemize}
\end{enumerate}

\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{ccc.png}
    \captionof{figure}{Medida experimental de una condensación de Bose-Einstein.}
    \label{fig:53-algos}
\end{Figura}
Vamos a calcular la energía media y el potencial químico de un gas de Bose para temperaturas
próximas al cero absoluto, que son
\begin{equation}
    \mu=-g\frac{k}{\overline{N}}T\hspace{15mm}\overline{E}=g\frac{3V}{\beta^{5/2}}\frac{\left(2m^3\right)\pi^{3/2}}{h^3}\zeta\left(\frac{5}{2}\right)
\end{equation}
\subsubsection*{Potencial químico}
Partimos de $\overline{N}$, pero como estamos en un gas de Bose, tenemos
\[N=N_0+N'=g\frac{1}{e^{-\beta\mu}-1}+\int_0^{\infty}\overline{n}(\epsilon)\rho(\epsilon)d\epsilon\]
Para $T\rightarrow0$, casi todas las partículas estarán en el estado fundamental, $N_0$, luego $N_0\gg N'$, con lo que podemos despreciar $N'$, así
\[\overline{N}\approx N_0=g\frac{1}{e^{-\beta\mu}-1}\Rightarrow\frac{g}{\overline{N}}=e^{-\beta\mu}-1\Rightarrow e^{-\beta\mu}=1+\frac{g}{\overline{N}}\Rightarrow-\beta\mu=\ln(1+\frac{g}{\overline{N}})\Rightarrow\mu=-kT\ln(1+\frac{g}{\overline{N}})\]
Como $\frac{g}{\overline{N}}\ll1$, aproximamos el logaritmo usando $\ln(1+x)\approx x$, luego
\[\mu\approx-kT\frac{g}{\overline{N}}=-g\frac{k}{\overline{N}}T\qedh\]

\subsubsection*{Energía media}

Como para temperaturas inferiores a $T_0$ sabemos que $\mu\ll1$, podemos escribir
\[\overline{E}=\int_0^{\infty}\rho(\epsilon)\epsilon\overline{n}(\epsilon)d\epsilon=g\frac{4\pi V}{h^3}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{\epsilon^{3/2}}{e^{\beta\epsilon}-1}d\epsilon=\curlybraces{\begin{matrix}
    z=\beta\epsilon\\
    dz=\beta d\epsilon
\end{matrix}}=g\frac{4\pi V}{h^3\beta^{5/2}}\left(2m^3\right)^{1/2}\int_0^{\infty}\frac{z^{3/2}}{e^{z}-1}dz\]

Vemos que este tipo de integrales no son analíticas, y se debe recurrir a la Gamma de Euler y a la Zeta de Riemann, tal que

\[\int_0^{\infty}\frac{z^{x-1}}{e^z-1}dz=\Gamma(x)\zeta(x)\hspace{5mm}x\geq0\]

luego, como tenemos $x-1=\frac{3}{2}$, entonces $x=\frac{5}{2}$, sabiendo que $\Gamma\left(\frac{5}{2}\right)=\frac{3\sqrt{\pi}}{4}$\footnote{Usando la propiedad $\Gamma(a+1)=a\Gamma(a)$}, tenemos

\[\overline{E}=g\frac{4\pi V}{h^3\beta^{5/2}}\left(2m^3\right)^{1/2}\frac{3\sqrt{\pi}}{4}\zeta\left(\frac{5}{2}\right)=g\frac{3V}{\beta^{5/2}}\frac{\left(2m^3\right)\pi^{3/2}}{h^3}\zeta\left(\frac{5}{2}\right)\qedh\]



\chapter{TEMA 6: Estudio estadístico de la radiación y el
magnetismo}

\section{Radiación electromagnética y fotones}

La radiación en clásica se interpreta como un campo/onda, pero en cuántica, como existe la dualidad onda-partícula, tendremos que la partícula de la radiación será el fotón.\\ \\
Definimos la densidad espectral de energía como la energía del espectro electromagnético en
equilibrio con el recinto que la contiene a la temperatura $T$, con frecuencia
comprendida entre $\omega$ y $\omega+d\omega$, denotada por $E(\omega)d\omega$. 
\\ \\
Si usamos la expresión de Rayleigh-Jeans, la cuál trata la radiación como ondas, tenemos
\begin{equation}
    E(\omega)d\omega=V\frac{kT}{\pi^2c^3}\omega^2
\end{equation}
que representándola y comparando con los datos experimentales,
\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{ff.png}
    \captionof{figure}{Representación de la función de Rayleigh-Jeans junto a los datos experimentales}
    \label{fig:54-algos}
\end{Figura}
donde vemos que no coincide con los datos experimentales. Esto es lo que se conoce como \textit{catástrofe del ultravioleta}.
\subsection{Interpretación de la radiación electromagnética}
Podemos interpretar la radiación como una imagen ondulatoria, siendo ondas planas, y como una imagen corpuscular, siendo los fotones. Tal que
\[\begin{matrix}
    \text{Imagen ondulatoria} & & & & \text{Imagen corpuscular}\\ \\
    \vec{k} & \leftarrow & \vec{p}=\hbar\vec{k} & \rightarrow & \vec{p}\\
    \omega & \leftarrow & E=\hbar\omega & \rightarrow & E
\end{matrix}\]
Ahora, supondremos que la radiación será un conjunto de partículas (fotones), caracterizados por su momento $\vec{p}$ y su energía $E$.
\\ \\
\begin{itemize}
    \item Como las ondas planas son indistinguibles, los fotones deberán ser indistinguibles también, únicamente podrían diferenciarse por el estado en el que se encuentran (frecuencia), al igual que ocurre con las ondas. 
    \item Como puede haber dos estados de polarización de las ondas planas, la degeneración de los fotones debe ser 2. \item La intensidad de la radiación es proporcional al número de fotones, pues $I(\omega)\propto\abs{E(\omega)}^2$. Como no hay un límite superior para $I(\omega)$, podremos tener una intensidad infinita, luego, podremos tener infinitos fotones con la misma frecuencia, implicando que los fotones deberán ser \textbf{bosones}.
    \item Al tener una degeneración de 2, los fotones deberán tener espín 1/2, pero esto corresponde a los \textbf{fermiones} y hemos visto que los fotones son \textbf{bosones}, así, tendremos que asignarles espín 1.
    \item Como las ondas plana no interaccionan entre sí, los fotones tampoco lo harán, así, un conjunto de fotones constituye un \textbf{gas ideal real}.
    \item Como las ondas planas interaccionan con el recinto que las contiene, entonces el número de fotones cambia, luego usaremos el \textbf{colectivo canónico generalizado}, asignando n potencial químico nulo, $\mu=0$
    \item Un gas de fotones constituye un gas ideal cuántico de Bose-Einstein con
potencial químico nulo.
\end{itemize}
\subsection{Densidad de estados}
\begin{itemize}
    \item Número de ondas con vector de onda comprendido entre $\vec{k}$ y $\vec{k}+d\vec{k}$,
    \begin{equation}\rho(\vec{k})d\vec{k}=\frac{V}{(2\pi)^3}d\vec{k}\end{equation}
    \item Número de ondas con vector de onda de módulo comprendido entre $k$ y $k+dk$,
    \begin{equation}\rho(k)dk=\frac{4\pi V}{(2\pi)^3}k^2dk=\frac{V}{2\pi^2}k^2dk\end{equation}
    \item Número de ondas con frecuencia comprendida entre $\omega$ y $\omega+d\omega$,
    \begin{equation}\rho(\omega)d\omega=\frac{V}{\pi^2c^3}\omega^2d\omega\end{equation}
\end{itemize}
sabiendo que
\begin{equation}
    k=\frac{2\pi}{\lambda}=2\pi\frac{\omega}{c}\hspace{15mm}\omega=k\cdot c
\end{equation}
\section{Distribución de Planck}
Definimos la densidad espectral de energía como la energía del espectro electromagnético en
equilibrio con el recinto que la contiene a la temperatura $T$, con frecuencia
comprendida entre $\omega$ y $\omega+d\omega$, siendo la densidad espectral de energía, $dE(\omega)=E(\omega)d\omega$, Tal que
\[\begin{matrix}
    dE(\omega)\equiv&\text{Energía de un fotón}&\times&\begin{matrix}\text{Número de fotones con
frecuencia}\\
\text{comprendida
entre }\omega\text{ y }\omega+d\omega\end{matrix}&\equiv \hbar\omega\times dN(\omega)\\ \\
dN(\omega)\equiv&\begin{matrix}\text{Número de fotones en el}\\
\text{estado con frecuencia }\omega\end{matrix}&\times&\begin{matrix}\text{Número de fotones con
frecuencia}\\
\text{comprendida
entre }\omega\text{ y }\omega+d\omega\end{matrix}&\equiv \overline{n}(\omega)\times\rho(\omega)d\omega
\end{matrix}\]

siendo $\overline{n}(\omega)$ la distribución de Bose-Einstein para $\mu=0$. Luego,
\begin{equation}
    E(\omega)d\omega=\hbar\omega\overline{n}(\omega)\rho(\omega)d\omega=\frac{V\hbar}{\pi^2c^3}\frac{\omega^3}{e^{\beta\hbar\omega}-1}d\omega
\end{equation}
siendo esta la distribución de Planck para la
radiación electromagnética
del cuerpo negro, que sí la representamos, vemos que sí coincide con los datos experimentales.

\begin{Figura}
    \centering
    \includegraphics[width=0.5\linewidth]{hh.png}
    \captionof{figure}{Representación de la función de Planck junto a los datos experimentales}
    \label{fig:61-algos}
\end{Figura}
Vamos a estudiar los distintos límites de esta distribución,

\begin{itemize}
    \item \textbf{Límite de bajas frecuencias}\\ \\
    Tenemos que $\frac{\hbar\omega}{kT}\ll1$, con
    \[E(\omega)d\omega=\frac{V\hbar}{\pi^2c^3}\frac{\omega^3}{e^{\beta\hbar\omega}-1}d\omega=\frac{V\hbar}{\pi^2c^3}\frac{\omega^3}{e^{\frac{\hbar\omega}{kT}}-1}d\omega\]
    Hacemos Taylor en la exponencial, tal que
    \[e^{\frac{\hbar\omega}{kT}}\approx 1+\frac{\hbar\omega}{kT}+\dots\]
    Luego,
    \[E(\omega)=\frac{V\hbar}{\pi^2c^3}\frac{\omega^3}{1+\frac{\hbar\omega}{kT}-1}=\frac{VkT}{\pi^2c^3}\omega^2\]
    siendo la ecuación de Rayleigh-jeans
    \item \textbf{Límite de altas frecuencias}\\ \\
    Tenemos que $\frac{\hbar\omega}{kT}\gg1$, entonces podremos aproximar $e^{\frac{\hbar\omega}{kT}}-1\approx e^{\frac{\hbar\omega}{kT}}$. Así,
    \[\overline{E}=\frac{V\hbar}{\pi^2c^3}\omega^3e^{-\frac{\hbar\omega}{kT}}\]
    que será el límite clásico, pues si lo hacemos desde un punto de vista clásico, nos sale esta misma expresión, que es la ecuación de Wien. 
\end{itemize}
Como tenemos que para bajas frecuencias sube la curve, y para altas, baja, entonces debe haber un máximo de la curva, que lo calculamos derivando $E(\omega)$, pero para que esta derivada resulte más sencilla, definimos
\[\eta=\frac{\hbar\omega}{kT}\Rightarrow\omega=\frac{kT}{\hbar}\eta\]
tal que
\[E(\eta)=\frac{H\hbar}{\pi^2c^3}\frac{\left(\frac{kT}{\hbar}\eta\right)^3}{e^{\eta}-1}=\frac{V\hbar}{\pi^2c^3}\left(\frac{kt}{\hbar}\right)^3\frac{\eta^3}{e^{\eta}-1}\]
tomando
\[A=\frac{V\hbar}{\pi^2c^3}\left(\frac{kt}{\hbar}\right)^3\]
\[E(\eta)=A\frac{\eta^3}{e^{\eta}-1}\]
Derivamos respecto $\eta$ e igualamos a cero,
\[\frac{dE(\eta)}{d\eta}=\cancel{A}\frac{\left(e^{\eta}-1\right)3\cancel{\eta^2}-\eta^{\cancel{3}}\left(e^{\eta}\right)}{\cancel{\left(e^{\eta}-1\right)^2}}=0\]
Tendremos,
\[\eta^2\brackets{3\left(e^{\eta}-1\right)-\eta e^{\eta}}=0\]
Tendremos varias soluciones,
\[\begin{matrix}
    \eta=0 & \text{No sirve}
\end{matrix}\]
\[3\left(e^{\eta}-1\right)-\eta e^{\eta}=0\]
\[3e^{\eta}-3-\eta e^{\eta}=0\]
\[(3.\eta)e^{\eta}-3=0\]
\[e^{\eta}=\frac{3}{3-\eta}\]
\[e^{-\eta}=\frac{3-\eta}{3}=-\frac{\eta}{3}+1\]
es una ecuación trascendental, que se puede resolver o bien numéricamente, o bien graficando y viendo donde corta.
\\ \\
Gráficamente,

\begin{Figura}
    \centering
    \includegraphics[width=0.9\linewidth]{image.png}
    \captionof{figure}{Representación de $e^{-\eta}$ y de $1-\frac{\eta}{3}$}
    \label{fig:63-algos}
\end{Figura}

vemos que el punto de corte es aproximadamente $\eta\approx2.82$, siendo esta la otra solución.
\\ \\
Numéricamente, usamos el método de Newton,
\[X_{n+1}=X_n-\frac{f(X_n)}{f'(X_n)}\]
empezando en $X_n=2$, tenemos $x_{n+1}=2.8244$, que será la solución aproximada.\\ \\
Luego, el punto máximo de la gráfica será,
\[\eta_{max}=\frac{\hbar\omega_{max}}{kT}=\frac{\frac{h}{2\pi}2\pi\nu}{kT}=\frac{h}{k}\frac{\nu}{T}=\frac{hc}{k}\frac{1}{\lambda T}\approx2.824\]
tal que
\begin{Figura}
    \centering
    \includegraphics[width=1\textwidth]{h.png}
    \captionof{figure}{Representación de $E(\eta)$}
    \label{fig:64-algos}
\end{Figura}

El valor máximo de la distribución es constante y depende de la temperatura y de la longitud de onda. Entonces se conservarán,
\begin{equation}
    \lambda_1T_1=\lambda_2T_2
\end{equation}
denominada \textbf{Ley de Wien} (cuerpo negro).
\\ \\
\section{Propiedades termodinámicas del gas de fotones}
\subsection{Cálculo de la función de partición $Q$}
El colectivo canónico generalizado presenta una función de partición $Q$ con $\mu=0$, tal que
\begin{equation}
    \ln Q=-\sum_r\ln(1-e^{-\beta\hbar\omega})
\end{equation}
Para nuestro sistema, pasamos el sumatorio a una integral y deberemos añadir un término de degeneración $\rho(\omega)$, tal que
\begin{equation}
    \ln Q_F=-\int_0^{\infty}d\omega\rho(\omega)\ln(1-e^{-\beta\hbar\omega})
\end{equation}
Usando que $\rho(\omega)=\frac{V}{\pi^2c^3}\omega^2$,
\[\ln Q_F=-\int_0^{\infty}d\omega\frac{V}{\pi^2c^3}\omega^2\ln(1-e^{-\beta\hbar\omega})\]
Usando partes,
\[\curlybraces{\begin{matrix}
    u=\ln(1-e^{-\beta\hbar\omega}) & du=\frac{\beta\hbar e^{-\beta\hbar\omega}}{1-e^{-\beta\hbar\omega}}\\
    dv=\omega^2d\omega & v=\frac{\omega^3}{3}
\end{matrix}}\]
Así,
\[\ln Q_F=\left.\cancelto{0}{\frac{\omega^3}{3}\ln(1-e^{-\beta\hbar\omega})}\right|_0^{\infty}-\int_0^{\infty}\frac{\omega^3\ln(1-e^{-\beta\hbar\omega})}{3\left(1-e^{-\beta\hbar\omega}\right)}d\omega\]

donde anulamos el primer término aplicando L'Hôpital para el cero, para cuando sustituimos el infinito sí es cero porque el logaritmo le gana a $\omega^3$. Vemos cómo aplicar L'Hôpital,
\[\lim_{\omega\rightarrow0}\frac{\omega^3}{3}\ln(1-e^{-\beta\hbar\omega})=0\cdot\infty\]
Aplicamos la fórmula $\ln(\alpha/\beta)=ºln(\alpha)-\ln(\beta)$,
\[\ln(1-\frac{1}{e^{\beta\hbar\omega}})=\ln(\frac{e^{\beta\hbar\omega}-1}{e^{\beta\hbar\omega}})=\ln(e^{\beta\hbar\omega-1})-\ln{e^{\beta\hbar\omega}}=\ln(e^{\beta\hbar\omega-1})-\beta\hbar\omega\]
\[\frac{1}{3}\lim_{\omega\rightarrow0}\omega^3\brackets{\ln(e^{\beta\hbar\omega-1})-\beta\hbar\omega}\]
Hacemos u cambio de variable,
\[\begin{matrix}
    t=\frac{1}{x}\\
    x\rightarrow0\\
    \Downarrow\\
    t\rightarrow\frac{1}{0}=\infty
\end{matrix}\]
\[\frac{1}{3}\lim_{t\rightarrow\infty}\frac{\ln(e^{\frac{\beta\hbar}{t}}-1)-\beta\hbar\frac{1}{t}}{t^3}=\frac{\infty}{\infty}\]
\\ \\
Luego, podemos aplicar L'Hôpital. Calculamos las derivadas del numerador y el denominador,

\[\frac{d t^3}{dt}=3t^2\]
\[\frac{d}{dt}\brackets{\ln(e^{\frac{\beta\hbar}{t}}-1)-\beta\hbar\frac{1}{t}}=\frac{\beta\hbar}{t^2}-\frac{\beta\hbar e^{\frac{\beta\hbar}{t}}}{t^2\left(e^{\frac{\beta\hbar}{t}-1}\right)}\]
Luego,
\[\dfrac{1}{3}\,\lim_{t\to{\infty}}{\dfrac{\dfrac{\beta\hbar}{t^{2}}-\dfrac{\beta\hbar\,e^{\frac{\beta\hbar}{t}}}{t^{2}\,\left(e^{\frac{\beta\hbar}{t}}-1\right)}}{3\,t^{2}}}\]
Agrupando,
\[\dfrac{1}{9}\,\lim_{t\to{\infty}}{\dfrac{-\dfrac{\beta\hbar\,\dfrac{e^{\frac{\beta\hbar}{t}}-1}{\dfrac{\beta\hbar}{t}}}{t}-1}{t^{3}\,\dfrac{e^{\frac{\beta\hbar}{t}}-1}{\dfrac{\beta\hbar}{t}}}+\dfrac{\beta\hbar}{t^{4}}}\]
Aplicamos la fórmula,

\[\lim_{t\to{0}}{\dfrac{a^{t}-1}{t}}=\ln\left(a\right)\]

Luego,

\[\dfrac{1}{9}\,\lim_{t\to{\infty}}{-\dfrac{1}{t^{3}}}=0\qedh\]


Volvemos a la integral,

\[\ln Q_F=\frac{V}{\pi^2c^3}\int_0^{\infty}\frac{\omega^3e^{-\beta\hbar\omega}\beta\hbar}{3\left(1-e^{-\beta\hbar\omega}\right)}d\omega=\curlybraces{\begin{matrix}
    z=\beta\hbar\omega\\
    dz=\beta\hbar d\omega
\end{matrix}}=\frac{V}{3\pi^2c^3(\beta\hbar)^3}\int_0^{\infty}\frac{z^3e^{-z}}{1-e^{-z}}dz=\frac{V}{3\pi^2c^3(\beta\hbar)^3}\int_0^{\infty}\frac{z^3}{e^{z}-1}dz\]

Vemos que este tipo de integrales no son analíticas, y se debe recurrir a la Gamma de Euler y a la Zeta de Riemann, tal que

\[\int_0^{\infty}\frac{z^{x-1}}{e^z-1}dz=\Gamma(x)\zeta(x)\hspace{5mm}x\geq0\]

luego, como tenemos $x-1=3$, entonces $x=4$, sabiendo que $\Gamma\left(4\right)=3!$\footnote{Usando la propiedad $\Gamma(a)=(a-1)!,a\in\mathbb{N}$} y además, como la Zeta de Riemann está tabulada, $\zeta(4)=\frac{\pi^4}{90}$, así tenemos

\begin{equation}
\ln Q_F=\frac{V}{3\pi^2c^3(\beta\hbar)^3}\frac{\pi^4}{90}3!=\frac{V\pi^2}{45(\hbar c)^3}(kT)^3
\end{equation}

Como siempre, tras calcular la función de partición, podemos calcular
\begin{equation}
    \overline{E}=\frac{4\sigma}{c}VT^4\hspace{10mm}\overline{P}V=\frac{4\sigma}{3c}VT^4=\frac{\overline{E}}{3}\hspace{10mm}\overline{N}=\frac{V}{\pi^2(\hbar c)^3}\Gamma(3)\zeta(3)(kT)^{3}
\end{equation}
siendo $\sigma$ la constante de Stefan-Boltzmann, que es
\begin{equation}
    \sigma=\frac{k^4\pi^2}{60\hbar^3c^2}
\end{equation}
\subsubsection*{Energía media}
\[\overline{E}=-\left.\frac{\partial\ln Q_F}{\partial\beta}\right|_V=\frac{3V\pi^2}{45(\hbar c)^3}(kT)^4=\frac{V\pi^2}{15(\hbar c)^3}(kT)^4=\cancelto{\sigma}{\frac{k^4\pi^2}{60\hbar^3c^2}}\frac{4VT^4}{c}=\frac{4\sigma}{c}VT^4\qedh\]
\subsubsection*{Función de estado}
\[\overline{P}V=kT\ln Q_F=\frac{V\pi^2}{45(\hbar c)^3}(kT)^4=\cancelto{\sigma}{\frac{k^4\pi^2}{60\hbar^3c^2}}\frac{4V4}{3c}=\frac{4\sigma}{3c}VT^4=\frac{\overline{E}}{3}\qedh\]
\subsubsection*{Número medio de partículas}

\[\overline{N}=\int_0^{\infty}d\omega\overline{n}(\omega)\rho(\omega)=\int_0^{\infty}d\omega\frac{V\omega^2}{\pi^2 c^3}\frac{1}{e^{\beta\hbar\omega}-1}=\frac{V}{\pi^2(\hbar c)^3}\Gamma(3)\zeta(3)(kT)^{3}\qedh\]
Donde hemos usado la propiedad, 
\[\int_0^{\infty}\frac{z^{x-1}}{e^{z}-1}dz=\Gamma(x)\zeta(x)\]
\subsubsection*{Nota}
Vemos que $\overline{N}\propto T^3$, por tanto
\[\begin{matrix}
    T\uparrow & \rightarrow & \overline{N}\uparrow & \text{Emisión}\\
    T\downarrow & \rightarrow & \overline{N}\downarrow & \text{Absorción}
\end{matrix}\]
Pero cuando $T\rightarrow0$, entonces $\overline{N}\rightarrow0$, por tanto, al no haber partículas, no se produce la condensación de Bose-Einstein.
\\ \\
\section{Estudio estadístico el magnetismo}
\subsection{Teoría clásica}
Estudiaremos el de un gas ideal paramagnético, considerándolo como un conjunto de dipolos magnéticos iguales que no interaccionan entre sí y están en un campo magnético.\\ \\
Designando $\vec{\mu}=\vec{m}$, momento dipolar, tenemos que la energía magnética será
\begin{equation}
    E_m=-\mu_0\vec{\mu}\cdot\vec{H}=-\mu_0\mu H\cos\theta
\end{equation}
Haciendo un caso en el espacio y designando un ángulo sólido $d\Omega$, podemos decir que la probabilidad de que la orientación del dipolo respecto del campo magnético está comprendida dentro de un elemento de ángulo sólido $d\omega=\sin\theta d\theta d\phi$ alrededor de las dirección $(\theta,\phi)$. Luego, la definimos como
\begin{equation}
    P(\theta,\phi)d\Omega\propto e^{-\beta E_m}d\Omega
\end{equation}
Definimos el vector imanación medio,
\begin{equation}
    \overline{M}_z=\frac{N}{V}\overline{\mu}_z
\end{equation}
Tal que $\overline{\mu}_z=\mu\overline{\cos\theta}$, 
\[\overline{M}_z=\frac{N}{V}\mu\overline{\cos\theta}=\frac{N}{V}\mu\frac{\int d\Omega\cos\theta e^{-\beta E_m}}{\int d\omega e^{-\beta E_m}}\overset{\curlybraces\alpha=\frac{\mu_0\mu H}{kT}}{=}\frac{N}{V}\mu\brackets{\cot\alpha-\frac{1}{\alpha}}=\frac{N}{V}\mu L(\alpha)\]
siendo $L(\alpha)$ la función de Lagevin.

\[\begin{matrix}
    \text{si} & \alpha\gg1 & \Rightarrow & \overline{M}_z=\frac{N}{V}\mu \\
    \text{si} & \alpha\ll1 & \overline{M}_z=\frac{N}{V}\frac{\mu\alpha}{3}=\frac{n\mu_0\mu^2}{3k}\frac{H}{T}
\end{matrix}\]
siendo esta última la Ley de Curie.
\subsection{Teoría cuántica}
Sea un gas ideal paramagnético, cada partícula tendrá asociada un momento angular total y un momento magnético asociado, tales
\begin{equation}
    \vec{J}=\vec{L}+\vec{S}\hspace{7mm}\left\lbrace\begin{matrix}
        \hat{J^2} & \hbar^2j(j+1) &\\
        \hat{J}_z & m\hbar & m=-j,-j+1,\dots,0,\dots,j-1,j
    \end{matrix}\right.
\end{equation}

siendo este $\vec{J}$ el momento angular total, $\vec{L}$ el momento lineal y $\vec{S}$ el espín. El momento magnético propio viene dado por,

\begin{equation}
    \vec{\mu}=g\frac{e}{2m_e}\vec{J}
\end{equation}

siendo $g$ el factor de Landé, que viene dado por
\begin{equation}
    g=\frac{3}{2}+\frac{s(s+1)-l(l+1)}{2j(j+1)}
\end{equation}

Estudiaremos la estadística de Maxwell-Boltzmann, considerando un gas ideal cuántico en el límite clásico.\\ \\
Sabemos que la función de partición es,
\begin{equation}
    Z=\frac{\xi^N}{N!}\hspace{10mm}\xi=\sum_re^{-\beta\epsilon_r}\hspace{5mm}\epsilon_r=\epsilon_{njm}=\epsilon_n+\epsilon_j+\epsilon_m
\end{equation}
siendo $\epsilon_n$ el referido al movimiento de traslación, rotación y vibración, $\epsilon_j$ es referido al movimiento del orbital electrónico y depende del número cuántico $j$ y $\epsilon_m$ es referido al movimiento de magnética, debido a la presencia de una campo magnético externo; depende del número cuántico $m$. 
\\ \\
Por tanto,
\[\xi=\sum_re^{-\beta\epsilon_r}=\sum_{n,j,m}e^{-\beta(\epsilon_n+\epsilon_j+\epsilon_m)}=\sum_ne^{-\beta\epsilon_n}\sum_je^{-\beta\epsilon_j}\sum_me^{-\beta\epsilon_m}\]
Para temperaturas ordinarias (ambiente), solamente contribuirá el valor del $j$ correspondiente al estado fundamental, tal que
\[\xi=\sum_ne^{-\beta\epsilon_n}e^{-\beta\epsilon_j}\sum_me^{-\beta\epsilon_j}=\xi_{nm}\xi_{m}
\]
Entonces,
\[Z=Z_{nm}Z_m=\frac{\xi_{nm}^N}{N!}\xi_m^N\]
siendo $\xi_m^N$ la función de partición magnética de un conjunto
de momentos magnéticos (espines) de valor $j$
que no interaccionan entre si pero sí lo hacen
con un campo magnético externo.
\\ \\
Sabiendo que la energía de interacción es $U=-\vec{\mu}\cdot\vec{B}=-\mu_0\vec{mu}\cdot\vec{H}=\epsilon_m$, luego,
\[\epsilon_m=-\mu_0\vec{\mu}\cdot\vec{H}=-\mu_0g\frac{e}{2m_e}\vec{J}\cdot\vec{H}\]
Vamos a tomar el sistema de referencia tal que el eje z tenga la dirección de $\vec{H}$, así
\[\epsilon_m=-\mu_0g\frac{e}{2m_e}J_zH\]
siendo $J_z=m\hbar$.
\[\epsilon_m=-\mu_0g\frac{e\hbar}{2m_e}mH=-g\mu_0\mu_BmH\]
siendo $\mu_B=\frac{e\hbar}{2m_e}$, que es el magnetón de Bohr, cuyas unidades son las mismas que la de momento magnético.\\ \\
El magnetón de Bohr es el cuanto del momento magnético (sería el equivalente a la carga electrónica del electrón para electrostática). Luego,

\[\xi_m=\sum_me^{-\beta\epsilon_m}=\sum_{m=-j}^{m=+j}e^{\beta g\mu_0\mu_BmH}=\sum_{m=-j}^{m=+j}e^{mx}\]
con $x=\frac{g\mu_0\mu_B H}{kT}$, que es la razón entre la
energía
magnética y la
energía
térmica.
\\ \\
Este sumatorio debe converger, pues la suma es finita. Luego,
\[\xi_m=\sum_{m=-j}^{m=+j}e^{mx}=e^{-jx}+e^{(-j+1)x}+\dots+1+e^{x}+\dots+e^{jx}\]
Tenemos la suma de los términos de una progresión geométrica de razón $e^x$, entonces la suma total será,
\[\xi_m=\frac{e^{-jx}-e^{jx}e^x}{1-e^x}=\frac{e^{-jx}-e^{(j+1)x}}{e^{\frac{x}{2}}\left(e^{-\frac{x}{2}}-e^{\frac{x}{2}}\right)}=\frac{\left(e^{-jx}-e^{(j+1)x}\right)e^{-\frac{x}{2}}}{2\sinh(\frac{x}{2})}=\frac{e^{-\left(j+\frac{1}{2}\right)x}-e^{\left(j+\frac{1}{2}\right)x}}{2\sinh(\frac{x}{2})}\]
Luego, la función de partición magnética es
\begin{equation}
    \xi_m=\frac{\sinh(\brackets{j+\frac{1}{2}}x)}{2\sinh(\frac{x}{2})}
\end{equation}
\subsection{Cálculo de la imanación}
Una vez tenemos la función de partición, calculamos el valor medio del momento magnético,
\[\overline{\mu}_z=\frac{\cancel{\sum_ne^{-\beta\epsilon_n}}\cancel{e^{-\beta\epsilon_j}}\sum_{m=-j}^{m=+j}\mu_ze^{-\beta\epsilon_m}}{\cancel{\sum_ne^{-\beta\epsilon_n}}\cancel{e^{-\beta\epsilon_j}}\sum_me^{e^{-\beta\epsilon_m}}}=\frac{\sum_{m=-j}^{m=+j}\mu_ze^{-\beta\epsilon_m}}{\sum_me^{e^{-\beta\epsilon_m}}}=\frac{\sum_{m=-j}^{m=+j}g\mu_Bme^{-\beta g\mu_0\mu_BmH}}{\sum_me^{e^{-\beta\epsilon_m}}}\]
Usando que $\frac{\partial e^{ax}}{\partial x}=ae^x$, podemos hacer
\[\overline{\mu}_z=\frac{\frac{1}{\beta\mu_0}\frac{\partial}{\partial H}\left(\sum_{m=-j}^{m=+j}e^{\beta g\mu_0\mu_BmH}\right)}{\sum_me^{-\beta\epsilon_m}}=\frac{\frac{1}{\beta\mu_0}\frac{\partial}{\partial H}\left(\sum_{m=-j}^{m=+j}e^{-\beta \epsilon_m}\right)}{\sum_me^{-\beta\epsilon_m}}\]
Usando que $\frac{\partial\ln(f(x))}{\partial x}=\frac{1}{x}\frac{\partial f(x)}{\partial x}$, tenemos
\[\overline{\mu}_z=\frac{1}{\beta\mu_0}\left.\frac{\partial\ln(\sum_{m=-j}^{m=+j}e^{\beta g\mu_0\mu_BmH})}{\partial H}\right|_{\beta}=\frac{1}{\beta\mu_0}\left.\frac{\partial\ln\xi_m}{\partial H}\right|_{\beta}\]
Entonces la imanación, que es el momento magnético por unidad de volumen será,

\[\overline{M}=\frac{N}{V}\overline{\mu}_z=\frac{N}{V}\frac{1}{\beta\mu_0}\left.\frac{\partial\ln\xi_m}{\partial H}\right|_{\beta}=\frac{1}{V}\frac{1}{\beta\mu_0}\left.\frac{\partial N\ln\xi_m}{\partial H}\right|_{\beta}=\frac{1}{V\beta\mu_0}\left.\frac{\partial\ln\xi^N_m}{\partial H}\right|_{\beta}=\frac{1}{V\beta\mu_0}\left.\frac{\partial\ln Z_m}{\partial H}\right|_{\beta}\]

Podemos reemplazar la $Z_m$ por la $Z$, pues como $Z=Z_{mn}Z_m$, entonces $\ln Z=\ln(Z_{nm}Z_m)=\ln Z_{nm}+\ln Z_m$ y también $\frac{\partial\ln Z}{\partial H}=\cancelto{0}{\frac{\partial\ln Z_{nm}}{\partial H}}+\frac{\partial\ln Z_m}{\partial H}$, y por tanto, la imanación media será
\begin{equation}
    \overline{M}=\frac{1}{V\beta\mu_0}\left.\frac{\partial\ln Z}{\partial H}\right|_{\beta}
\end{equation}

La ecuación de estado asociada al parámetro externo $H$, será la misma, pues tiene la misma estructura que las ecuaciones de estado anteriores.
\\ \\
Calculamos,

\[\overline{\mu}_z=\frac{1}{\beta\mu_0}\left.\frac{\partial\ln\xi_m}{\partial H}\right|_{\beta}=\frac{1}{\beta\mu_0}\left.\frac{\partial\ln\xi_m}{\partial x}\frac{\partial x}{\partial H}\right|_{\beta}=\]\[=g\mu_B\left.\frac{\partial\ln\xi_m}{\partial x}\right|_{\beta}=g\mu_B\curlybraces{\brackets{j+\frac{1}{2}}\cot(\brackets{j+\frac{1}{2}}x)-\frac{1}{2}\cot(\frac{x}{2})}=g\mu_BjB_j(x)\]
siendo $B_j(x)$ la función de  Brillouin de orden $j$.
Por tanto,
\begin{equation}
    \overline{M}=\frac{N}{V}g\mu_BjB_j(x)
\end{equation}
Representamos,
\begin{Figura}
    \centering
    \includegraphics[width=1\textwidth]{bb.png}
    %\caption{Enter Caption}
    \label{fig:65-algos}
\end{Figura}
Al estudiar la imanación tenemos dos casos,
\begin{itemize}
    \item $\frac{g\mu_0\mu_BH}{kT}\gg1$, entonces $\overline{M}\rightarrow\frac{N}{V}g\mu_Bj$, es decir, todas las partículas se orientan en la misma dirección del campo.
    \item $\frac{g\mu_0\mu_BH}{kT}\ll1$, entonces $B_j(x)\approx \frac{x}{3}(j+1)$ y por tanto 
    \[\overline{M}\approx\frac{N}{V}g\mu_Bj(j+1)\frac{x}{3}=\frac{N}{V}\frac{g\mu_Bj(j+1)}{3}\frac{g\mu_0\mu_BH}{kT}=\frac{N}{3V}\frac{g^2\mu_0\mu_B^2j(j+1)}{k}\frac{H}{T}\]
    obteniendo así la Ley de Curie de clásica.
\end{itemize}

\subsection{Cálculo de la energía media}
Vamos a comprobar que la energía media es,
\begin{equation}
    \overline{E}_m=-\mu_0V\vec{M}\cdot\vec{H}
\end{equation}
Usamos que,
\[\overline{E}_m=N\overline{\epsilon}_m=N\frac{e^{-\beta\epsilon_j\sum_n\sum_{m=-j}^{m=+j}\epsilon_me^{-\beta\epsilon_n-\beta\epsilon_m}}}{e^{-\beta\epsilon_j}\sum_n\sum_{m=-j}^{m=+j}e^{-\beta\epsilon_n-\beta\epsilon_m}}=N\frac{\sum_{m=-j}^{m=+j}(-g\mu_0\mu_BmH)e^{\beta g\mu_0\mu_BmH}}{\sum_{m=-j}^{m=+j}e^{\beta\mu_0\mu_BmH}}\]

Usando que $\frac{\partial e^{ax}}{\partial x}=ae^x$, podemos hacer

\[\overline{E}_m=-N\left.\frac{\partial}{\partial\beta}\right.\left(\sum_{m=-j}^{m=+j}e^{\beta g\mu_0\mu_B mH}\right)_H\frac{1}{\sum_{m=-j}^{m=+j}e^{\beta g\mu_0\mu_BmH}}\]

Usando que $\frac{\partial\ln(f(x))}{\partial x}=\frac{1}{x}\frac{\partial f(x)}{\partial x}$, tenemos

\[\overline{E}_m=-N\left.\frac{\partial\ln(\xi_m)}{\partial\beta}\right|_H=-N\frac{\partial\ln\xi_m}{\partial x}\left.\frac{\partial x}{\partial\beta}\right|_H=-NJB_j(x)\left.\frac{\partial x}{\partial \beta}\right|_H=-Ng\mu_0\mu_BH
jB_j(x)\]
Sabiendo que $M=\frac{N}{V}g\mu_BjB_j(x)$, entonces
\[\overline{E}_m=-\mu_0V\vec{H}\cdot\vec{M}\qedh\]

\section{Ferromagnetismo: Campo molecular de Weiss}

El Ferromagnetismo no tiene explicación clásica, solo cuántica.\\ \\
En la paramagnética hemos llegado a que si o existe un campo externo, entonces no hay imanación, pero existen materiales que sin haber un campo magnético externo tienen imanación, estos son los materiales ferromagnéticos.\\ \\
Sea un conjunto de espines magnéticos que interaccionan entre sí y que se encuentran en el seno de un campo magnético. En el Hamiltoniano ($\mathcal{H}$) hay dos contribuciones (interacciones), la del campo magnético externo y la de interacción de las partículas, tal que, el Hamiltoniano de interacción con el campo magnético externo es
\[\mathcal{H}=-\sum_{j=1}^N\vec{\mu}\cdot\vec{B}_{ext}=-\mu_0\sum_{j=1}^N\vec{mu}\cdot\vec{H}_{ext}=-g\mu_0\frac{e}{2m_e}\sum_{j=1}^N\vec{S}_j\cdot\vec{H}_{ext}\]
tomando el sistema de referencia de forma que el eje x coincida con la dirección de $\vec{H}_{ext}$, tenemos 
\[\mathcal{H}=-g\mu_0\frac{eH_{ext}}{2m_e}\sum_{j=1}^NS_{zj}\]
Por otro lado, el Hamiltoniano de interacción entre dos partículas será,
\[\mathcal{H}=-2J\vec{S}_j\cdot\vec{S}_k\]
siendo $J=J(r_{jk})$.\\ \\
Por tanto, el Hamiltoniano completo será,

\[\mathcal{H}=-g\mu_0\frac{eH_{ext}}{2m_e}\sum_{j=1}^NS_{zj}+\frac{1}{2}\brackets{-2\sum_{j=1}^N\sum_{k=1}^N\vec{S}_j\cdot\vec{S}_kJ}\]
donde multiplicamos por 1/2 el segundo sumando porque estamos sumando dos veces lo mismo, pues $jk=kj$.\\ \\
A continuación, llevaremos a cabo aproximaciones (denominadas modelo de Ising),
\begin{itemize}
    \item \textbf{Aproximación de vecinos próximos}: diremos que los momentos que alteran a mi partícula serán los más cercanos a ella, despreciando los demás, así
    \[\mathcal{H}=-g\mu_0\frac{eH_{ext}}{2m_e}\sum{j=1}^NS_{zj}-2\sum_{j=1}^N\sum_{k=1}^{N_1}J\vec{S}_k\cdot\vec{S}_j\]
    \item Decimos que la contribución de los demás ejes es mucho menor que las proyecciones sobre el eje z, por esto
     \[\vec{S}_j\cdot\vec{S}_k=S_{zj}S_{zk}\]
     y así,
     \[\mathcal{H}=\sum_{j=1}^N\brackets{-g\mu_0\frac{eH_{ext}}{2m_e}\delta_{zj}-2\sum{k=1}^{N_1}JS_{zj}S_{zk}}=\sum_{j=1}^N\brackets{-g\frac{\mu_0\mu_B}{\hbar}\delta_{zj}\left(H_{ext}+2J\sum_{k=1}^{N_1}S_{zk}\frac{\hbar}{g\mu_0\mu_B}\right)}\]
     Como vemos, tendremos la contribución del campo magnético externo y el campo producido por otros epines.\\ \\
     Las expresiones obtenidas en paramagnetismo son válidas cambiando el
campo externo por la suma del campo externo más el campo medio
     \item \textbf{Aproximación del campo medio}: despejando el campo,
     \[H_{medio}=\overline{2J\sum_{k=1}^{N_1}S_{zk}\frac{\hbar}{g\mu_0\mu_B}}=2J\frac{\hbar}{g\mu_0\mu_B}\overline{\sum_{k=1}^{N_1}S_{zk}}\approx2J\frac{\hbar}{g\mu_0\mu_B}N_1\overline{S}_z\]
     Calculamos $\overline{S}_z$ a partir del potencial, tal que
     \[\overline{S}_z=\frac{\hbar}{g\mu_B}\overline{\mu}_z=\frac{\hbar}{g\mu_B}g\mu_BjB_j(x)=\hbar j B_j(x)\]
     entonces,
     \[H_{medio}=2J\frac{\hbar}{g\mu_0\mu_B}N_1\hbar jB_j(x)\]
\end{itemize}
El problema es que $x(H_{medio})$, ya que $H_{medio}=\frac{kT}{g\mu_0\mu_B}x-H_{ext}$, que igualando,
\[\frac{kT}{g\mu_0\mu_B}x-H_{ext}=2J\frac{\hbar^2}{g\mu_0\mu_B}N_1jB_j(x)\]
Despejando $B_j(x)$,
\[B_j(x)=\frac{kT}{2J\hbar^2N_1j}x-\frac{g\mu_0\mu_B}{2JN_1\hbar^2j}H_{ext}\]
Lo resolvemos con métodos numéricos o graficando, vamos a graficarlo,

\begin{Figura}
    \centering
    \includegraphics[width=1\linewidth]{oo.png}
    %\caption{Enter Caption}
    \label{fig:66-algos}
\end{Figura}

donde vemos que $x\neq0$ si $\left.\frac{kT}{2J\hbar^2N_1j}<\frac{dB_j(x)}{dx}\right|_{x=0}$ y por tanto, $B_j(x)\approx\frac{x}{3}(j+1)$. Luego, $\frac{kT}{2J\hbar^2N_1j}<\frac{j+1}{3}$, que despejando la temperatura,
\[T<\frac{2J\hbar^2N_1j(j+1)}{3k}=T_C\]
denominando a $T_C$ como \textbf{Temperatura de Curie}.\\ \\
Ocurren dos comportamiento con la temperatura,
\begin{itemize}
    \item Si aumentamos la temperatura en un material ferromagnético, la ecuación no tendría solución y no se comportará como un ferromagnético.
    \item Si $T\rightarrow0$, entonces cualquier material paramagnético obtendrá las propiedades de un ferromagnético.
\end{itemize}
\subsubsection*{Nota}
La susceptibilidad magnética viene dada por
\begin{equation}
    \chi_m=\frac{Ng^2\mu_0\mu_Bj(j+1)}{3Vk(T-T_C}
\end{equation}
denominándose Ley de Curie-Weiss.

\begin{thebibliography}{99}
    \bibitem[1]{avogadro}
        \url{https://physics.nist.gov/cgi-bin/cuu/Value?na}
    \bibitem[2]{libro} Brey Abalo, J., Rubia Pacheco, J. d. l., Rubia Sánchez, J. d. l. (2001). MECÁNICA ESTADÍSTICA. España: UNED.
    \bibitem[3]{apuntes}Universidad de Córdoba, Facultad de Ciencias, Grado de Física, Apuntes de Física Estadística, profesor Manuel Alcaraz Peregrina, curso 2023-2024
    \bibitem[4]{apuntes2}Universidad de Córdoba, Facultad de Ciencias, Grado de Física, Apuntes de Mecánica y Ondas II, profesor Manuel Alcaraz Peregrina, curso 2022-2023
\end{thebibliography}
\label{Bibliography}


\end{document}
