\subsection{El espacio de tensores (r,s): Definición, propiedades y ejemplos} % Main chapter title
\label{cap1-sec2-subsec2} 
 Una vez visto el producto tensorial de dos elementos, vamos a ver una generalización.
\begin{definition}
    Sea $V$ un espacio vectorial y $V^*$ un espacio dual, definimos 
    \[\Omega^{r,s}(V)=\curlybraces{f\text{ aplicación multilineal};\hspace{2mm}f:V^*\times\overset{r}{\dots}\times V^*\times V\times\overset{s}{\dots}\times V}\]
    es decir,
    \[\Omega^{r,s}(V)\equiv V\otimes\overset{r}{\dots}\otimes V\otimes V^*\otimes\overset{s}{\dots}\otimes V^*\]
\end{definition}
\begin{note}
    Sea $V$ un espacio vectorial. Las formas multilineales cuyas variables están en $V^*$ o $V$, se denominan \textbf{tensores sobre  }$\mathbf{V}$ y los espacios vectoriales que forman, se denominan \textbf{espacios tensoriales sobre} $\mathbf{V}$.\\ \\
    El número de variables de $V^*$ y $V$ se denominan los \textbf{grados} de un tensor; al número de variables de $V^*$ se les denomina \textbf{grados contravariantes} y al número de variables de $V$, \textbf{grados covariantes}. Así, una forma multilineal del tipo $V^*\times V\times V$ es un tensor de tipo $(1,2)$, denotado como $\ptensor{V}{V^*}\otimes V^*=T_2^1$.
\end{note}
\begin{note}
    Un tensor de tipo $(0,0)$ se define como un escalar, tal que $T_0^0=\lambda$. \\
    Un tensor de tipo $(1,0)$ se denomina \textbf{vector contravariante o vector} y a uno del tipo $(0,1)$, \textbf{vector covariante o covector}.\\
    Un tensor de tipo $(r,0)$ se denomina \textbf{tensor contravariante} y uno del tipo $(0,s)$, se denomina \textbf{tensor covariante}.
\end{note}
\noindent Al igual que $V\otimes V$ es un espacio vectorial, el conjunto $\Omega^{r,s}(V)$ también debe serlo.
\begin{proposition}
    $\Omega^{r,s}(V)$ es espacio vectorial.
\end{proposition}
\begin{proof}
    Tenemos que ver que $(\Omega^{r,s}(V),+,\cdot)$ es un $\mathbb{R}$-espacio vectorial, siendo $+$ una operación interna y $\cdot$ una operación externa, tal que
    \[\begin{matrix}
        +: & \Omega^{r,s}(V)\times\Omega^{r,s}(V) & \to & \Omega^{r,s}(V)
    \end{matrix}\hspace{5mm}y\hspace{5mm}\begin{matrix}
        \cdot: & \Omega^{r,s}(V)\times\mathbb{R} & \to & \Omega^{r,s}(V)
    \end{matrix}\]
    Veamos si verifica las condiciones de espacio vectorial:
    \begin{enumerate}
        \item ¿$(\Omega^{r,s}(V),+)$ es un grupo abeliano?
        \begin{enumerate}[label=(\roman*)]
            \item ¿$+$ es una operación cerrada?\\
            Sabiendo que $f,g\in\Omega^{r,s}(V)$ son aplicaciones multilineales, entonces $h=f+g$ será otra aplicación multilineal, lo vemos,

            \[\begin{array}{l}
            h(v_1^*,\dots,\alpha v_i^*+\lambda w_i^*,\dots,v_s^*)=(f+g)(v_1^*,\dots,\alpha v_i^*+\lambda w_i^*,\dots, v_s^*)=\\
            =f(v_1^*,\dots,\alpha v_i^*+\lambda w_i^*,\dots, v_s^*)+g(v_1^*,\dots,\alpha v_i^*+\lambda w_i^*,\dots, v_s^*)=\\
            =\alpha f(v_1^*,\dots,v_i^*,\dots,v_s^*)+\lambda f(v_1^*,\dots,w_i^*,\dots,v_s^*)+\\+\alpha g(v_1^*,\dots,v_i^*,\dots,v_s^*)+\lambda g(v_1^*,\dots,w_i^*,\dots,v_s^*)=\\
            =\alpha\brackets{f(v_1^*,\dots,v_i^*,\dots,v_s^*)+g(v_1^*,\dots,v_i^*,v_s^*)}+\\+\lambda\brackets{f(v_1^*,\dots,w_i^*,\dots,v_s^*)+g(v_1^*,\dots,w_i^*,\dots,v_s^*)}=\\
            =\alpha(f+g)(v_1^*,\dots,v_i^*,\dots,v_s^*)+\lambda(f+g)(v_1^*,\dots,w_i^*,\dots,v_s^*)=\\
            =\alpha h(v_1^*,\dots,v_i^*,\dots,v_s^*)+\lambda h(v_1^*,\dots,w_i^*,\dots,v_s^*)\checkmark
            \end{array}\]
            
            Luego, $h\in\Omega^{r,s}(V)$, y por tanto, la operación es cerrada. $\checkmark$
            \item ¿Asociatividad?\\
            $\forall f,g,h\in\Omega^{r,s}(V)$
            \[(f+(g+h))(v)=f(v)+(g+h)(v)=f(v)+(g(v)+h(v))=((f)(v)+g(v))+h(v)=\]\[=(f+g)(v)+h(v)=((f+g)+h)(v)\checkmark\]
            \item ¿Elemento neutro?\\
            $\forall f\in\Omega^{r,s}(V)$, $\exists f^0\in\Omega^{r,s}(V)$ tal que $f^0+f=f+f^0=f$
            \[(f+f^0)(v)=f(v)+f^0(v)=f(v)\Rightarrow f^0(v)=0\Rightarrow f^0\equiv0\checkmark\]
            \item ¿Elemento simétrico?\\
            $\forall f\in\Omega^{r,s}(V)$, $\exists\Tilde{f}\in\Omega^{r,s}(V)$ tal que $f+\Tilde{f}=\Tilde{f}+f=f^0$
            \[(f+\Tilde{f})(v)=f(v)+\Tilde{f}(v)=f^0(v)=0\Rightarrow\Tilde{f}(v)=-f(v)\Rightarrow\Tilde{f}\equiv-f\checkmark\]
            \item ¿Conmutabilidad?\\
            $\forall f,g\in\Omega^{r,s}(V)$, 
            \[(f+g)(v)=f(v)+g(v)=g(v)+f(v)=(g+f)(v)\checkmark\]
            Luego, $(\Omega^{r,s}(V), +)$ es grupo abeliano.$\qedh$
        \end{enumerate}
        \item Doble propiedad distributiva:
        \begin{enumerate}
            \item $\forall\lambda,\mu\in\mathbb{R}$ y $\forall f\in\Omega^{r,s}(V)$,
            \[(\lambda+\mu)f(v)=f((\lambda+\mu)v)=f(\lambda v)+f(\mu v)=\lambda f(v)+\mu f(v)\checkmark\]
            \item $\forall\lambda\in\mathbb{R}$ y $\forall f,g\in \Omega^{r,s}(V)$,
            \[\lambda(f+g)(v)=(f+g)(\lambda v)=f(\lambda v)+g(\lambda v)=\lambda f(v)+\lambda g(v)\checkmark\]
        \end{enumerate}
        \item Propiedad pseudo-asociativa:\\
        $\forall\lambda,\mu\in\mathbb{R}$ y $\forall f\in\Omega^{r,s}(V)$,
        \[\lambda(\mu f(v))=\lambda f(\mu v)=f((\lambda\mu) v)=(\lambda\mu)f(v)\checkmark\]
        \item Elemento unitario de $\mathbb{R}$:\\
        $\forall f\in\Omega^{r,s}(V)$, $\exists\Tilde{\lambda}\in\mathbb{R}$ tal que $\Tilde{\lambda}\cdot f=f\cdot\Tilde{\lambda}=f$,
        \[\Tilde{\lambda}\cdot f(v)=f(\Tilde{\lambda}\cdot v)=f(v)\Rightarrow \Tilde{\lambda}v=v\Rightarrow\Tilde{\lambda}=1\checkmark\]
    \end{enumerate}
    Luego, $(\Omega^{r,s}(V), +, \cdot)$ es un $\mathbb{R}$-espacio vectorial. \qedhere
\end{proof}
\noindent Como $\Omega^{r,s}(V)$ es un espacio vectorial, deberá de tener una \textbf{base}.
\begin{proposition}
    Si tenemos un conjunto $V$ que sea un $\mathbb{R}$-espacio vectorial con base $B=\curlybraces{v_1,\dots,v_n}$ y $V^*$ el espacio dual de $V$ con base $B^*=\curlybraces{f^1,\dots,f^n}$, entonces todo $h\in\Omega^{r,s}$ será combinación lineal de $B^{r,s}=\curlybraces{v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s}}$ \\
    tal que $h\equiv\left(h^{i_1,\dots,i_r}_{j_1,\dots,j_s}\right)^n_{i_1,\dots,i_r,j_1,\dots,j_s}$
\end{proposition}
\begin{proof}
    Tenemos que ver que los elementos de la base son linealmente independientes y para ello, se debe cumplir que
    \[\sum\limits_{\overset{i_1,\dots,i_r}{j_1,\dots,j_s}}^n\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s}=0\Leftrightarrow\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}=0\]
    Luego, dados $i_1^0,\dots,i_r^0,j_1^0,\dots,j_s^0$ índices fijos, y vamos a tomar
    \[f\equiv\sum\limits_{\overset{i_1,\dots,i_r}{j_1,\dots,j_s}}^n\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}(v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s})=\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}(v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s})\]
    luego,
    \[0=f(f^{i_1^0},\dots,f^{i_r^0},v_{j_1^0},\dots,v_{j_s^0})=\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}\left(f^{i_1^0}(v_{i_1})\dots f^{i_r^0}(v_{i_r})f^{j_1}(v_{j_1^0})\dots f^{j_s}(v_{j_s^0})\right)\]
    sabemos que un elemento de la base de $V$ con un elemento de la base de $V^*$ cumple que
    \[\left\lbrace\begin{matrix}
        f^i(v_j)\overset{j\neq i}{=}0\\
        f^i(v_i)=1
    \end{matrix}\right.\]
    luego, esto es una delta de Kronecker $\delta_{i,j}$, y entonces,
    \[0=\lambda_{j_1,\dots,j_s}^{i_1,\dots,i_r}\delta_{i_1}^{i_1^0}\dots\delta_{i_r}^{i_r^0}\delta_{j_1^0}^{j_1}\dots\delta_{j_s^0}^{j_s}=\lambda_{j_1^0,\dots,j_s^0}^{i_1^0,\dots,i_r^0}\Rightarrow\lambda_{j_1^0,\dots,j_s^0}^{i_1^0,\dots,i_r^0}=0\]
    y por tanto, los elementos son linealmente independientes. $\checkmark$\\ 
    Ahora tenemos que comprobar que un elemento $h\in\Omega^{r,s}(V)$ se puede escribir como combinación lineal de los elementos de la base, es decir,
     \[h(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})=h_{j_1,\dots,j_s}^{i_1,\dots,i_r}\left(v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s}\right)(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})\]
     Vamos a verlo:
     \begin{note}
         Vamos a hacer primero una aclaración acerca de los índices. Cuando decimos que $i_1$ va desde $1$ hasta $n$, estamos diciendo que tenemos la sucesión $1_1,2_1,\dots,n_1$. Por tanto, aunque en los sumatorios pongamos $\sum\limits_{i_1=1}^n a_{i_1}$, lo correcto sería poner $\sum\limits_{i=1}^na_{i_1}$, pero esto puede llevar a confusión o a problemas cuando, por ejemplo, $i_1$ no tenga los mismos elementos que $i_7$. Luego, para referirnos a un elemento $i_k$-ésimo, escribiremos $a_{i_k}=\sum\limits_{i_k=1}^n\gamma_{i_k}b_{i_k}$.
     \end{note}
\noindent Luego, $\forall w_{j_1},\dots,w_{j_s}\in V$ y $\forall g^{i_1},\dots g^{i_r}\in V^*$, tal que
     \[\begin{matrix}
         w_{j_k}=\sum\limits_{j_k=1}^n\mu^{j_k}v_{j_k}=\mu^{j_k}v_{j_k}\\
         g^{i_k}(v)=\sum\limits_{i_k=1}^n\mu_{i_k}f^{i_k}(v)=\mu_{i_k}f^{i_k}(v)
     \end{matrix}\]
     Luego, tomando un $h\in\Omega^{r,s}(V)$, tal que
     \[\begin{matrix}
         h(v_{j_p})=h_{j_p}; & h_{j_p}\in\mathbb{R} & p=1,2,\dots,s\\
         h(f^{i_q})=h^{i_q}; & h^{i_q}\in\mathbb{R} & q=1,2,\dots,r
     \end{matrix}\]  
    Entonces,
    \[h(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})=h(g^{i_1})\dots h(g^{i_r})h(w_{j_1})\dots h(w_{j_s})=\]\[=h\left(\sum\limits_{i_1=1}^n\mu^{i_1}f^{i_1}\right)\dots h\left(\sum\limits_{i_r=1}^n\mu^{i_r}f^{i_r}\right)h\left(\sum\limits_{j_1=1}^n\mu_{j_1}v_{j_1}\right)\dots h\left(\sum\limits_{j_s=1}^n\mu_{j_s}v_{j_s}\right)=\]\[=\mu^{i_1}h(f^{i_1})\dots \mu^{i_r}h(f^{i_r})\mu_{j_1}h(v_{j_1})\dots \mu_{j_s}h(v_{j_s})=\]\[=\curlybraces{\text{Podemos agrupar los escalares de tal forma que}}=\]\[=\mu_{j_1,\dots,j_s}^{i_1,\dots,i_r}h(f^{i_1})\dots h(f^{i_r})h(v_{j_1})\dots h(v_{j_s})=\]\[=\mu_{j_1,\dots,j_s}^{i_1,\dots,i_r}h^{i_1}\dots h^{i_r}h_{j_1}\dots h_{j_s}=\mu_{j_1,\dots,j_s}^{i_1,\dots,i_r}h_{j_1,\dots,j_s}^{i_1,\dots,i_r} \]
    Usando que $\mu_{j_1,\dots,j_s}^{i_1,\dots,i_r}=(v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes f^{j_s})(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})$, tenemos
    \[h(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})=h_{j_1,\dots,j_s}^{i_1,\dots,i_r}(v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes f^{j_s})(g^{i_1},\dots,g^{i_r},w_{j_1},\dots,w_{j_s})\]
    Luego, $h\in\Omega^{r,s}(V)$ es combinación lineal de los elementos de la base. $\checkmark$\\
    Por tanto, $B^{r,s}=\curlybraces{v_{i_1}\otimes\dots\otimes v_{i_r}\otimes f^{j_1}\otimes\dots\otimes f^{j_s}}$ es base de $\Omega^{r,s}(V)$. \qedhere
\end{proof} 
\begin{corollary}
    Sea $V$ un espacio vectorial con $\rm{dim}(V)=n$. Luego, $\rm{dim}~\Omega^{r,s}(V)=n^{r+s}$
\end{corollary}
\begin{proof}
    Sabemos que
    \[\Omega^{r,s}(V)\equiv V\otimes\overset{r}{\dots}\otimes V\otimes V^*\otimes\overset{s}{\dots}\otimes V^*\]
Por tanto, al ser $\rm{dim}(V)=n$, al tener $V$ $r-$veces, tenemos que $\rm{dim}(V\otimes\overset{r}{\dots}\otimes V)=n^r$ y como $\rm{dim}(V)=\rm{dim}(V^*)$, entonces tenemos que $\rm{dim}(V^*\otimes\overset{s}{\dots}\otimes V^*)=n^s$, por tanto, $\rm{dim}~\Omega^{r,s}(V)=n^{r}\cdot n^s=n^{r+s}$
\end{proof}
 \begin{note}
  El producto escalar es un tensor de tipo $(0,2)$, es decir, $\scalar{\cdot}{\cdot}\in\Omega^{0,2}(V)$ tal que
  \[\scalar{\cdot}{\cdot}=\sum g_{ij}\ptensor{f^i}{f^j}\equiv g_{ij}\ptensor{f^i}{f^j}\]
 \end{note}
\noindent Ahora vamos a ver el caso particular del producto escalar. El cuál es muy importante, pues nos permitirá escribir \textbf{tensores independientes de sus bases}.
 \begin{note}
     El producto escalar $\scalar{\cdot}{v}$ es un tensor de tipo $(0,1)$, es decir, $\scalar{\cdot}{v}\in\Omega^{0,1}(V)$ tal que
     \[\scalar{\cdot}{v}=g_{ij}(\ptensor{f^i}{f^j})(\cdot, v)\]
         \label{Nota1}
 \end{note}

 
 \begin{proposition}
     Dado $\curlybraces{v_1,v_2,\dots,v_n}$ base de $V$, tenemos que $\scalar{\cdot}{v_1},\dots,\scalar{\cdot}{v_n}$ es base de $V^*$
 \end{proposition}
\begin{proof}
    Tenemos que comprobar que los elementos de la base son linealmente independientes, es decir,
    \[\lambda_1\scalar{\cdot}{v_1}+\lambda_2\scalar{\cdot}{v_2}+\dots+\lambda_n\scalar{\cdot}{v_n}=0\Leftrightarrow\lambda_1=\dots=\lambda_n=0\]
    Evaluando un $v_k\in B$, tenemos
    \[0=\lambda_1\scalar{v_k}{v_1}+\dots+\lambda_k\scalar{v_k}{v_k}+\dots+\lambda_n\scalar{v_k}{v_n}\]
    Como los elementos de $B$ son linealmente independientes, por la condición de base dual, se debe cumplir que
    \[\scalar{v_i}{v_j}=\delta_{ij}\]
    Entonces,
     \[0=\lambda_1\cancelto{0}{\scalar{v_k}{v_1}}+\dots+\lambda_k\cancelto{1}{\scalar{v_k}{v_k}}+\dots+\lambda_n\cancelto{0}{\scalar{v_k}{v_n}}=\lambda_k\]
     Por tanto, $\lambda_k=0$, luego son linealmente independientes. Además, como esta base es una base dual de $V^*$, tendrá la misma dimensión que $V$, por la Proposición \ref{Prop1.6} y así, el conjunto generador pasa a ser base.
\end{proof}
\noindent Así, dado un espacio vectorial métrico no degenerado, $(V,g)$ de dimensión finita, se puede establecer un isomorfismo entre $V$ y su dual $V^*$, que \textbf{no depende de bases, sino solo de la métrica}, $g$. Esto permite identificar de forma natural los vectores de $V$ con las formas lineales sobre $V$.
\begin{note}
    Vamos a identificar el producto escalar como $\scalar{\cdot}{v}\equiv g(\cdot,v)$ para simplificar la notación. 
\end{note}
\noindent Ahora vamos a definir dos aplicaciones, \textbf{bemol} y \textbf{sostenido}, las cuáles nos permitirán 'bajar' o 'subir' índices, es decir, transformar el tipo del tensor.
\begin{definition}
    Definimos la aplicación \textbf{bemol} como,
    \[\begin{array}{rlll}
        \flat: & V  & \to & V^*\\
         &  v & \mapsto & v^\flat
    \end{array}\]
    la cuál nos sirve para 'bajar índices'
\end{definition}
\begin{proposition}%\footnote{Jónatan: La aplicación ${\flat}$ hay que definirla como una aplicación lineal ${\flat}:V\rightarrow V^*$. Una vez definida, hay que probar que es biyectiva, y una vez sabemos que es biyectiva, podemos definir $\sharp$ como la inversa de $\flat$.}
    Sea $(V,g)$ un espacio vectorial dotado de una métrica no degenerada.
    \begin{enumerate}
        \item Para cada $v\in V$ la aplicación \[\begin{array}{rll}
             v^\flat\equiv g(v,\cdot):V &\to &\mathbb{R}  
        \end{array}\]
        \[v^\flat(\omega)=g(v,\omega)\hspace{5mm}\forall\omega\in V\]
        es lineal, es decir, $v^\flat\in V^*$.
        \item La aplicación \textbf{bemol} ('bajar índices'), dada por
        \[\begin{array}{rll}
             \flat:V&\to& V^*\\
             v& \mapsto&v^\flat
        \end{array}\]
        es un isomorfismo de espacios vectoriales.
    \end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
    \item Veamos que es aplicación lineal,\\
    $\forall v,w,u\in V$ y $\forall\lambda\in\mathbb{R}$,
    \[\begin{array}{rrrl}
        (i) & v^\flat(w+u)\equiv g(v,w+u) & = & g(v,w)+g(v,u)=v^\flat(w)+v^\flat(u)~\checkmark \\ \\
        (ii) & v^\flat(\lambda w)\equiv g(v,\lambda w) & = & \lambda ~g(v,w)=\lambda~v^\flat(w)~\checkmark
    \end{array}\]
    \qedhere
    \item Debemos probar que es una aplicación lineal y biyectiva.\\
    Vemos que es lineal por 1. $\checkmark$\\
    Comprobamos que es biyectiva, es decir, es inyectiva y sobreyectiva. Comprobamos que es inyectiva viendo que el $Ker\curlybraces{\flat}=\curlybraces{0}$,
    \[ker\curlybraces{\flat}=\curlybraces{\Tilde{w}\in V;~v^\flat(\Tilde{w})=0}\]
    usando que $v^\flat\equiv g(v,\cdot)$, tenemos que $0=v^flat(\tilde{w})\equiv g(v,\Tilde{w})$ y como $v$ es cualesquiera $v\in V$, al ser $g$ no degenerada, se cumple que $g(v,\Tilde{w})=0$ si y solo si $\Tilde{w}=0$ ó $v=0$, pero como $v$ es arbitrario, nos quedamos con $\Tilde{w}=0$ y por tanto, $ker\curlybraces{\flat}=\curlybraces{0}$, luego es inyectiva. $\checkmark$\\
    Vemos que es sobreyectiva usando el Primer Teorema de Isomorfía, el cuál nos dice que
    \[dim(V)=dim(ker\curlybraces{\flat})+dim(Im\curlybraces{\flat})\]
    pero como $ker\curlybraces{\flat}=\curlybraces{0}$, entonces, $dim(ker\curlybraces{\flat})=0$ y además, vemos que $Im\curlybraces{\flat}=V^*$ y sabemos que $dim (V)=dim(V^*)$, luego
    \[dim(V)=\cancelto{0}{dim(ker\curlybraces{\flat})}+dim(Im\curlybraces{\flat})=dim(V^*)\]
    luego, al ser la imagen de la misma dimensión que el espacio de entrada, decimos que $\flat$ es sobreyectiva. $\checkmark$\\
    Por tanto, la aplicación $\flat$ es un isomorfismo. $\qedh$
\end{enumerate}
\end{proof}
\begin{definition}
    Definimos la aplicación \textbf{sostenido} como la inversa de la aplicación bemol (pues la aplicación bemol es biyectiva y por tanto tiene inversa), dada por
        \[\begin{array}{rll}
             \sharp:V^*&\to& V  \\
            v^\flat & \mapsto & v
        \end{array}\]
        la cuál nos permite subir índices.
\end{definition}
\noindent Una caracterización alternativa del sostenido es la siguiente.
\begin{proposition}
    Sea $(V,g)$ un espacio vectorial métrico no degenerado de dimensión finita y
    $\phi\in V^*$. Entonces $\phi^\sharp$ es el único vector que verifica,
    \[g(\phi^\sharp,v)=\phi(v),\hspace{6mm}\forall v\in V\]
\end{proposition}
\begin{proof}
    Aplicando las definiciones,
    \[g(\phi^\sharp,v)=(\phi^\sharp)^\flat(v)=\phi(v)\]
    Además, si otro vector $u_\phi\in V$ verificara esa relación, se tendría
    \[g(\phi^\sharp-u_\phi,v)=\phi(v)-\phi(v)=0,\hspace{5mm}\cfoot{ v\in V}\]
    y, al ser no degenerada, $\phi^\sharp-u_\phi=0\Rightarrow u_\phi=\phi^\sharp$.
\end{proof}
\begin{note}
    
    Sea $w=w^iv_i\equiv w^i$ un vector de $V$ y sea $g(\cdot,w)$ una $1-$forma métrica asociada, tal que $g(\cdot,w)=w_j f^j\equiv w_j$. Se tiene entonces que
    \[w_j=g_{ij}w^i\hspace{5mm}y\hspace{5mm}w^i=g^{ij}w^i\]
    A esto lo denominamos \textbf{subida} y \textbf{bajada} de índices (métrica).
    \\ \\
    Además, usaremos la base de productos escalares, pues
    \[V\longleftrightarrow V^*\]
    \[v\to v^*\hspace{3mm}\text{depende de la base }B^*\]
    \[v\to g(\cdot,v)\hspace{3mm}\text{no depende de la base }B^*\text{, sino de la métrica }g \]
\end{note}

\begin{note}
    Los nombres 'subir y bajar' índices provienen de la Relatividad General. En Mecánica Cuántica, Dirac introdujo una nomenclatura distinta. Considerando un espacio vectorial euclídeo (y, con más generalidad, un espacio de Hilbert) con producto escalar $\scalar{\cdot}{\cdot}$; los vectores $v$ y $w$ se denotan como un 'ket', $v\equiv\ket{v}$, $w\equiv\ket{w}$ y sus bemoles como un 'bra', $v^\flat\equiv\bra{v}$, $w^\flat\equiv\bra{w}$, de modo que $v^\flat(w)$ es el 'braket' $\braket{v|w}$.
\end{note}